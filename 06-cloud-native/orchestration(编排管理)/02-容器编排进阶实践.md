# 容器编排进阶实践

## Kubernetes高级特性

### 资源管理优化

```yaml
# 资源配额和限制
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-team-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "10"
    pods: "50"
    services: "10"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: development
spec:
  limits:
  - default:
      cpu: "1"
      memory: 1Gi
    defaultRequest:
      cpu: "0.5"
      memory: 512Mi
    type: Container
  - max:
      cpu: "2"
      memory: 4Gi
    min:
      cpu: "100m"
      memory: 128Mi
    type: Container
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: development
spec:
  hard:
    requests.storage: "100Gi"
    persistentvolumeclaims: "20"
```

### Pod调度策略

```yaml
# 节点亲和性
apiVersion: apps/v1
kind: Deployment
metadata:
  name: high-performance-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: high-performance-app
  template:
    metadata:
      labels:
        app: high-performance-app
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - compute-optimized
                - gpu-enabled
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: instance-family
                operator: In
                values:
                - c5
                - c6i
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - high-performance-app
              topologyKey: kubernetes.io/hostname
      containers:
      - name: app
        image: myapp:latest
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "4"
            memory: 8Gi
```

### 污点和容忍度

```yaml
# 配置节点污点
kubectl taint nodes node1 dedicated=gpu:NoSchedule
kubectl taint nodes node1 dedicated=gpu:NoExecute

# 应用容忍度
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gpu-workload
  template:
    metadata:
      labels:
        app: gpu-workload
    spec:
      containers:
      - name: gpu-container
        image: tensorflow/tensorflow:latest-gpu
        resources:
          limits:
            nvidia.com/gpu: 1
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "gpu"
        effect: "NoSchedule"
      - key: "dedicated"
        operator: "Equal"
        value: "gpu"
        effect: "NoExecute"
```

### 网络策略

```yaml
# 严格的网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
# 应用特定的入站策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-ingress-policy
spec:
  podSelector:
    matchLabels:
      app: web-app
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api-gateway
    ports:
    - protocol: TCP
      port: 8080
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 8080
---
# 出站策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-egress-policy
spec:
  podSelector:
    matchLabels:
      app: web-app
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - namespaceSelector:
        matchLabels:
          name: external
    ports:
    - protocol: TCP
      port: 443
```

## 服务网格（Service Mesh）

### Istio配置

```yaml
# Istio Gateway配置
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: web-app-gateway
  namespace: production
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: web-app-tls
    hosts:
    - web-app.example.com
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - web-app.example.com
    redirect:
      httpsRedirect: true
---
# VirtualService配置
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: web-app-vs
  namespace: production
spec:
  hosts:
  - web-app.example.com
  gateways:
  - web-app-gateway
  http:
  - match:
    - uri:
        prefix: /api/v1
    route:
    - destination:
        host: api-service
        port:
          number: 8080
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 5s
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: web-app-service
        port:
          number: 3000
    timeout: 5s
---
# DestinationRule配置
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: api-service-dr
  namespace: production
spec:
  host: api-service
  trafficPolicy:
    loadBalancer:
      simple: LEAST_CONN
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

### 流量管理策略

```yaml
# 金丝雀部署
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: canary-vs
spec:
  hosts:
  - api-service
  http:
  - match:
    - headers:
        user-group:
          exact: premium
    route:
    - destination:
        host: api-service
        subset: v2
  - route:
    - destination:
        host: api-service
        subset: v1
      weight: 90
    - destination:
        host: api-service
        subset: v2
      weight: 10
---
# 熔断器配置
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: api-service-circuit-breaker
spec:
  host: api-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
        maxRetries: 3
    circuitBreaker:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 30
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
```

## 自动伸缩（HPA和VPA）

### 水平Pod自动伸缩

```yaml
# HPA配置
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
---
# 自定义指标HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa-custom
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-service
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Object
    object:
      metric:
        name: queue_length
      describedObject:
        apiVersion: v1
        kind: Service
        name: message-queue-service
      target:
        type: Value
        value: "1000"
```

### 垂直Pod自动伸缩

```yaml
# VPA配置
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: api-container
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 4
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
```

## 存储管理

### 动态卷 provisioning

```yaml
# StorageClass配置
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  labels:
    storage-type: ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain
---
# PVC配置
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-pvc
  namespace: production
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: fast-ssd
  selector:
    matchLabels:
      disk-type: ssd
```

### StatefulSet配置

```yaml
# StatefulSet应用
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
  namespace: production
spec:
  serviceName: mongodb-headless
  replicas: 3
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo:5.0
        ports:
        - containerPort: 27017
          name: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: password
        volumeMounts:
        - name: mongodb-data
          mountPath: /data/db
        - name: mongodb-config
          mountPath: /etc/mongod.conf
          subPath: mongod.conf
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
  volumeClaimTemplates:
  - metadata:
      name: mongodb-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
```

### 备份和恢复策略

```yaml
# Velero备份配置
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: production-backup
  namespace: velero
spec:
  includedNamespaces:
  - production
  includedResources:
  - pods
  - deployments
  - services
  - persistentvolumeclaims
  storageLocation: default
  ttl: 720h0m0s
  snapshotVolumes: true
  volumeSnapshotLocations:
  - default
---
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"
  template:
    includedNamespaces:
    - production
    storageLocation: default
    ttl: 168h0m0s
```

## 安全配置

### RBAC权限管理

```yaml
# ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-service-account
  namespace: production
automountServiceAccountToken: true
---
# Role配置
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: app-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-rolebinding
  namespace: production
subjects:
- kind: ServiceAccount
  name: app-service-account
  namespace: production
roleRef:
  kind: Role
  name: app-role
  apiGroup: rbac.authorization.k8s.io
---
# Pod中使用ServiceAccount
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
  namespace: production
spec:
  replicas: 2
  selector:
    matchLabels:
      app: secure-app
  template:
    metadata:
      labels:
        app: secure-app
    spec:
      serviceAccountName: app-service-account
      containers:
      - name: app
        image: myapp:latest
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
```

### 网络安全策略

```yaml
# 零信任网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-namespaces
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
# 白名单网络策略
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-traffic
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api-service
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: api-gateway
    - namespaceSelector:
        matchLabels:
          name: monitoring
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
```

## 监控和告警

### Prometheus监控配置

```yaml
# ServiceMonitor配置
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: app-metrics
  namespace: production
  labels:
    app: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
  namespaceSelector:
    matchNames:
    - production
---
# AlertManager配置
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: critical-alerts
  namespace: production
spec:
  groups:
  - name: kubernetes.rules
    rules:
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 3
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"
        
    - alert: HighCPUUsage
      expr: 100 * (1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        
    - alert: PodMemoryUsageHigh
      expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage on {{ $labels.pod }}"
```

## 持续部署（GitOps）

### ArgoCD配置

```yaml
# ArgoCD Application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: production-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/org/repo
    targetRevision: HEAD
    path: k8s/production
    helm:
      valueFiles:
      - values-production.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - PruneLast=true
```

### GitOps工作流

```bash
#!/bin/bash
# deploy.sh - GitOps部署脚本

REPO_DIR="/tmp/app-repo"
APP_NAME="myapp"
ENVIRONMENT="production"

# 克隆代码仓库
git clone https://github.com/org/app-repo.git $REPO_DIR
cd $REPO_DIR

# 更新应用版本
echo "更新应用到版本 $APP_VERSION"
sed -i "s/image: myapp:.*/image: myapp:$APP_VERSION/g" k8s/$ENVIRONMENT/deployment.yaml

# 更新Helm Values
if [ "$USE_HELM" = "true" ]; then
  helm upgrade --install $APP_NAME ./charts/$APP_NAME \
    --namespace $ENVIRONMENT \
    --set image.tag=$APP_VERSION \
    --set replicaCount=$REPLICA_COUNT \
    --values values-$ENVIRONMENT.yaml
else
  # 直接应用Kubernetes配置
  kubectl apply -f k8s/$ENVIRONMENT/ -n $ENVIRONMENT
fi

# 提交变更
git add .
git commit -m "Deploy $APP_NAME version $APP_VERSION to $ENVIRONMENT"
git push origin main

# 触发ArgoCD同步
argocd app sync $APP_NAME

echo "部署完成: $APP_NAME v$APP_VERSION"
```

## 故障排除和诊断

### 故障诊断脚本

```bash
#!/bin/bash
# k8s-diagnostics.sh

NAMESPACE=$1
APP_NAME=$2

echo "=== Kubernetes故障诊断报告 ==="
echo "时间: $(date)"
echo "命名空间: $NAMESPACE"
echo "应用: $APP_NAME"
echo "================================"

# 检查Pod状态
echo "## Pod状态"
kubectl get pods -n $NAMESPACE -l app=$APP_NAME -o wide

# Pod详细状态
echo "## Pod详细状态"
kubectl describe pods -n $NAMESPACE -l app=$APP_NAME

# 检查资源使用情况
echo "## 资源使用情况"
kubectl top pods -n $NAMESPACE -l app=$APP_NAME

# 检查事件
echo "## 最近事件"
kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' | grep $APP_NAME

# 检查日志
echo "## 最近错误日志"
for pod in $(kubectl get pods -n $NAMESPACE -l app=$APP_NAME -o jsonpath='{.items[*].metadata.name}'); do
  echo "--- Pod: $pod ---"
  kubectl logs -n $NAMESPACE $pod --tail=20 --timestamps | grep -i error || echo "无错误日志"
done

# 检查网络连接
echo "## 服务端点"
kubectl get endpoints -n $NAMESPACE -l app=$APP_NAME

# 检查配置映射和密钥
echo "## 配置信息"
kubectl get configmaps -n $NAMESPACE -l app=$APP_NAME
kubectl get secrets -n $NAMESPACE -l app=$APP_NAME
```

### 性能调优脚本

```python
#!/usr/bin/env python3
# k8s_performance_analyzer.py

import kubectl
import json
from datetime import datetime, timedelta

class K8sPerformanceAnalyzer:
    def __init__(self, namespace=None):
        self.namespace = namespace
        self.client = kubectl.Kubectl()
    
    def analyze_resource_utilization(self):
        """分析资源利用率"""
        pods = self.client.get_pods(namespace=self.namespace)
        
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'total_pods': len(pods),
            'high_cpu_pods': [],
            'high_memory_pods': [],
            'recommendations': []
        }
        
        for pod in pods:
            pod_info = self.client.get_pod_metrics(pod['metadata']['name'], self.namespace)
            
            # CPU分析
            cpu_usage = self._calculate_cpu_usage(pod_info)
            if cpu_usage > 0.8:
                analysis['high_cpu_pods'].append({
                    'name': pod['metadata']['name'],
                    'cpu_usage': cpu_usage,
                    'limits': self._get_cpu_limits(pod)
                })
            
            # 内存分析
            memory_usage = self._calculate_memory_usage(pod_info)
            if memory_usage > 0.8:
                analysis['high_memory_pods'].append({
                    'name': pod['metadata']['name'],
                    'memory_usage': memory_usage,
                    'limits': self._get_memory_limits(pod)
                })
        
        # 生成建议
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        return analysis
    
    def _generate_recommendations(self, analysis):
        """生成优化建议"""
        recommendations = []
        
        if analysis['high_cpu_pods']:
            recommendations.append({
                'type': 'cpu_scaling',
                'message': f"检测到{len(analysis['high_cpu_pods'])}个Pod CPU使用率过高，建议增加副本数或CPU限制"
            })
        
        if analysis['high_memory_pods']:
            recommendations.append({
                'type': 'memory_scaling',
                'message': f"检测到{len(analysis['high_memory_pods'])}个Pod内存使用率过高，建议增加内存限制或添加内存优化"
            })
        
        return recommendations
    
    def export_analysis_report(self, output_file):
        """导出分析报告"""
        analysis = self.analyze_resource_utilization()
        
        with open(output_file, 'w') as f:
            json.dump(analysis, f, indent=2)
        
        print(f"分析报告已导出到: {output_file}")

if __name__ == "__main__":
    analyzer = K8sPerformanceAnalyzer(namespace="production")
    analyzer.export_analysis_report("k8s_performance_analysis.json")
```

## 总结

Kubernetes进阶实践涵盖了资源管理、网络安全、服务网格、自动伸缩、存储管理等多个关键领域。通过合理配置和优化，可以构建出高可用、高性能、易扩展的容器编排系统。

### 关键要点

1. **资源管理**：合理配置CPU/内存限制，使用资源配额和限制范围
2. **网络策略**：实施严格的网络策略，确保最小权限原则
3. **服务网格**：使用Istio等工具实现流量管理和熔断
4. **自动伸缩**：结合HPA和VPA实现智能资源调整
5. **安全加固**：实施RBAC、Pod安全策略等安全措施
6. **监控告警**：建立完善的监控体系，及时发现和处理问题