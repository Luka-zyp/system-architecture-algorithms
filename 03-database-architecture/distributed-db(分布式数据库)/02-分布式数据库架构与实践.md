# 分布式数据库架构与实践

## 概述

分布式数据库是现代企业级应用的核心基础设施，随着数据量的爆炸性增长和业务复杂度的提升，单体数据库已无法满足高可用、高性能、可扩展性的需求。本文档将深入探讨分布式数据库的架构设计、核心算法、实际实现和最佳实践。

## 1. 分布式数据库架构模式

### 1.1 分布式架构核心概念

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import hashlib
import time
from datetime import datetime
import json

class ConsistencyLevel(Enum):
    """一致性级别"""
    ONE = "ONE"           # 单节点一致性
    QUORUM = "QUORUM"     # 多数节点一致性
    ALL = "ALL"          # 所有节点一致性
    LOCAL_QUORUM = "LOCAL_QUORUM"  # 本地多数一致性
    EACH_QUORUM = "EACH_QUORUM"    # 每个数据中心多数一致性

class NodeState(Enum):
    """节点状态"""
    UP = "UP"           # 正常
    DOWN = "DOWN"       # 故障
    JOINING = "JOINING" # 加入中
    LEAVING = "LEAVING" # 离开中

@dataclass
class DataNode:
    """数据节点"""
    node_id: str
    host: str
    port: int
    data_center: str
    rack: str
    state: NodeState = NodeState.UP
    load: float = 0.0
    last_heartbeat: Optional[datetime] = None
    token_range: Optional[Tuple[str, str]] = None  # (start_token, end_token)

@dataclass
class PartitionKey:
    """分区键"""
    value: str
    hash_value: int
    
    def __post_init__(self):
        self.hash_value = self._calculate_hash()
    
    def _calculate_hash(self) -> int:
        """计算哈希值"""
        return int(hashlib.md5(self.value.encode()).hexdigest(), 16)

@dataclass
class ReplicaPlacement:
    """副本放置策略"""
    primary_node: DataNode
    replica_nodes: List[DataNode]
    consistency_level: ConsistencyLevel

class DistributedDBNode:
    """分布式数据库节点"""
    
    def __init__(self, node: DataNode):
        self.node_info = node
        self.data_store = {}
        self.commit_log = []
        self.vnode_table = {}  # 虚拟节点映射表
        self.node_ring_position = 0
        
    async def start(self):
        """启动节点"""
        print(f"Starting node {self.node_info.node_id} at {self.node_info.host}:{self.node_info.port}")
        self.node_info.last_heartbeat = datetime.utcnow()
        
    async def handle_read(self, key: str, consistency_level: ConsistencyLevel) -> Optional[Any]:
        """处理读请求"""
        # 从本地存储读取
        value = self.data_store.get(key)
        
        # 如果需要更高一致性，从其他副本节点读取
        if consistency_level in [ConsistencyLevel.QUORUM, ConsistencyLevel.ALL]:
            # 这里应该从副本节点读取并合并结果
            # 简化实现
            pass
        
        return value
    
    async def handle_write(self, key: str, value: Any, timestamp: datetime) -> bool:
        """处理写请求"""
        # 写入本地存储
        self.data_store[key] = {
            'value': value,
            'timestamp': timestamp,
            'version': self._get_next_version(key)
        }
        
        # 写入提交日志
        self.commit_log.append({
            'key': key,
            'value': value,
            'timestamp': timestamp,
            'node_id': self.node_info.node_id
        })
        
        return True
    
    def _get_next_version(self, key: str) -> int:
        """获取下一个版本号"""
        if key in self.data_store:
            return self.data_store[key].get('version', 0) + 1
        return 1

class VirtualNodeRing:
    """虚拟节点环"""
    
    def __init__(self, num_tokens: int = 256):
        self.num_tokens = num_tokens
        self.vnodes: Dict[int, DataNode] = {}
        self.ring_positions = sorted([i for i in range(num_tokens)])
        
    def add_node(self, node: DataNode, num_vnodes: int = 1):
        """添加节点到环中"""
        # 为节点分配虚拟节点
        for i in range(num_vnodes):
            token = (node.node_id.__hash__() + i) % self.num_tokens
            self.vnodes[token] = node
            
        # 重新排序
        self.ring_positions = sorted(self.vnodes.keys())
    
    def remove_node(self, node: DataNode):
        """从环中移除节点"""
        tokens_to_remove = []
        for token, node_in_ring in self.vnodes.items():
            if node_in_ring.node_id == node.node_id:
                tokens_to_remove.append(token)
        
        for token in tokens_to_remove:
            del self.vnodes[token]
        
        self.ring_positions = sorted(self.vnodes.keys())
    
    def get_primary_node(self, partition_key: PartitionKey) -> Optional[DataNode]:
        """获取主节点"""
        token = partition_key.hash_value % self.num_tokens
        
        # 找到第一个大于等于token的节点
        for ring_token in self.ring_positions:
            if ring_token >= token:
                return self.vnodes[ring_token]
        
        # 如果没有找到，返回环中的第一个节点
        if self.ring_positions:
            return self.vnodes[self.ring_positions[0]]
        
        return None
    
    def get_replica_nodes(self, partition_key: PartitionKey, replica_count: int) -> List[DataNode]:
        """获取副本节点"""
        if not self.ring_positions:
            return []
        
        primary_token = partition_key.hash_value % self.num_tokens
        
        # 找到主节点
        primary_node = None
        primary_token_idx = None
        for i, ring_token in enumerate(self.ring_positions):
            if ring_token >= primary_token:
                primary_node = self.vnodes[ring_token]
                primary_token_idx = i
                break
        
        if primary_node is None:
            primary_node = self.vnodes[self.ring_positions[0]]
            primary_token_idx = 0
        
        # 获取后续节点作为副本
        replicas = [primary_node]
        for i in range(1, replica_count):
            next_idx = (primary_token_idx + i) % len(self.ring_positions)
            replica_node = self.vnodes[self.ring_positions[next_idx]]
            replicas.append(replica_node)
        
        return replicas

# 分布式数据库协调器
class DistributedDBCoordinator:
    """分布式数据库协调器"""
    
    def __init__(self, replica_factor: int = 3):
        self.nodes: Dict[str, DistributedDBNode] = {}
        self.vnode_ring = VirtualNodeRing()
        self.replica_factor = replica_factor
        self.consistency_manager = ConsistencyManager()
    
    def add_node(self, node: DataNode):
        """添加数据节点"""
        db_node = DistributedDBNode(node)
        self.nodes[node.node_id] = db_node
        self.vnode_ring.add_node(node)
        print(f"Added node {node.node_id} to cluster")
    
    def remove_node(self, node_id: str):
        """移除数据节点"""
        if node_id in self.nodes:
            node = self.nodes[node_id].node_info
            self.vnode_ring.remove_node(node)
            del self.nodes[node_id]
            print(f"Removed node {node_id} from cluster")
    
    async def write(self, key: str, value: Any, 
                   consistency_level: ConsistencyLevel = ConsistencyLevel.QUORUM) -> bool:
        """分布式写操作"""
        # 创建分区键
        partition_key = PartitionKey(value=key)
        
        # 获取副本节点
        replica_nodes = self.vnode_ring.get_replica_nodes(partition_key, self.replica_factor)
        
        if not replica_nodes:
            raise ValueError("No replica nodes available")
        
        timestamp = datetime.utcnow()
        success_count = 0
        
        # 向所有副本节点写入
        for replica_node in replica_nodes:
            if replica_node.node_id in self.nodes:
                db_node = self.nodes[replica_node.node_id]
                success = await db_node.handle_write(key, value, timestamp)
                if success:
                    success_count += 1
        
        # 检查一致性要求
        required_successes = self._get_required_successes(len(replica_nodes), consistency_level)
        
        if success_count >= required_successes:
            print(f"Write successful: {success_count}/{len(replica_nodes)} replicas acknowledged")
            return True
        else:
            print(f"Write failed: only {success_count}/{len(replica_nodes)} replicas acknowledged")
            return False
    
    async def read(self, key: str, 
                  consistency_level: ConsistencyLevel = ConsistencyLevel.QUORUM) -> Optional[Any]:
        """分布式读操作"""
        # 创建分区键
        partition_key = PartitionKey(value=key)
        
        # 获取副本节点
        replica_nodes = self.vnode_ring.get_replica_nodes(partition_key, self.replica_factor)
        
        if not replica_nodes:
            return None
        
        read_results = []
        
        # 从所有副本节点读取
        for replica_node in replica_nodes:
            if replica_node.node_id in self.nodes:
                db_node = self.nodes[replica_node.node_id]
                result = await db_node.handle_read(key, consistency_level)
                if result is not None:
                    read_results.append(result)
        
        # 返回最新版本的数据
        if read_results:
            # 简化版本选择逻辑，实际应该基于时间戳和版本号
            return read_results[0]
        
        return None
    
    def _get_required_successes(self, total_replicas: int, 
                               consistency_level: ConsistencyLevel) -> int:
        """获取需要的成功响应数"""
        if consistency_level == ConsistencyLevel.ONE:
            return 1
        elif consistency_level == ConsistencyLevel.QUORUM:
            return (total_replicas // 2) + 1
        elif consistency_level == ConsistencyLevel.ALL:
            return total_replicas
        else:
            return (total_replicas // 2) + 1  # 默认quorum
    
    def get_cluster_info(self) -> Dict[str, Any]:
        """获取集群信息"""
        return {
            "total_nodes": len(self.nodes),
            "replica_factor": self.replica_factor,
            "ring_tokens": len(self.vnode_ring.ring_positions),
            "nodes": [
                {
                    "node_id": node.node_info.node_id,
                    "host": node.node_info.host,
                    "state": node.node_info.state.value,
                    "load": node.node_info.load
                }
                for node in self.nodes.values()
            ]
        }

class ConsistencyManager:
    """一致性管理器"""
    
    def __init__(self):
        self.vector_clocks = {}  # 节点ID -> 版本号
    
    def update_vector_clock(self, node_id: str):
        """更新向量时钟"""
        if node_id not in self.vector_clocks:
            self.vector_clocks[node_id] = 0
        self.vector_clocks[node_id] += 1
    
    def compare_versions(self, version1: Dict[str, int], version2: Dict[str, int]) -> int:
        """比较版本：-1表示version1<version2，0表示相等，1表示version1>version2"""
        # 简化的向量时钟比较
        if version1 == version2:
            return 0
        
        # 检查是否为祖先关系
        is_ancestor = all(
            version1.get(node_id, 0) <= version2.get(node_id, float('inf'))
            for node_id in version2
        )
        
        is_descendant = all(
            version1.get(node_id, float('inf')) >= version2.get(node_id, 0)
            for node_id in version1
        )
        
        if is_ancestor and not is_descendant:
            return -1  # version1是version2的祖先
        elif is_descendant and not is_ancestor:
            return 1   # version1是version2的后代
        else:
            return 0   # 并行版本，需要合并

# 使用示例
async def demo_distributed_db():
    """演示分布式数据库"""
    
    # 创建分布式数据库协调器
    db_coordinator = DistributedDBCoordinator(replica_factor=3)
    
    # 创建并添加数据节点
    nodes_data = [
        ("node-1", "192.168.1.1", 8000, "dc1", "rack1"),
        ("node-2", "192.168.1.2", 8000, "dc1", "rack1"),
        ("node-3", "192.168.1.3", 8000, "dc1", "rack2"),
        ("node-4", "192.168.1.4", 8000, "dc1", "rack2"),
        ("node-5", "192.168.1.5", 8000, "dc2", "rack1")
    ]
    
    for node_data in nodes_data:
        node = DataNode(*node_data)
        db_coordinator.add_node(node)
        
        # 启动节点
        db_node = db_coordinator.nodes[node.node_id]
        await db_node.start()
    
    print(f"\nCluster initialized with {len(db_coordinator.nodes)} nodes")
    
    # 执行分布式写入
    print("\nPerforming distributed writes...")
    test_keys = ["user:001", "user:002", "order:001", "order:002", "product:001"]
    
    for key in test_keys:
        value = {
            "data": f"content for {key}",
            "timestamp": datetime.utcnow().isoformat(),
            "source": "demo_app"
        }
        
        success = await db_coordinator.write(key, value)
        print(f"Write {key}: {'Success' if success else 'Failed'}")
    
    # 执行分布式读取
    print("\nPerforming distributed reads...")
    for key in test_keys[:3]:
        result = await db_coordinator.read(key)
        if result:
            print(f"Read {key}: {result.get('data', 'No data')}")
        else:
            print(f"Read {key}: Not found")
    
    # 显示集群信息
    print("\nCluster Information:")
    cluster_info = db_coordinator.get_cluster_info()
    print(json.dumps(cluster_info, indent=2, default=str))
```

## 2. 分布式事务处理

### 2.1 两阶段提交协议(2PC)

```python
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum
import uuid
import time

class TransactionState(Enum):
    """事务状态"""
    INITIATED = "initiated"
    PREPARING = "preparing"
    PREPARED = "prepared"
    COMMITTING = "committing"
    COMMITTED = "committed"
    ABORTING = "aborting"
    ABORTED = "aborted"

@dataclass
class TransactionOperation:
    """事务操作"""
    operation_id: str
    resource_id: str  # 数据库节点ID
    operation_type: str  # "read" or "write"
    key: str
    value: Any = None

@dataclass
class Transaction:
    """分布式事务"""
    transaction_id: str
    coordinator_id: str
    operations: List[TransactionOperation] = field(default_factory=list)
    state: TransactionState = TransactionState.INITIATED
    participants: List[str] = field(default_factory=list)  # 参与者节点ID
    created_at: float = field(default_factory=time.time)
    timeout_seconds: int = 30

class TwoPhaseCommitCoordinator:
    """两阶段提交协调器"""
    
    def __init__(self, coordinator_id: str):
        self.coordinator_id = coordinator_id
        self.active_transactions: Dict[str, Transaction] = {}
        self.participant_responses: Dict[str, Dict[str, bool]] = {}  # transaction_id -> participant_id -> prepared
        
    async def begin_transaction(self) -> str:
        """开始事务"""
        transaction_id = str(uuid.uuid4())
        
        transaction = Transaction(
            transaction_id=transaction_id,
            coordinator_id=self.coordinator_id
        )
        
        self.active_transactions[transaction_id] = transaction
        print(f"Transaction {transaction_id} initiated by {self.coordinator_id}")
        
        return transaction_id
    
    async def add_operation(self, transaction_id: str, operation: TransactionOperation):
        """添加事务操作"""
        if transaction_id not in self.active_transactions:
            raise ValueError(f"Transaction {transaction_id} not found")
        
        transaction = self.active_transactions[transaction_id]
        transaction.operations.append(operation)
        
        # 添加参与者
        if operation.resource_id not in transaction.participants:
            transaction.participants.append(operation.resource_id)
    
    async def commit_transaction(self, transaction_id: str) -> bool:
        """提交事务"""
        if transaction_id not in self.active_transactions:
            return False
        
        transaction = self.active_transactions[transaction_id]
        print(f"Starting 2PC for transaction {transaction_id}")
        
        # 第一阶段：准备阶段
        if not await self._prepare_phase(transaction):
            await self._abort_transaction(transaction_id)
            return False
        
        # 第二阶段：提交阶段
        success = await self._commit_phase(transaction)
        
        # 清理事务
        del self.active_transactions[transaction_id]
        if transaction_id in self.participant_responses:
            del self.participant_responses[transaction_id]
        
        return success
    
    async def _prepare_phase(self, transaction: Transaction) -> bool:
        """准备阶段"""
        print(f"Prepare phase for transaction {transaction.transaction_id}")
        
        transaction.state = TransactionState.PREPARING
        
        # 初始化参与者响应记录
        self.participant_responses[transaction.transaction_id] = {
            participant: False for participant in transaction.participants
        }
        
        # 发送准备请求给所有参与者
        prepare_tasks = []
        for participant_id in transaction.participants:
            task = self._send_prepare_request(transaction, participant_id)
            prepare_tasks.append(task)
        
        # 等待所有参与者的响应
        prepare_results = await asyncio.gather(*prepare_tasks, return_exceptions=True)
        
        # 检查所有参与者是否准备好
        all_prepared = all(result is True for result in prepare_results)
        
        if all_prepared:
            transaction.state = TransactionState.PREPARED
            print(f"All participants prepared for transaction {transaction.transaction_id}")
        else:
            print(f"Failed to prepare all participants for transaction {transaction.transaction_id}")
        
        return all_prepared
    
    async def _commit_phase(self, transaction: Transaction) -> bool:
        """提交阶段"""
        print(f"Commit phase for transaction {transaction.transaction_id}")
        
        transaction.state = TransactionState.COMMITTING
        
        # 发送提交请求给所有参与者
        commit_tasks = []
        for participant_id in transaction.participants:
            task = self._send_commit_request(transaction, participant_id)
            commit_tasks.append(task)
        
        # 等待所有参与者的响应
        commit_results = await asyncio.gather(*commit_tasks, return_exceptions=True)
        
        # 检查提交结果
        all_committed = all(result is True for result in commit_results)
        
        if all_committed:
            transaction.state = TransactionState.COMMITTED
            print(f"Transaction {transaction.transaction_id} committed successfully")
        else:
            print(f"Failed to commit transaction {transaction.transaction_id}")
        
        return all_committed
    
    async def _send_prepare_request(self, transaction: Transaction, participant_id: str) -> bool:
        """发送准备请求给参与者"""
        print(f"Sending prepare request to participant {participant_id}")
        
        # 模拟网络延迟和参与者处理
        await asyncio.sleep(0.1)
        
        # 模拟参与者准备逻辑
        operations_for_participant = [
            op for op in transaction.operations 
            if op.resource_id == participant_id
        ]
        
        # 检查参与者是否能够执行这些操作
        can_prepare = await self._participant_can_prepare(participant_id, operations_for_participant)
        
        if can_prepare:
            self.participant_responses[transaction.transaction_id][participant_id] = True
            print(f"Participant {participant_id} prepared for transaction {transaction.transaction_id}")
        else:
            print(f"Participant {participant_id} cannot prepare for transaction {transaction.transaction_id}")
        
        return can_prepare
    
    async def _send_commit_request(self, transaction: Transaction, participant_id: str) -> bool:
        """发送提交请求给参与者"""
        print(f"Sending commit request to participant {participant_id}")
        
        # 模拟网络延迟
        await asyncio.sleep(0.1)
        
        # 模拟参与者提交逻辑
        operations_for_participant = [
            op for op in transaction.operations 
            if op.resource_id == participant_id
        ]
        
        # 执行提交操作
        commit_success = await self._participant_commit(participant_id, operations_for_participant)
        
        if commit_success:
            print(f"Participant {participant_id} committed for transaction {transaction.transaction_id}")
        else:
            print(f"Participant {participant_id} failed to commit for transaction {transaction.transaction_id}")
        
        return commit_success
    
    async def _participant_can_prepare(self, participant_id: str, 
                                     operations: List[TransactionOperation]) -> bool:
        """检查参与者是否能够准备"""
        # 简化的参与者准备检查
        # 实际实现中需要检查锁、磁盘空间、资源可用性等
        
        # 模拟检查过程
        await asyncio.sleep(0.05)
        
        # 90%的成功率
        import random
        return random.random() > 0.1
    
    async def _participant_commit(self, participant_id: str, 
                                operations: List[TransactionOperation]) -> bool:
        """参与者执行提交"""
        # 简化的提交执行
        # 实际实现中需要执行实际的数据库操作
        
        await asyncio.sleep(0.05)
        
        # 95%的成功率
        import random
        return random.random() > 0.05
    
    async def _abort_transaction(self, transaction_id: str):
        """中止事务"""
        if transaction_id in self.active_transactions:
            transaction = self.active_transactions[transaction_id]
            transaction.state = TransactionState.ABORTED
            
            # 通知所有参与者中止
            for participant_id in transaction.participants:
                await self._send_abort_request(transaction, participant_id)
            
            print(f"Transaction {transaction_id} aborted")
    
    async def _send_abort_request(self, transaction: Transaction, participant_id: str):
        """发送中止请求给参与者"""
        print(f"Sending abort request to participant {participant_id}")
        
        await asyncio.sleep(0.05)
        print(f"Participant {participant_id} aborted transaction {transaction.transaction_id}")

# 使用示例
async def demo_two_phase_commit():
    """演示两阶段提交协议"""
    
    # 创建协调器
    coordinator = TwoPhaseCommitCoordinator("coordinator-001")
    
    # 开始事务
    transaction_id = await coordinator.begin_transaction()
    
    # 添加事务操作
    operations = [
        TransactionOperation(
            operation_id="op-001",
            resource_id="database-node-1",
            operation_type="write",
            key="account:001",
            value={"balance": 1000, "action": "deposit"}
        ),
        TransactionOperation(
            operation_id="op-002", 
            resource_id="database-node-2",
            operation_type="write",
            key="account:002", 
            value={"balance": 500, "action": "withdraw"}
        ),
        TransactionOperation(
            operation_id="op-003",
            resource_id="database-node-1", 
            operation_type="write",
            key="transaction:001",
            value={"amount": 100, "from": "002", "to": "001"}
        )
    ]
    
    for operation in operations:
        await coordinator.add_operation(transaction_id, operation)
    
    print(f"Added {len(operations)} operations to transaction {transaction_id}")
    
    # 提交事务
    success = await coordinator.commit_transaction(transaction_id)
    
    print(f"Transaction commit result: {'Success' if success else 'Failed'}")
```

## 3. 分布式查询优化

### 3.1 查询执行引擎

```python
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import asyncio

class QueryType(Enum):
    """查询类型"""
    SELECT = "select"
    INSERT = "insert"
    UPDATE = "update"
    DELETE = "delete"
    AGGREGATE = "aggregate"
    JOIN = "join"

class JoinType(Enum):
    """连接类型"""
    INNER_JOIN = "inner_join"
    LEFT_JOIN = "left_join"
    RIGHT_JOIN = "right_join"
    FULL_JOIN = "full_join"
    HASH_JOIN = "hash_join"
    SORT_MERGE_JOIN = "sort_merge_join"

@dataclass
class QueryPlan:
    """查询执行计划"""
    plan_id: str
    query_type: QueryType
    tables: List[str]
    conditions: Dict[str, Any]
    projections: List[str]
    limit: Optional[int] = None
    offset: Optional[int] = None
    join_operations: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class QueryFragment:
    """查询片段"""
    fragment_id: str
    node_id: str  # 执行节点
    query_plan: QueryPlan
    dependencies: List[str] = field(default_factory=list)
    estimated_cost: float = 0.0

@dataclass
class QueryResult:
    """查询结果"""
    fragment_id: str
    node_id: str
    data: List[Dict[str, Any]]
    row_count: int
    execution_time: float
    metadata: Dict[str, Any] = field(default_factory=dict)

class DistributedQueryOptimizer:
    """分布式查询优化器"""
    
    def __init__(self, cluster_info: Dict[str, Any]):
        self.cluster_info = cluster_info
        self.statistics = {}  # 表统计信息
        self.query_history = []  # 查询历史
        
    def optimize_query(self, query: QueryPlan) -> List[QueryFragment]:
        """优化查询"""
        print(f"Optimizing query {query.plan_id}")
        
        # 1. 谓词下推
        query = self._push_down_predicates(query)
        
        # 2. 选择投影列
        query = self._select_columns(query)
        
        # 3. 生成执行片段
        fragments = self._generate_fragments(query)
        
        # 4. 优化片段执行顺序
        fragments = self._optimize_execution_order(fragments)
        
        # 5. 估算成本
        fragments = self._estimate_costs(fragments)
        
        print(f"Generated {len(fragments)} query fragments")
        return fragments
    
    def _push_down_predicates(self, query: QueryPlan) -> QueryPlan:
        """谓词下推优化"""
        # 简化实现：识别可下推的条件
        pushable_conditions = {}
        remaining_conditions = {}
        
        for key, condition in query.conditions.items():
            if self._can_push_down_condition(condition):
                pushable_conditions[key] = condition
            else:
                remaining_conditions[key] = condition
        
        # 更新查询计划
        query.conditions = remaining_conditions
        
        print(f"Predicate pushdown: {len(pushable_conditions)} conditions pushed down")
        return query
    
    def _can_push_down_condition(self, condition: Any) -> bool:
        """判断条件是否可以下推"""
        # 简化实现：基于条件类型判断
        return isinstance(condition, dict) and condition.get("type") in ["equality", "range"]
    
    def _select_columns(self, query: QueryPlan) -> QueryPlan:
        """列选择优化"""
        if query.projections and query.projections != ["*"]:
            # 保持指定的投影列
            print(f"Column selection: {len(query.projections)} columns projected")
        else:
            # 简化：默认投影列
            query.projections = ["id", "name", "created_at"]
            print("Column selection: using default columns")
        
        return query
    
    def _generate_fragments(self, query: QueryPlan) -> List[QueryFragment]:
        """生成查询片段"""
        fragments = []
        
        if query.query_type == QueryType.SELECT:
            # 为每个表生成扫描片段
            for table in query.tables:
                fragment = QueryFragment(
                    fragment_id=f"scan_{table}_{query.plan_id}",
                    node_id=self._select_best_node(table),
                    query_plan=query
                )
                fragments.append(fragment)
            
            # 如果有连接操作，生成连接片段
            if query.join_operations:
                join_fragment = QueryFragment(
                    fragment_id=f"join_{query.plan_id}",
                    node_id=self._select_coordinator_node(),
                    query_plan=query,
                    dependencies=[f.fragment_id for f in fragments]
                )
                fragments.append(join_fragment)
        
        elif query.query_type == QueryType.INSERT:
            # 插入操作的目标节点
            target_table = query.tables[0]  # 简化假设
            fragment = QueryFragment(
                fragment_id=f"insert_{target_table}_{query.plan_id}",
                node_id=self._select_target_node(target_table),
                query_plan=query
            )
            fragments.append(fragment)
        
        return fragments
    
    def _select_best_node(self, table: str) -> str:
        """选择最佳执行节点"""
        # 简化的节点选择：基于表的数据分布
        nodes = self.cluster_info.get("nodes", [])
        if not nodes:
            return "coordinator"
        
        # 选择负载最低的节点
        best_node = min(nodes, key=lambda x: x.get("load", 0))
        return best_node["node_id"]
    
    def _select_coordinator_node(self) -> str:
        """选择协调器节点"""
        return "coordinator"
    
    def _select_target_node(self, table: str) -> str:
        """选择目标节点"""
        # 简化实现：随机选择节点
        import random
        nodes = self.cluster_info.get("nodes", [])
        if nodes:
            return random.choice(nodes)["node_id"]
        return "coordinator"
    
    def _optimize_execution_order(self, fragments: List[QueryFragment]) -> List[QueryFragment]:
        """优化执行顺序"""
        # 拓扑排序：依赖的片段先执行
        execution_order = []
        processed = set()
        
        def process_fragment(fragment: QueryFragment):
            if fragment.fragment_id in processed:
                return
            
            # 先处理依赖的片段
            for dep_id in fragment.dependencies:
                dep_fragment = next((f for f in fragments if f.fragment_id == dep_id), None)
                if dep_fragment:
                    process_fragment(dep_fragment)
            
            execution_order.append(fragment)
            processed.add(fragment.fragment_id)
        
        # 从叶子节点开始
        for fragment in fragments:
            if not fragment.dependencies:
                process_fragment(fragment)
        
        print(f"Optimized execution order: {len(execution_order)} fragments")
        return execution_order
    
    def _estimate_costs(self, fragments: List[QueryFragment]) -> List[QueryFragment]:
        """估算成本"""
        for fragment in fragments:
            # 简化的成本估算
            base_cost = 1.0
            
            # 根据查询类型调整成本
            if fragment.query_plan.query_type == QueryType.AGGREGATE:
                base_cost *= 2.0
            elif fragment.query_plan.query_type == QueryType.JOIN:
                base_cost *= 5.0
            
            fragment.estimated_cost = base_cost
        
        return fragments

class DistributedQueryExecutor:
    """分布式查询执行器"""
    
    def __init__(self, coordinator_id: str):
        self.coordinator_id = coordinator_id
        self.node_executors = {}  # node_id -> executor
        self.execution_results = {}
        
    def register_node_executor(self, node_id: str, executor: 'NodeExecutor'):
        """注册节点执行器"""
        self.node_executors[node_id] = executor
        print(f"Registered executor for node {node_id}")
    
    async def execute_query(self, fragments: List[QueryFragment]) -> List[QueryResult]:
        """执行分布式查询"""
        print(f"Executing distributed query with {len(fragments)} fragments")
        
        execution_tasks = []
        
        # 按依赖顺序执行片段
        executed_fragments = set()
        ready_fragments = [f for f in fragments if not f.dependencies]
        
        while ready_fragments or len(executed_fragments) < len(fragments):
            # 执行就绪的片段
            for fragment in ready_fragments:
                task = self._execute_fragment(fragment)
                execution_tasks.append((fragment, task))
            
            # 等待当前批次的片段完成
            if execution_tasks:
                results = await asyncio.gather(
                    *[task for _, task in execution_tasks],
                    return_exceptions=True
                )
                
                # 处理结果
                for (fragment, _), result in zip(execution_tasks, results):
                    if isinstance(result, QueryResult):
                        self.execution_results[fragment.fragment_id] = result
                        executed_fragments.add(fragment.fragment_id)
                        print(f"Fragment {fragment.fragment_id} completed with {result.row_count} rows")
                    else:
                        print(f"Fragment {fragment.fragment_id} failed: {result}")
                
                execution_tasks.clear()
            
            # 找到下一批就绪的片段
            ready_fragments = []
            for fragment in fragments:
                if (fragment.fragment_id not in executed_fragments and
                    all(dep in executed_fragments for dep in fragment.dependencies)):
                    ready_fragments.append(fragment)
        
        return list(self.execution_results.values())
    
    async def _execute_fragment(self, fragment: QueryFragment) -> QueryResult:
        """执行单个查询片段"""
        print(f"Executing fragment {fragment.fragment_id} on node {fragment.node_id}")
        
        start_time = time.time()
        
        # 获取节点执行器
        executor = self.node_executors.get(fragment.node_id)
        if not executor:
            # 如果没有对应的执行器，使用协调器执行
            executor = self
        
        # 执行查询
        result = await executor._execute_on_node(fragment)
        
        execution_time = time.time() - start_time
        result.execution_time = execution_time
        
        return result
    
    async def _execute_on_node(self, fragment: QueryFragment) -> QueryResult:
        """在节点上执行查询（协调器实现）"""
        # 模拟执行结果
        import random
        
        sample_data = []
        for i in range(fragment.query_plan.limit or random.randint(1, 100)):
            record = {
                "id": i + 1,
                "name": f"Record {i + 1}",
                "created_at": datetime.utcnow().isoformat()
            }
            # 添加查询条件中的字段
            for key, value in fragment.query_plan.conditions.items():
                if isinstance(value, str):
                    record[key] = value
            sample_data.append(record)
        
        return QueryResult(
            fragment_id=fragment.fragment_id,
            node_id=self.coordinator_id,
            data=sample_data,
            row_count=len(sample_data),
            execution_time=0.0,
            metadata={
                "query_type": fragment.query_plan.query_type.value,
                "tables": fragment.query_plan.tables
            }
        )

class NodeExecutor:
    """节点执行器（模拟数据库节点）"""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.local_data = {}
    
    async def execute_query(self, fragment: QueryFragment) -> QueryResult:
        """在本地执行查询"""
        start_time = time.time()
        
        # 模拟数据访问和查询执行
        await asyncio.sleep(0.1)  # 模拟I/O延迟
        
        # 生成模拟结果
        row_count = random.randint(1, 50)
        data = []
        
        for i in range(row_count):
            record = {
                "node_id": self.node_id,
                "fragment_id": fragment.fragment_id,
                "record_id": i + 1
            }
            data.append(record)
        
        execution_time = time.time() - start_time
        
        return QueryResult(
            fragment_id=fragment.fragment_id,
            node_id=self.node_id,
            data=data,
            row_count=row_count,
            execution_time=execution_time
        )

# 使用示例
async def demo_distributed_query():
    """演示分布式查询"""
    
    # 模拟集群信息
    cluster_info = {
        "nodes": [
            {"node_id": "node-1", "load": 0.3, "tables": ["users", "orders"]},
            {"node_id": "node-2", "load": 0.5, "tables": ["products", "categories"]},
            {"node_id": "node-3", "load": 0.2, "tables": ["users", "products"]},
        ]
    }
    
    # 创建查询优化器和执行器
    optimizer = DistributedQueryOptimizer(cluster_info)
    executor = DistributedQueryExecutor("coordinator-001")
    
    # 注册节点执行器
    for node_info in cluster_info["nodes"]:
        node_executor = NodeExecutor(node_info["node_id"])
        executor.register_node_executor(node_info["node_id"], node_executor)
    
    # 创建复杂查询
    query = QueryPlan(
        plan_id="query-001",
        query_type=QueryType.SELECT,
        tables=["users", "orders"],
        conditions={
            "users.created_at": {"type": "range", "start": "2023-01-01", "end": "2023-12-31"},
            "orders.status": {"type": "equality", "value": "completed"}
        },
        projections=["users.id", "users.name", "orders.total_amount", "orders.created_at"],
        limit=1000,
        join_operations=[
            {
                "type": "inner_join",
                "left_table": "users",
                "right_table": "orders",
                "condition": "users.id = orders.user_id"
            }
        ]
    )
    
    # 优化查询
    print("=== Query Optimization ===")
    fragments = optimizer.optimize_query(query)
    
    # 显示执行计划
    print("\nExecution Plan:")
    for fragment in fragments:
        print(f"  Fragment {fragment.fragment_id}:")
        print(f"    Node: {fragment.node_id}")
        print(f"    Dependencies: {fragment.dependencies}")
        print(f"    Estimated Cost: {fragment.estimated_cost}")
    
    # 执行查询
    print("\n=== Query Execution ===")
    results = await executor.execute_query(fragments)
    
    # 显示结果
    print("\nExecution Results:")
    total_rows = 0
    total_time = 0.0
    
    for result in results:
        total_rows += result.row_count
        total_time += result.execution_time
        print(f"  Fragment {result.fragment_id}: {result.row_count} rows in {result.execution_time:.3f}s")
    
    print(f"\nTotal: {total_rows} rows in {total_time:.3f}s")
```

## 4. 分布式数据库高可用设计

### 4.1 故障检测与自动故障转移

```python
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import time
import json
from datetime import datetime, timedelta

class FailureType(Enum):
    """故障类型"""
    NODE_DOWN = "node_down"
    NETWORK_PARTITION = "network_partition"
    SLOW_NODE = "slow_node"
    DATA_CORRUPTION = "data_corruption"

class NodeRole(Enum):
    """节点角色"""
    PRIMARY = "primary"        # 主节点
    SECONDARY = "secondary"    # 副本节点
    ARBITER = "arbiter"        # 仲裁节点
    OBSERVER = "observer"      # 观察者节点

@dataclass
class HealthCheckResult:
    """健康检查结果"""
    node_id: str
    is_healthy: bool
    response_time: float
    error_message: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.utcnow)

@dataclass
class FailureDetectionEvent:
    """故障检测事件"""
    event_id: str
    node_id: str
    failure_type: FailureType
    detection_time: datetime
    severity: str  # "low", "medium", "high", "critical"
    description: str
    metadata: Dict[str, Any] = field(default_factory=dict)

class HealthMonitor:
    """健康监控器"""
    
    def __init__(self, check_interval: int = 10):
        self.check_interval = check_interval
        self.node_status: Dict[str, HealthCheckResult] = {}
        self.check_functions: Dict[str, Callable] = {}
        self.monitoring_tasks: Dict[str, asyncio.Task] = {}
        self.failure_history: List[FailureDetectionEvent] = []
        
    def register_node(self, node_id: str, check_function: Callable):
        """注册节点健康检查"""
        self.check_functions[node_id] = check_function
        print(f"Registered health check for node {node_id}")
    
    async def start_monitoring(self, node_id: str):
        """开始监控节点"""
        if node_id in self.monitoring_tasks:
            print(f"Node {node_id} is already being monitored")
            return
        
        # 创建监控任务
        task = asyncio.create_task(self._monitor_node(node_id))
        self.monitoring_tasks[node_id] = task
        print(f"Started monitoring node {node_id}")
    
    async def stop_monitoring(self, node_id: str):
        """停止监控节点"""
        if node_id in self.monitoring_tasks:
            self.monitoring_tasks[node_id].cancel()
            del self.monitoring_tasks[node_id]
            print(f"Stopped monitoring node {node_id}")
    
    async def _monitor_node(self, node_id: str):
        """监控节点健康状态"""
        while True:
            try:
                start_time = time.time()
                
                # 执行健康检查
                check_function = self.check_functions.get(node_id)
                if check_function:
                    is_healthy = await check_function(node_id)
                else:
                    is_healthy = True  # 默认健康
                
                response_time = time.time() - start_time
                
                # 记录检查结果
                result = HealthCheckResult(
                    node_id=node_id,
                    is_healthy=is_healthy,
                    response_time=response_time
                )
                
                self.node_status[node_id] = result
                
                # 检查是否需要告警
                if not is_healthy:
                    await self._handle_unhealthy_node(node_id, "Health check failed")
                
                # 等待下次检查
                await asyncio.sleep(self.check_interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Error monitoring node {node_id}: {e}")
                await asyncio.sleep(self.check_interval)
    
    async def _handle_unhealthy_node(self, node_id: str, description: str):
        """处理不健康的节点"""
        print(f"Unhealthy node detected: {node_id} - {description}")
        
        # 创建故障检测事件
        failure_event = FailureDetectionEvent(
            event_id=f"fail_{node_id}_{int(time.time())}",
            node_id=node_id,
            failure_type=FailureType.NODE_DOWN,
            detection_time=datetime.utcnow(),
            severity="high",
            description=description
        )
        
        self.failure_history.append(failure_event)

class FailureDetector:
    """故障检测器"""
    
    def __init__(self, health_monitor: HealthMonitor):
        self.health_monitor = health_monitor
        self.failure_threshold = 3  # 连续失败次数阈值
        self.timeout_threshold = 5.0  # 响应时间阈值（秒）
        self.detection_handlers: List[Callable] = []
        
    def register_failure_handler(self, handler: Callable):
        """注册故障处理器"""
        self.detection_handlers.append(handler)
    
    async def detect_failures(self):
        """检测故障"""
        while True:
            try:
                for node_id, status in self.health_monitor.node_status.items():
                    # 检查响应时间超时
                    if status.response_time > self.timeout_threshold:
                        await self._trigger_failure_detection(
                            node_id, FailureType.SLOW_NODE, 
                            f"Response time {status.response_time:.2f}s exceeds threshold"
                        )
                    
                    # 检查连续失败次数
                    consecutive_failures = self._count_consecutive_failures(node_id)
                    if consecutive_failures >= self.failure_threshold:
                        await self._trigger_failure_detection(
                            node_id, FailureType.NODE_DOWN,
                            f"{consecutive_failures} consecutive failures detected"
                        )
                
                await asyncio.sleep(1)  # 每秒检查一次
                
            except Exception as e:
                print(f"Error in failure detection: {e}")
                await asyncio.sleep(1)
    
    def _count_consecutive_failures(self, node_id: str) -> int:
        """计算连续失败次数"""
        failures = 0
        recent_events = [
            event for event in self.health_monitor.failure_history
            if event.node_id == node_id and 
            (datetime.utcnow() - event.detection_time).seconds < 300  # 最近5分钟
        ]
        
        for event in recent_events:
            if event.failure_type == FailureType.NODE_DOWN:
                failures += 1
            else:
                break  # 重置连续失败计数
        
        return failures
    
    async def _trigger_failure_detection(self, node_id: str, failure_type: FailureType, description: str):
        """触发故障检测"""
        failure_event = FailureDetectionEvent(
            event_id=f"detected_{node_id}_{int(time.time())}",
            node_id=node_id,
            failure_type=failure_type,
            detection_time=datetime.utcnow(),
            severity="high" if failure_type == FailureType.NODE_DOWN else "medium",
            description=description
        )
        
        # 调用故障处理器
        for handler in self.detection_handlers:
            try:
                await handler(failure_event)
            except Exception as e:
                print(f"Error in failure handler: {e}")

class AutoFailoverManager:
    """自动故障转移管理器"""
    
    def __init__(self, cluster_nodes: Dict[str, Dict[str, Any]]):
        self.cluster_nodes = cluster_nodes
        self.primary_nodes: Dict[str, str] = {}  # service -> primary_node
        self.failover_history: List[Dict[str, Any]] = []
        
    async def handle_failure(self, failure_event: FailureDetectionEvent):
        """处理故障事件"""
        print(f"Handling failure event: {failure_event.node_id} - {failure_event.failure_type.value}")
        
        # 检查是否有服务以故障节点为主节点
        affected_services = [
            service for service, primary in self.primary_nodes.items()
            if primary == failure_event.node_id
        ]
        
        for service in affected_services:
            success = await self._perform_failover(service, failure_event.node_id)
            if success:
                print(f"Failover successful for service {service}")
            else:
                print(f"Failover failed for service {service}")
    
    async def _perform_failover(self, service: str, failed_node: str) -> bool:
        """执行故障转移"""
        print(f"Starting failover for service {service} from failed node {failed_node}")
        
        # 找到最佳副本节点
        new_primary = await self._select_best_replica(service, failed_node)
        
        if not new_primary:
            print(f"No suitable replica found for service {service}")
            return False
        
        # 执行故障转移
        failover_success = await self._execute_failover(service, failed_node, new_primary)
        
        if failover_success:
            # 更新主节点映射
            old_primary = self.primary_nodes.get(service)
            self.primary_nodes[service] = new_primary
            
            # 记录故障转移历史
            self.failover_history.append({
                "service": service,
                "failed_node": failed_node,
                "new_primary": new_primary,
                "timestamp": datetime.utcnow().isoformat(),
                "old_primary": old_primary
            })
            
            print(f"Failover completed: {service} -> {new_primary}")
            return True
        
        return False
    
    async def _select_best_replica(self, service: str, failed_node: str) -> Optional[str]:
        """选择最佳副本节点"""
        candidates = []
        
        for node_id, node_info in self.cluster_nodes.items():
            if node_id == failed_node:
                continue
            
            # 检查节点健康状态
            if not await self._is_node_healthy(node_id):
                continue
            
            # 检查节点是否包含服务数据
            if self._node_has_service_data(node_id, service):
                # 计算节点评分
                score = self._calculate_node_score(node_id, node_info)
                candidates.append((node_id, score))
        
        # 选择评分最高的节点
        if candidates:
            best_node = max(candidates, key=lambda x: x[1])
            return best_node[0]
        
        return None
    
    async def _is_node_healthy(self, node_id: str) -> bool:
        """检查节点健康状态"""
        # 简化实现：检查节点是否在集群中且未标记为故障
        return node_id in self.cluster_nodes
    
    def _node_has_service_data(self, node_id: str, service: str) -> bool:
        """检查节点是否包含服务数据"""
        node_info = self.cluster_nodes.get(node_id, {})
        services = node_info.get("services", [])
        return service in services
    
    def _calculate_node_score(self, node_id: str, node_info: Dict[str, Any]) -> float:
        """计算节点评分"""
        score = 0.0
        
        # 基于负载评分
        load = node_info.get("load", 0.0)
        score += (1.0 - load) * 40
        
        # 基于性能评分
        performance = node_info.get("performance_score", 0.5)
        score += performance * 30
        
        # 基于网络延迟评分
        network_latency = node_info.get("network_latency_ms", 100)
        score += max(0, (100 - network_latency)) * 0.2
        
        # 基于数据同步状态评分
        sync_status = node_info.get("sync_status", "behind")
        if sync_status == "up_to_date":
            score += 20
        elif sync_status == "near_realtime":
            score += 10
        
        return score
    
    async def _execute_failover(self, service: str, failed_node: str, new_primary: str) -> bool:
        """执行故障转移操作"""
        print(f"Executing failover: {service} {failed_node} -> {new_primary}")
        
        try:
            # 1. 停止向故障节点写入
            # 2. 确保新主节点数据同步完成
            # 3. 更新路由表
            # 4. 通知客户端新的主节点
            
            await asyncio.sleep(0.5)  # 模拟故障转移操作
            
            return True
            
        except Exception as e:
            print(f"Failover execution failed: {e}")
            return False
    
    def get_failover_statistics(self) -> Dict[str, Any]:
        """获取故障转移统计信息"""
        return {
            "total_failovers": len(self.failover_history),
            "recent_failovers": self.failover_history[-5:],  # 最近5次
            "success_rate": self._calculate_success_rate()
        }
    
    def _calculate_success_rate(self) -> float:
        """计算故障转移成功率"""
        if not self.failover_history:
            return 1.0
        
        # 简化计算：假设所有故障转移都成功
        return 1.0

# 使用示例
async def demo_high_availability():
    """演示高可用设计"""
    
    # 模拟集群节点
    cluster_nodes = {
        "node-1": {
            "load": 0.3,
            "performance_score": 0.9,
            "network_latency_ms": 10,
            "sync_status": "up_to_date",
            "services": ["user_service", "order_service"]
        },
        "node-2": {
            "load": 0.5,
            "performance_score": 0.8,
            "network_latency_ms": 15,
            "sync_status": "up_to_date",
            "services": ["order_service", "product_service"]
        },
        "node-3": {
            "load": 0.2,
            "performance_score": 0.95,
            "network_latency_ms": 5,
            "sync_status": "up_to_date",
            "services": ["user_service", "product_service"]
        }
    }
    
    # 创建健康监控器
    health_monitor = HealthMonitor(check_interval=2)
    
    # 注册节点健康检查函数
    for node_id in cluster_nodes.keys():
        def create_check_function(nid: str):
            return lambda x: cluster_nodes[nid]["load"] < 0.8
        
        health_monitor.register_node(node_id, create_check_function(node_id))
        await health_monitor.start_monitoring(node_id)
    
    # 创建故障检测器
    failure_detector = FailureDetector(health_monitor)
    
    # 创建自动故障转移管理器
    failover_manager = AutoFailoverManager(cluster_nodes)
    
    # 注册故障处理器
    failure_detector.register_failure_handler(failover_manager.handle_failure)
    
    # 启动故障检测
    detection_task = asyncio.create_task(failure_detector.detect_failures())
    
    # 启动健康监控
    monitoring_tasks = [
        task for task in health_monitor.monitoring_tasks.values()
    ]
    
    # 模拟系统运行
    print("Starting high availability demonstration...")
    
    try:
        # 等待一段时间让监控系统稳定
        await asyncio.sleep(2)
        
        # 模拟节点故障
        print("\nSimulating node failure...")
        cluster_nodes["node-1"]["load"] = 0.9  # 模拟节点负载过高
        
        # 等待故障检测和转移
        await asyncio.sleep(5)
        
        # 显示故障转移统计
        stats = failover_manager.get_failover_statistics()
        print(f"\nFailover Statistics:")
        print(json.dumps(stats, indent=2, default=str))
        
    finally:
        # 清理任务
        detection_task.cancel()
        for task in monitoring_tasks:
            task.cancel()

# 使用示例
async def demo_distributed_database():
    """演示完整的分布式数据库系统"""
    
    print("=== Distributed Database Architecture Demo ===\n")
    
    # 1. 基础分布式数据库
    print("1. Basic Distributed Database")
    await demo_distributed_db()
    
    print("\n" + "="*60 + "\n")
    
    # 2. 事务处理
    print("2. Distributed Transaction Processing")
    await demo_two_phase_commit()
    
    print("\n" + "="*60 + "\n")
    
    # 3. 查询优化
    print("3. Distributed Query Optimization")
    await demo_distributed_query()
    
    print("\n" + "="*60 + "\n")
    
    # 4. 高可用设计
    print("4. High Availability Design")
    await demo_high_availability()

# 运行演示
if __name__ == "__main__":
    asyncio.run(demo_distributed_database())
```

## 5. 分布式数据库性能优化

### 5.1 读写分离与负载均衡

```python
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import random
import time

class QueryType(Enum):
    """查询类型"""
    READ = "read"
    WRITE = "write"

@dataclass
class QueryRequest:
    """查询请求"""
    request_id: str
    query_type: QueryType
    target_table: str
    query_data: Dict[str, Any]
    consistency_requirement: str = "eventual"  # "strong", "eventual"

class LoadBalancer:
    """负载均衡器"""
    
    def __init__(self):
        self.read_nodes = []  # 读节点列表
        self.write_nodes = [] # 写节点列表
        self.node_weights = {}  # 节点权重
        self.node_stats = {}    # 节点统计信息
        
    def add_read_node(self, node_id: str, weight: int = 1):
        """添加读节点"""
        self.read_nodes.append(node_id)
        self.node_weights[node_id] = weight
        self.node_stats[node_id] = {
            "requests": 0,
            "avg_response_time": 0.0,
            "error_count": 0,
            "cpu_usage": 0.0,
            "memory_usage": 0.0
        }
    
    def add_write_node(self, node_id: str, weight: int = 1):
        """添加写节点"""
        self.write_nodes.append(node_id)
        if node_id not in self.node_weights:
            self.node_weights[node_id] = weight
            self.node_stats[node_id] = {
                "requests": 0,
                "avg_response_time": 0.0,
                "error_count": 0,
                "cpu_usage": 0.0,
                "memory_usage": 0.0
            }
    
    def select_read_node(self, target_table: str) -> Optional[str]:
        """选择读节点"""
        if not self.read_nodes:
            return None
        
        # 加权随机选择
        candidates = []
        total_weight = 0
        
        for node_id in self.read_nodes:
            # 检查节点是否支持目标表
            if self._node_supports_table(node_id, target_table):
                weight = self.node_weights.get(node_id, 1)
                candidates.append((node_id, weight))
                total_weight += weight
        
        if not candidates:
            return self.read_nodes[0]  # 默认返回第一个节点
        
        # 按权重随机选择
        rand_weight = random.uniform(0, total_weight)
        cumulative_weight = 0
        
        for node_id, weight in candidates:
            cumulative_weight += weight
            if rand_weight <= cumulative_weight:
                return node_id
        
        return candidates[0][0]
    
    def select_write_node(self, target_table: str) -> Optional[str]:
        """选择写节点"""
        if not self.write_nodes:
            return None
        
        # 写操作通常选择主节点或主副本
        for node_id in self.write_nodes:
            if self._node_supports_table(node_id, target_table):
                return node_id
        
        return self.write_nodes[0] if self.write_nodes else None
    
    def _node_supports_table(self, node_id: str, table: str) -> bool:
        """检查节点是否支持指定表"""
        # 简化实现：假设所有节点都支持所有表
        return True
    
    def update_node_stats(self, node_id: str, response_time: float, success: bool):
        """更新节点统计信息"""
        if node_id in self.node_stats:
            stats = self.node_stats[node_id]
            stats["requests"] += 1
            
            # 更新平均响应时间
            current_avg = stats["avg_response_time"]
            new_avg = (current_avg * (stats["requests"] - 1) + response_time) / stats["requests"]
            stats["avg_response_time"] = new_avg
            
            # 更新错误计数
            if not success:
                stats["error_count"] += 1
    
    def get_least_loaded_node(self, query_type: QueryType) -> Optional[str]:
        """获取负载最低的节点"""
        nodes = self.read_nodes if query_type == QueryType.READ else self.write_nodes
        
        if not nodes:
            return None
        
        # 按响应时间和错误率综合评估负载
        best_node = None
        best_score = float('inf')
        
        for node_id in nodes:
            if node_id in self.node_stats:
                stats = self.node_stats[node_id]
                
                # 计算负载评分（响应时间 + 错误率惩罚）
                response_time_score = stats["avg_response_time"]
                error_rate = stats["error_count"] / max(stats["requests"], 1)
                error_score = error_rate * 100  # 错误率权重
                
                total_score = response_time_score + error_score
                
                if total_score < best_score:
                    best_score = total_score
                    best_node = node_id
        
        return best_node

class ReadWriteSplitRouter:
    """读写分离路由器"""
    
    def __init__(self, load_balancer: LoadBalancer):
        self.load_balancer = load_balancer
        self.circuit_breakers = {}  # 熔断器状态
        
    async def route_request(self, request: QueryRequest) -> str:
        """路由请求"""
        if request.query_type == QueryType.READ:
            # 读请求根据一致性要求选择节点
            if request.consistency_requirement == "strong":
                # 强一致性：从主节点读取
                target_node = self.load_balancer.select_write_node(request.target_table)
            else:
                # 最终一致性：从读副本读取
                target_node = self.load_balancer.select_read_node(request.target_table)
        else:
            # 写请求路由到主节点
            target_node = self.load_balancer.select_write_node(request.target_table)
        
        # 检查熔断器状态
        if self._is_circuit_open(target_node):
            # 如果熔断器打开，选择备用节点
            backup_node = self.load_balancer.get_least_loaded_node(request.query_type)
            if backup_node:
                target_node = backup_node
        
        return target_node
    
    def _is_circuit_open(self, node_id: str) -> bool:
        """检查熔断器是否打开"""
        # 简化实现：假设熔断器状态存储在circuit_breakers中
        return self.circuit_breakers.get(node_id, {}).get("open", False)

# 分布式缓存层
class DistributedCache:
    """分布式缓存"""
    
    def __init__(self, nodes: List[str]):
        self.nodes = nodes
        self.cache_data = {node: {} for node in nodes}
        self.cache_policy = "lru"  # lru, lfu, ttl
        
    def get(self, key: str) -> Optional[Any]:
        """获取缓存"""
        # 一致性哈希选择节点
        node = self._select_node(key)
        
        if node in self.cache_data:
            return self.cache_data[node].get(key)
        
        return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """设置缓存"""
        node = self._select_node(key)
        
        if node not in self.cache_data:
            self.cache_data[node] = {}
        
        cache_entry = {
            "value": value,
            "timestamp": time.time(),
            "ttl": ttl
        }
        
        self.cache_data[node][key] = cache_entry
    
    def _select_node(self, key: str) -> str:
        """一致性哈希选择节点"""
        # 简化实现：简单的哈希取模
        hash_value = hash(key)
        node_index = hash_value % len(self.nodes)
        return self.nodes[node_index]
    
    def invalidate_key(self, key: str):
        """使缓存键失效"""
        node = self._select_node(key)
        if node in self.cache_data and key in self.cache_data[node]:
            del self.cache_data[node][key]

# 使用示例
async def demo_performance_optimization():
    """演示性能优化"""
    
    print("=== Distributed Database Performance Optimization ===\n")
    
    # 1. 负载均衡器
    print("1. Load Balancer Setup")
    load_balancer = LoadBalancer()
    
    # 添加读节点（读副本）
    read_nodes = ["read-node-1", "read-node-2", "read-node-3"]
    for node in read_nodes:
        weight = random.randint(1, 5)
        load_balancer.add_read_node(node, weight)
    
    # 添加写节点（主节点）
    write_nodes = ["write-node-1", "write-node-2"]
    for node in write_nodes:
        weight = random.randint(1, 3)
        load_balancer.add_write_node(node, weight)
    
    print(f"Added {len(read_nodes)} read nodes and {len(write_nodes)} write nodes")
    
    # 2. 路由测试
    print("\n2. Request Routing Test")
    router = ReadWriteSplitRouter(load_balancer)
    
    # 测试读请求
    read_requests = [
        QueryRequest("req-1", QueryType.READ, "users", {}, "eventual"),
        QueryRequest("req-2", QueryType.READ, "products", {}, "strong"),
        QueryRequest("req-3", QueryType.READ, "orders", {}, "eventual")
    ]
    
    for request in read_requests:
        target_node = await router.route_request(request)
        print(f"Read request {request.request_id} -> {target_node}")
    
    # 测试写请求
    write_requests = [
        QueryRequest("req-4", QueryType.WRITE, "users", {"name": "张三"}),
        QueryRequest("req-5", QueryType.WRITE, "products", {"name": "商品1"})
    ]
    
    for request in write_requests:
        target_node = await router.route_request(request)
        print(f"Write request {request.request_id} -> {target_node}")
    
    # 3. 分布式缓存
    print("\n3. Distributed Cache Demo")
    cache_nodes = ["cache-node-1", "cache-node-2", "cache-node-3"]
    cache = DistributedCache(cache_nodes)
    
    # 缓存测试数据
    test_data = {
        "user:001": {"id": "001", "name": "张三", "age": 30},
        "user:002": {"id": "002", "name": "李四", "age": 25},
        "product:001": {"id": "001", "name": "商品1", "price": 99.99}
    }
    
    # 设置缓存
    for key, value in test_data.items():
        cache.set(key, value, ttl=300)  # 5分钟TTL
        print(f"Cached: {key}")
    
    # 读取缓存
    print("\nCache Retrieval:")
    for key in test_data.keys():
        cached_value = cache.get(key)
        print(f"  {key}: {cached_value is not None}")
    
    # 4. 性能监控
    print("\n4. Performance Monitoring")
    
    # 模拟性能统计更新
    import random
    for node_id in load_balancer.node_stats:
        for i in range(10):  # 模拟10个请求
            response_time = random.uniform(0.01, 0.5)
            success = random.random() > 0.05  # 95%成功率
            load_balancer.update_node_stats(node_id, response_time, success)
    
    # 显示节点统计
    print("Node Statistics:")
    for node_id, stats in load_balancer.node_stats.items():
        error_rate = stats["error_count"] / max(stats["requests"], 1)
        print(f"  {node_id}:")
        print(f"    Requests: {stats['requests']}")
        print(f"    Avg Response Time: {stats['avg_response_time']:.3f}s")
        print(f"    Error Rate: {error_rate:.2%}")
    
    # 5. 负载均衡效果测试
    print("\n5. Load Balancing Effect Test")
    
    # 模拟大量读请求
    node_selection_count = {node_id: 0 for node_id in read_nodes}
    
    for i in range(100):  # 100个请求
        selected_node = load_balancer.select_read_node("users")
        node_selection_count[selected_node] += 1
    
    print("Request Distribution (100 requests):")
    for node_id, count in node_selection_count.items():
        weight = load_balancer.node_weights.get(node_id, 1)
        expected_percentage = weight / sum(load_balancer.node_weights[n] for n in read_nodes)
        actual_percentage = count / 100
        print(f"  {node_id}: {count} requests ({actual_percentage:.1%}, expected {expected_percentage:.1%})")

# 运行性能优化演示
if __name__ == "__main__":
    asyncio.run(demo_performance_optimization())
```

## 6. 最佳实践与架构建议

### 6.1 架构设计原则

1. **数据分片策略**
   - 选择合适的分片键，避免热点
   - 考虑数据增长和重分片的成本
   - 实现动态分片调整能力

2. **一致性模型选择**
   - 根据业务需求选择强一致性或最终一致性
   - 在性能和一致性之间找到平衡点
   - 使用向量时钟等技术解决冲突

3. **故障处理机制**
   - 实现自动故障检测和转移
   - 设计合理的熔断器机制
   - 保证数据不丢失

4. **性能优化策略**
   - 读写分离架构
   - 分布式缓存使用
   - 查询优化和执行计划

### 6.2 监控与运维

```python
# 监控系统指标
@dataclass
class SystemMetrics:
    """系统指标"""
    node_id: str
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_latency: float
    active_connections: int
    query_rate: float
    error_rate: float

class MonitoringDashboard:
    """监控仪表板"""
    
    def __init__(self):
        self.metrics_history = []
        self.alert_rules = []
        self.dashboards = {}
    
    def collect_metrics(self, metrics: SystemMetrics):
        """收集指标数据"""
        self.metrics_history.append(metrics)
        
        # 检查告警规则
        self._check_alerts(metrics)
    
    def _check_alerts(self, metrics: SystemMetrics):
        """检查告警规则"""
        alerts = []
        
        if metrics.cpu_usage > 0.8:
            alerts.append(f"High CPU usage: {metrics.cpu_usage:.1%}")
        
        if metrics.memory_usage > 0.9:
            alerts.append(f"High memory usage: {metrics.memory_usage:.1%}")
        
        if metrics.error_rate > 0.05:
            alerts.append(f"High error rate: {metrics.error_rate:.1%}")
        
        if alerts:
            print(f"ALERT for {metrics.node_id}: {'; '.join(alerts)}")
    
    def generate_report(self) -> Dict[str, Any]:
        """生成监控报告"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-100:]  # 最近100条记录
        
        avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
        avg_error_rate = sum(m.error_rate for m in recent_metrics) / len(recent_metrics)
        
        return {
            "summary": {
                "total_nodes": len(set(m.node_id for m in recent_metrics)),
                "time_range": {
                    "start": recent_metrics[0].timestamp.isoformat(),
                    "end": recent_metrics[-1].timestamp.isoformat()
                }
            },
            "average_metrics": {
                "cpu_usage": avg_cpu,
                "memory_usage": avg_memory,
                "error_rate": avg_error_rate
            },
            "recommendations": self._generate_recommendations(avg_cpu, avg_memory, avg_error_rate)
        }
    
    def _generate_recommendations(self, avg_cpu: float, avg_memory: float, avg_error_rate: float) -> List[str]:
        """生成优化建议"""
        recommendations = []
        
        if avg_cpu > 0.7:
            recommendations.append("Consider scaling out or optimizing queries")
        
        if avg_memory > 0.8:
            recommendations.append("Monitor memory usage and consider increasing resources")
        
        if avg_error_rate > 0.01:
            recommendations.append("Investigate error causes and improve error handling")
        
        return recommendations
```

## 7. 总结

分布式数据库架构是现代企业级应用的核心基础架构，涉及多个复杂的技术领域：

**关键技术要素：**
1. **一致性哈希与数据分片**
2. **分布式事务处理（2PC、3PC、TCC）**
3. **查询优化与执行**
4. **故障检测与自动恢复**
5. **读写分离与负载均衡**
6. **性能监控与运维**

**架构选择指南：**
- **强一致性**：金融、订单等核心业务
- **最终一致性**：社交、日志等非关键业务
- **分区容错性**：跨数据中心部署必需

**实施关键点：**
- 数据模型设计考虑分布式特性
- 故障处理机制设计完善
- 性能优化策略制定合理
- 监控告警体系建立完整

通过合理的