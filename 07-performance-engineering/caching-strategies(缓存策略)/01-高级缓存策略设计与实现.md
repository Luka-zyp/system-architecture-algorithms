# 高级缓存策略设计与实现

## 1. 缓存架构设计模式

### 多层缓存架构
```python
from typing import Any, Optional, Dict, List, Callable
import time
import threading
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

class CacheLevel(Enum):
    L1 = "l1"      # 进程内缓存
    L2 = "l2"      # 本地缓存
    L3 = "l3"      # 分布式缓存

@dataclass
class CacheEntry:
    key: str
    value: Any
    timestamp: float
    ttl: int
    access_count: int = 0
    
    def is_expired(self) -> bool:
        return time.time() - self.timestamp > self.ttl

class CacheStrategy(ABC):
    @abstractmethod
    def get(self, key: str) -> Optional[Any]:
        pass
    
    @abstractmethod
    def put(self, key: str, value: Any, ttl: int = 300):
        pass
    
    @abstractmethod
    def remove(self, key: str):
        pass
    
    @abstractmethod
    def clear(self):
        pass

class L1Cache(CacheStrategy):
    """L1 进程内缓存 - 最快但容量有限"""
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.cache = {}
        self.access_order = []
        self.lock = threading.RLock()
    
    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key in self.cache:
                entry = self.cache[key]
                if not entry.is_expired():
                    entry.access_count += 1
                    self._update_access_order(key)
                    return entry.value
                else:
                    del self.cache[key]
                    self.access_order.remove(key)
            return None
    
    def put(self, key: str, value: Any, ttl: int = 300):
        with self.lock:
            if len(self.cache) >= self.max_size and key not in self.cache:
                self._evict_lru()
            
            entry = CacheEntry(
                key=key,
                value=value,
                timestamp=time.time(),
                ttl=ttl
            )
            
            self.cache[key] = entry
            self.access_order.append(key)
    
    def remove(self, key: str):
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                self.access_order.remove(key)
    
    def clear(self):
        with self.lock:
            self.cache.clear()
            self.access_order.clear()
    
    def _evict_lru(self):
        """LRU淘汰策略"""
        if self.access_order:
            lru_key = self.access_order.pop(0)
            del self.cache[lru_key]
    
    def _update_access_order(self, key: str):
        """更新访问顺序"""
        if key in self.access_order:
            self.access_order.remove(key)
        self.access_order.append(key)

class L2Cache(CacheStrategy):
    """L2 本地缓存 - 使用Redis等"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.local_cache = {}
        self.lock = threading.RLock()
    
    def get(self, key: str) -> Optional[Any]:
        # 先检查本地缓存
        with self.lock:
            if key in self.local_cache:
                entry = self.local_cache[key]
                if not entry.is_expired():
                    return entry.value
                else:
                    del self.local_cache[key]
        
        # 检查Redis缓存
        try:
            data = self.redis.get(key)
            if data:
                value = self._deserialize(data)
                # 回写到本地缓存
                with self.lock:
                    self.local_cache[key] = CacheEntry(
                        key=key,
                        value=value,
                        timestamp=time.time(),
                        ttl=300  # 本地缓存TTL较短
                    )
                return value
        except Exception:
            pass
        
        return None
    
    def put(self, key: str, value: Any, ttl: int = 300):
        # 同时写入Redis和本地缓存
        try:
            self.redis.setex(key, ttl, self._serialize(value))
        except Exception:
            pass
        
        with self.lock:
            self.local_cache[key] = CacheEntry(
                key=key,
                value=value,
                timestamp=time.time(),
                ttl=ttl
            )
    
    def remove(self, key: str):
        try:
            self.redis.delete(key)
        except Exception:
            pass
        
        with self.lock:
            self.local_cache.pop(key, None)
    
    def clear(self):
        try:
            self.redis.flushdb()
        except Exception:
            pass
        
        with self.lock:
            self.local_cache.clear()
    
    def _serialize(self, obj: Any) -> bytes:
        """序列化对象"""
        import pickle
        return pickle.dumps(obj)
    
    def _deserialize(self, data: bytes) -> Any:
        """反序列化对象"""
        import pickle
        return pickle.loads(data)

class MultiLevelCache:
    """多级缓存管理器"""
    
    def __init__(self, l1_cache: L1Cache, l2_cache: L2Cache):
        self.l1 = l1_cache
        self.l2 = l2_cache
        self.stats = {
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0,
            'l1_hit_rate': 0.0,
            'l2_hit_rate': 0.0
        }
    
    def get(self, key: str) -> Optional[Any]:
        # 先查L1
        value = self.l1.get(key)
        if value is not None:
            self.stats['l1_hits'] += 1
            return value
        
        # 再查L2
        value = self.l2.get(key)
        if value is not None:
            self.stats['l2_hits'] += 1
            # 回写到L1
            self.l1.put(key, value)
            return value
        
        self.stats['misses'] += 1
        return None
    
    def put(self, key: str, value: Any, ttl: int = 300):
        self.l1.put(key, value, ttl)
        self.l2.put(key, value, ttl)
    
    def remove(self, key: str):
        self.l1.remove(key)
        self.l2.remove(key)
    
    def clear(self):
        self.l1.clear()
        self.l2.clear()
    
    def get_stats(self) -> Dict:
        total_requests = self.stats['l1_hits'] + self.stats['l2_hits'] + self.stats['misses']
        if total_requests > 0:
            self.stats['l1_hit_rate'] = self.stats['l1_hits'] / total_requests
            self.stats['l2_hit_rate'] = (self.stats['l1_hits'] + self.stats['l2_hits']) / total_requests
        
        return self.stats.copy()
```

### 缓存预热策略
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Set, Callable, Any

class CacheWarmer:
    """缓存预热器"""
    
    def __init__(self, cache: MultiLevelCache, max_workers: int = 5):
        self.cache = cache
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.preload_tasks = {}
        self.is_warming = False
    
    def preload_keys(self, keys: Set[str], data_loader: Callable[[str], Any]):
        """预加载指定key集合"""
        if self.is_warming:
            return False
        
        self.is_warming = True
        
        try:
            futures = []
            for key in keys:
                future = self.executor.submit(self._load_and_cache, key, data_loader)
                futures.append(future)
            
            # 等待所有任务完成
            for future in futures:
                future.result()
            
            return True
        except Exception as e:
            print(f"Cache warming failed: {e}")
            return False
        finally:
            self.is_warming = False
    
    def _load_and_cache(self, key: str, data_loader: Callable[[str], Any]):
        """加载数据并缓存"""
        try:
            value = data_loader(key)
            if value is not None:
                self.cache.put(key, value)
        except Exception as e:
            print(f"Failed to load data for key {key}: {e}")
    
    def schedule_preload(self, keys: Set[str], data_loader: Callable[[str], Any], 
                        interval: int = 3600):
        """定时预加载"""
        import schedule
        
        schedule.every(interval).seconds.do(
            self.preload_keys, keys, data_loader
        )
        
        # 在新线程中运行调度器
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(1)
        
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()

class HotDataDetector:
    """热点数据检测器"""
    
    def __init__(self, cache: MultiLevelCache, detection_window: int = 300):
        self.cache = cache
        self.access_counts = {}
        self.detection_window = detection_window
        self.lock = threading.Lock()
    
    def record_access(self, key: str):
        """记录数据访问"""
        with self.lock:
            current_time = time.time()
            
            # 清理过期记录
            expired_keys = [
                k for k, (timestamp, _) in self.access_counts.items()
                if current_time - timestamp > self.detection_window
            ]
            for k in expired_keys:
                del self.access_counts[k]
            
            # 更新访问计数
            if key in self.access_counts:
                timestamp, count = self.access_counts[key]
                self.access_counts[key] = (timestamp, count + 1)
            else:
                self.access_counts[key] = (current_time, 1)
    
    def get_hot_keys(self, threshold: int = 10) -> Set[str]:
        """获取热点key"""
        with self.lock:
            current_time = time.time()
            hot_keys = set()
            
            for key, (timestamp, count) in self.access_counts.items():
                # 在检测窗口内的访问频率
                if current_time - timestamp < self.detection_window:
                    if count >= threshold:
                        hot_keys.add(key)
            
            return hot_keys
    
    def auto_preload_hot_data(self, data_loader: Callable[[str], Any]):
        """自动预热热点数据"""
        hot_keys = self.get_hot_keys()
        
        # 过滤出未缓存的key
        uncached_keys = set()
        for key in hot_keys:
            if self.cache.get(key) is None:
                uncached_keys.add(key)
        
        if uncached_keys:
            print(f"Preloading {len(uncached_keys)} hot keys")
            for key in uncached_keys:
                try:
                    value = data_loader(key)
                    if value is not None:
                        self.cache.put(key, value)
                except Exception as e:
                    print(f"Failed to preload key {key}: {e}")
```

## 2. 缓存一致性策略

### 缓存更新策略
```python
from enum import Enum
import threading

class CacheUpdateStrategy(Enum):
    WRITE_THROUGH = "write_through"    # 写透
    WRITE_AROUND = "write_around"      # 绕写
    WRITE_BACK = "write_back"          # 写回

class CacheConsistencyManager:
    """缓存一致性管理器"""
    
    def __init__(self, cache: MultiLevelCache, update_strategy: CacheUpdateStrategy):
        self.cache = cache
        self.update_strategy = update_strategy
        self.invalidation_listeners = []
        self.lock = threading.RLock()
    
    def read_through(self, key: str, data_loader: Callable[[str], Any]) -> Any:
        """读透模式"""
        # 先查缓存
        value = self.cache.get(key)
        if value is not None:
            return value
        
        # 缓存未命中，加载数据
        value = data_loader(key)
        if value is not None:
            self.cache.put(key, value)
        
        return value
    
    def write_through(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """写透模式"""
        # 写入数据源
        success = data_writer(key, value)
        
        if success:
            # 写入缓存
            self.cache.put(key, value)
            
            # 通知失效监听器
            self._notify_invalidation_listeners(key, value)
        
        return success
    
    def write_around(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """绕写模式"""
        # 只写入数据源，不写入缓存
        success = data_writer(key, value)
        
        if success:
            # 失效缓存中可能存在的旧数据
            self.cache.remove(key)
            
            # 通知失效监听器
            self._notify_invalidation_listeners(key, value)
        
        return success
    
    def write_back(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """写回模式"""
        # 先写入缓存
        self.cache.put(key, value)
        
        # 异步写入数据源
        def async_write():
            try:
                success = data_writer(key, value)
                if not success:
                    # 写入失败，需要回滚缓存
                    self.cache.remove(key)
            except Exception as e:
                print(f"Async write failed for key {key}: {e}")
                self.cache.remove(key)
        
        threading.Thread(target=async_write, daemon=True).start()
        
        return True
    
    def add_invalidation_listener(self, listener: Callable[[str, Any], None]):
        """添加缓存失效监听器"""
        with self.lock:
            self.invalidation_listeners.append(listener)
    
    def _notify_invalidation_listeners(self, key: str, value: Any):
        """通知所有失效监听器"""
        for listener in self.invalidation_listeners:
            try:
                listener(key, value)
            except Exception as e:
                print(f"Invalidation listener failed: {e}")

class DistributedCacheInvalidator:
    """分布式缓存失效器"""
    
    def __init__(self, cache: MultiLevelCache, redis_pub_sub):
        self.cache = cache
        self.redis_pub_sub = redis_pub_sub
        self.subscribers = {}
        self._start_listening()
    
    def _start_listening(self):
        """开始监听Redis发布订阅"""
        def listen():
            pubsub = self.redis_pub_sub.pubsub()
            pubsub.subscribe("cache_invalidation")
            
            for message in pubsub.listen():
                if message['type'] == 'message':
                    try:
                        data = eval(message['data'])  # 注意：生产环境应使用JSON
                        key = data['key']
                        self._handle_invalidation_message(key, data)
                    except Exception as e:
                        print(f"Failed to handle invalidation message: {e}")
        
        threading.Thread(target=listen, daemon=True).start()
    
    def invalidate_key(self, key: str, source: str = "unknown"):
        """失效指定key"""
        # 本地失效
        self.cache.remove(key)
        
        # 广播给其他节点
        try:
            self.redis_pub_sub.publish(
                "cache_invalidation", 
                str({'key': key, 'source': source, 'timestamp': time.time()})
            )
        except Exception as e:
            print(f"Failed to publish invalidation message: {e}")
    
    def _handle_invalidation_message(self, key: str, data: dict):
        """处理失效消息"""
        # 验证消息时间戳（避免过期消息）
        if time.time() - data.get('timestamp', 0) > 300:  # 5分钟过期
            return
        
        self.cache.remove(key)
    
    def batch_invalidate(self, keys: List[str], source: str = "unknown"):
        """批量失效"""
        for key in keys:
            self.invalidate_key(key, source)
```

## 3. 缓存失效策略

### TTL管理
```python
import heapq
from typing import Dict, List, Tuple

class TTLCacheManager:
    """TTL缓存管理器"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.expiry_heap = []  # (expire_time, key)
        self.key_expiry_map = {}  # key -> expire_time
        self.lock = threading.RLock()
        self._start_cleanup_thread()
    
    def put_with_ttl(self, key: str, value: Any, ttl: int):
        """带TTL的缓存写入"""
        expire_time = time.time() + ttl
        
        with self.lock:
            # 添加到过期堆
            heapq.heappush(self.expiry_heap, (expire_time, key))
            self.key_expiry_map[key] = expire_time
        
        # 写入缓存
        self.cache.put(key, value, ttl)
    
    def _start_cleanup_thread(self):
        """启动清理线程"""
        def cleanup_expired():
            while True:
                try:
                    self._cleanup_expired_entries()
                    time.sleep(60)  # 每分钟清理一次
                except Exception as e:
                    print(f"Cache cleanup failed: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_expired, daemon=True)
        cleanup_thread.start()
    
    def _cleanup_expired_entries(self):
        """清理过期条目"""
        current_time = time.time()
        expired_keys = []
        
        with self.lock:
            while self.expiry_heap and self.expiry_heap[0][0] <= current_time:
                expire_time, key = heapq.heappop(self.expiry_heap)
                
                # 验证这个key是否真的过期（可能有重复）
                if self.key_expiry_map.get(key) == expire_time:
                    expired_keys.append(key)
                    del self.key_expiry_map[key]
        
        # 批量清理过期key
        for key in expired_keys:
            self.cache.remove(key)
        
        if expired_keys:
            print(f"Cleaned up {len(expired_keys)} expired cache entries")
    
    def get_keys_expiring_soon(self, threshold_seconds: int = 300) -> List[str]:
        """获取即将过期的key"""
        current_time = time.time()
        threshold_time = current_time + threshold_seconds
        
        with self.lock:
            expiring_keys = []
            for expire_time, key in self.expiry_heap:
                if expire_time <= threshold_time:
                    if self.key_expiry_map.get(key) == expire_time:
                        expiring_keys.append(key)
                else:
                    break
        
        return expiring_keys
    
    def refresh_expiring_keys(self, data_loader: Callable[[str], Any], 
                            refresh_threshold: int = 300):
        """刷新即将过期的key"""
        expiring_keys = self.get_keys_expiring_soon(refresh_threshold)
        
        for key in expiring_keys:
            try:
                value = data_loader(key)
                if value is not None:
                    # 重新设置TTL
                    self.put_with_ttl(key, value, refresh_threshold * 2)
            except Exception as e:
                print(f"Failed to refresh key {key}: {e}")

class CacheTagManager:
    """缓存标签管理器"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.tag_index = {}  # tag -> set of keys
        self.key_tags = {}   # key -> set of tags
        self.lock = threading.RLock()
    
    def put_with_tags(self, key: str, value: Any, ttl: int, tags: List[str]):
        """带标签的缓存写入"""
        # 写入缓存
        self.cache.put(key, value, ttl)
        
        # 更新索引
        with self.lock:
            for tag in tags:
                if tag not in self.tag_index:
                    self.tag_index[tag] = set()
                self.tag_index[tag].add(key)
            
            self.key_tags[key] = set(tags)
    
    def invalidate_by_tag(self, tag: str):
        """按标签失效缓存"""
        with self.lock:
            if tag in self.tag_index:
                keys_to_invalidate = self.tag_index[tag].copy()
                
                # 失效所有相关key
                for key in keys_to_invalidate:
                    self.cache.remove(key)
                    # 清理key的标签记录
                    self.key_tags.pop(key, None)
                
                # 清理标签索引
                del self.tag_index[tag]
                
                print(f"Invalidated {len(keys_to_invalidate)} keys with tag: {tag}")
    
    def get_keys_by_tag(self, tag: str) -> List[str]:
        """获取指定标签的所有key"""
        with self.lock:
            return list(self.tag_index.get(tag, set()))
    
    def get_tags_for_key(self, key: str) -> List[str]:
        """获取key的所有标签"""
        with self.lock:
            return list(self.key_tags.get(key, set()))
```

## 4. 缓存性能优化

### 缓存性能监控
```python
import time
from collections import defaultdict, deque
from typing import Dict, List

class CachePerformanceMonitor:
    """缓存性能监控器"""
    
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.metrics = {
            'hits': deque(maxlen=window_size),
            'misses': deque(maxlen=window_size),
            'latencies': defaultdict(lambda: deque(maxlen=100)),
            'evictions': deque(maxlen=window_size)
        }
        self.lock = threading.RLock()
    
    def record_hit(self, key: str, latency: float):
        """记录缓存命中"""
        with self.lock:
            self.metrics['hits'].append({
                'timestamp': time.time(),
                'key': key,
                'latency': latency
            })
            self.metrics['latencies'][key].append(latency)
    
    def record_miss(self, key: str, latency: float):
        """记录缓存未命中"""
        with self.lock:
            self.metrics['misses'].append({
                'timestamp': time.time(),
                'key': key,
                'latency': latency
            })
    
    def record_eviction(self, key: str, reason: str):
        """记录缓存淘汰"""
        with self.lock:
            self.metrics['evictions'].append({
                'timestamp': time.time(),
                'key': key,
                'reason': reason
            })
    
    def get_hit_rate(self, window_minutes: int = 5) -> float:
        """计算命中率"""
        with self.lock:
            cutoff_time = time.time() - (window_minutes * 60)
            
            hits_in_window = [
                m for m in self.metrics['hits'] 
                if m['timestamp'] >= cutoff_time
            ]
            misses_in_window = [
                m for m in self.metrics['misses'] 
                if m['timestamp'] >= cutoff_time
            ]
            
            total_requests = len(hits_in_window) + len(misses_in_window)
            if total_requests == 0:
                return 0.0
            
            return len(hits_in_window) / total_requests
    
    def get_avg_latency(self, key: str = None) -> float:
        """获取平均延迟"""
        with self.lock:
            if key:
                latencies = self.metrics['latencies'][key]
                if not latencies:
                    return 0.0
                return sum(latencies) / len(latencies)
            else:
                # 全局平均延迟
                all_latencies = []
                for key_latencies in self.metrics['latencies'].values():
                    all_latencies.extend(key_latencies)
                
                if not all_latencies:
                    return 0.0
                return sum(all_latencies) / len(all_latencies)
    
    def get_slowest_keys(self, limit: int = 10) -> List[tuple]:
        """获取最慢的key"""
        with self.lock:
            key_latencies = []
            for key, latencies in self.metrics['latencies'].items():
                if latencies:
                    avg_latency = sum(latencies) / len(latencies)
                    key_latencies.append((key, avg_latency, len(latencies)))
            
            return sorted(key_latencies, key=lambda x: x[1], reverse=True)[:limit]
    
    def get_eviction_stats(self) -> Dict:
        """获取淘汰统计"""
        with self.lock:
            eviction_counts = defaultdict(int)
            recent_evictions = self.metrics['evictions'][-100:]  # 最近100次
            
            for eviction in recent_evictions:
                eviction_counts[eviction['reason']] += 1
            
            return dict(eviction_counts)

class AdaptiveCacheSizer:
    """自适应缓存大小调整器"""
    
    def __init__(self, cache: MultiLevelCache, min_size: int = 100, max_size: int = 10000):
        self.cache = cache
        self.min_size = min_size
        self.max_size = max_size
        self.performance_monitor = CachePerformanceMonitor()
        self.current_size = min_size
        self.adjustment_history = []
    
    def analyze_and_adjust(self):
        """分析并调整缓存大小"""
        hit_rate = self.performance_monitor.get_hit_rate()
        avg_latency = self.performance_monitor.get_avg_latency()
        
        # 性能评分算法
        performance_score = hit_rate * 100 - avg_latency * 10
        
        # 根据性能评分调整大小
        new_size = self.current_size
        
        if performance_score < 70:  # 性能较差
            if self.current_size < self.max_size:
                new_size = int(self.current_size * 1.2)  # 增加20%
        elif performance_score > 90:  # 性能很好
            if self.current_size > self.min_size:
                new_size = int(self.current_size * 0.9)  # 减少10%
        
        # 应用调整
        if new_size != self.current_size:
            self._adjust_cache_size(new_size)
            self.adjustment_history.append({
                'timestamp': time.time(),
                'old_size': self.current_size,
                'new_size': new_size,
                'hit_rate': hit_rate,
                'avg_latency': avg_latency,
                'performance_score': performance_score
            })
    
    def _adjust_cache_size(self, new_size: int):
        """调整缓存大小"""
        self.current_size = max(self.min_size, min(self.max_size, new_size))
        print(f"Cache size adjusted to {self.current_size}")
        # 这里应该实际调整缓存的大小
        # 具体的调整逻辑取决于缓存实现

class CacheHealthChecker:
    """缓存健康检查器"""
    
    def __init__(self, cache: MultiLevelCache, performance_monitor: CachePerformanceMonitor):
        self.cache = cache
        self.performance_monitor = performance_monitor
        self.health_metrics = {}
    
    def check_health(self) -> Dict:
        """健康检查"""
        health_status = {
            'status': 'healthy',
            'issues': [],
            'recommendations': [],
            'metrics': {}
        }
        
        # 检查命中率
        hit_rate = self.performance_monitor.get_hit_rate()
        health_status['metrics']['hit_rate'] = hit_rate
        
        if hit_rate < 0.7:
            health_status['issues'].append(f"Low hit rate: {hit_rate:.2%}")
            health_status['recommendations'].append("Consider increasing cache size")
        
        # 检查延迟
        avg_latency = self.performance_monitor.get_avg_latency()
        health_status['metrics']['avg_latency'] = avg_latency
        
        if avg_latency > 100:  # 100ms
            health_status['issues'].append(f"High latency: {avg_latency:.2f}ms")
            health_status['recommendations'].append("Check cache performance and consider optimization")
        
        # 检查淘汰率
        eviction_stats = self.performance_monitor.get_eviction_stats()
        total_evictions = sum(eviction_stats.values())
        health_status['metrics']['evictions'] = eviction_stats
        
        if total_evictions > 100:
            health_status['issues'].append(f"High eviction rate: {total_evictions}")
            health_status['recommendations'].append("Consider increasing cache capacity")
        
        # 确定整体状态
        if len(health_status['issues']) > 3:
            health_status['status'] = 'critical'
        elif len(health_status['issues']) > 0:
            health_status['status'] = 'warning'
        
        return health_status
```

## 5. 高级缓存模式

### 缓存旁路模式
```python
class CacheAsidePattern:
    """缓存旁路模式（Cache-Aside Pattern）"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
    
    def get(self, key: str, data_loader: Callable[[str], Any]) -> Any:
        """获取数据，缓存旁路模式"""
        # 1. 先查缓存
        cached_value = self.cache.get(key)
        if cached_value is not None:
            return cached_value
        
        # 2. 缓存未命中，从数据源加载
        value = data_loader(key)
        
        # 3. 将数据放入缓存
        if value is not None:
            self.cache.put(key, value)
        
        return value
    
    def set(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]) -> bool:
        """设置数据，缓存旁路模式"""
        # 1. 先写入数据源
        success = data_writer(key, value)
        
        if success:
            # 2. 成功后更新缓存
            self.cache.put(key, value)
        
        return success
    
    def delete(self, key: str, data_deleter: Callable[[str], bool]) -> bool:
        """删除数据，缓存旁路模式"""
        # 1. 先从数据源删除
        success = data_deleter(key)
        
        # 2. 无论数据源删除是否成功，都删除缓存
        self.cache.remove(key)
        
        return success

class ReadThroughCache:
    """读透模式缓存"""
    
    def __init__(self, cache: MultiLevelCache, data_loader: Callable[[str], Any]):
        self.cache = cache
        self.data_loader = data_loader
        self.lock = threading.RLock()
        self.pending_loads = {}
    
    def get(self, key: str) -> Any:
        """读透获取"""
        # 先查缓存
        cached_value = self.cache.get(key)
        if cached_value is not None:
            return cached_value
        
        with self.lock:
            # 检查是否已有其他线程在加载这个key
            if key in self.pending_loads:
                # 等待正在进行的加载
                return self.pending_loads[key].result()
            
            # 开始新的加载
            future = threading.Future()
            self.pending_loads[key] = future
        
        try:
            # 加载数据
            value = self.data_loader(key)
            
            # 放入缓存
            if value is not None:
                self.cache.put(key, value)
            
            # 通知等待的线程
            future.set_result(value)
            
            # 清理待加载记录
            with self.lock:
                del self.pending_loads[key]
            
            return value
        
        except Exception as e:
            # 处理异常
            future.set_exception(e)
            with self.lock:
                del self.pending_loads[key]
            raise

class WriteBehindCache:
    """写回模式缓存"""
    
    def __init__(self, cache: MultiLevelCache, batch_size: int = 100):
        self.cache = cache
        self.batch_size = batch_size
        self.write_queue = asyncio.Queue()
        self.pending_writes = []
        self.is_running = False
        self._start_write_worker()
    
    def _start_write_worker(self):
        """启动异步写入工作线程"""
        self.is_running = True
        
        def write_worker():
            while self.is_running:
                try:
                    # 收集一批写入
                    batch = []
                    for _ in range(self.batch_size):
                        try:
                            item = self.write_queue.get_nowait()
                            batch.append(item)
                        except asyncio.QueueEmpty:
                            break
                    
                    if batch:
                        self._flush_batch(batch)
                    
                    time.sleep(1)  # 等待新数据
                
                except Exception as e:
                    print(f"Write worker error: {e}")
        
        threading.Thread(target=write_worker, daemon=True).start()
    
    def set(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """写回设置"""
        # 立即写入缓存
        self.cache.put(key, value)
        
        # 异步写入数据源
        asyncio.run_coroutine_threadsafe(
            self.write_queue.put((key, value, data_writer)),
            asyncio.get_event_loop()
        )
    
    def _flush_batch(self, batch: List[tuple]):
        """批量写入数据源"""
        for key, value, data_writer in batch:
            try:
                success = data_writer(key, value)
                if not success:
                    print(f"Failed to write data for key {key}")
            except Exception as e:
                print(f"Error writing data for key {key}: {e}")
    
    def flush_all(self):
        """刷新所有待写入数据"""
        batch = []
        while not self.write_queue.empty():
            try:
                item = self.write_queue.get_nowait()
                batch.append(item)
            except asyncio.QueueEmpty:
                break
        
        if batch:
            self._flush_batch(batch)
```

## 6. 缓存架构最佳实践

### 缓存设计原则
1. **层次化设计**: L1/L2/L3多层缓存，根据访问频率和重要性分层
2. **一致性保障**: 根据业务需求选择合适的缓存一致性策略
3. **性能监控**: 实时监控缓存命中率、延迟、淘汰率等关键指标
4. **自动调优**: 基于性能数据自动调整缓存大小和策略
5. **故障恢复**: 建立完善的缓存故障恢复机制
6. **安全防护**: 防止缓存穿透、击穿、雪崩等常见问题

### 缓存部署建议
- **容量规划**: 根据业务数据量和访问模式合理规划缓存容量
- **节点分布**: 在分布式环境中合理分布缓存节点
- **监控告警**: 设置完善的监控和告警机制
- **运维自动化**: 实现缓存的自动化运维和管理

### 性能优化要点
- **压缩存储**: 对于大对象考虑使用压缩技术
- **批量操作**: 优化批量读取和写入操作
- **连接池**: 使用连接池管理数据库连接
- **异步处理**: 对于非关键路径使用异步处理

通过以上高级缓存策略的设计和实现，可以显著提升系统的性能、可用性和可扩展性。关键是要根据具体的业务场景和性能需求，选择合适的缓存策略和架构模式。