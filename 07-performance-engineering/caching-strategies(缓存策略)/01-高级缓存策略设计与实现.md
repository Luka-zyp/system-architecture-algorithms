# 高级缓存策略设计与实现

## 1. 缓存架构设计模式

### 多层缓存架构

缓存系统通常采用多层架构来平衡性能、容量和成本：

1. **L1缓存** - 内存缓存，访问速度最快，容量有限
2. **L2缓存** - Redis/数据库，容量较大，访问速度适中
3. **L3缓存** - 文件系统或外部存储，容量最大，访问较慢

### 核心缓存组件

```python
class CacheEntry:
    """缓存条目"""
    def __init__(self, value, ttl=300):
        self.value = value
        self.timestamp = time.time()
        self.ttl = ttl
        self.access_count = 0
    
    def is_expired(self):
        return time.time() - self.timestamp > self.ttl

class SimpleCache:
    """简单内存缓存"""
    
    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
        self.lock = threading.Lock()
        self.stats = {'hits': 0, 'misses': 0}
    
    def get(self, key):
        with self.lock:
            if key in self.cache:
                entry = self.cache[key]
                if not entry.is_expired():
                    self.stats['hits'] += 1
                    return entry.value
                else:
                    del self.cache[key]
            self.stats['misses'] += 1
            return None
    
    def put(self, key, value, ttl=300):
        with self.lock:
            if len(self.cache) >= self.max_size:
                # 简单的LRU eviction
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
            
            entry = CacheEntry(value, ttl)
            self.cache[key] = entry
```

## 2. 缓存策略模式

### Cache-Aside模式
```python
def get_data_cached(cache, key, data_loader):
    """Cache-Aside模式：缓存未命中时从数据源加载"""
    data = cache.get(key)
    if data is None:
        data = data_loader(key)
        cache.put(key, data)
    return data
```

### Read-Through模式
```python
def get_data_read_through(cache, key):
    """Read-Through模式：缓存自动加载数据"""
    return cache.get_or_load(key)
```

### Write-Through模式
```python
def update_data_write_through(cache, key, value, data_source):
    """Write-Through模式：同时更新缓存和数据源"""
    cache.put(key, value)
    data_source.update(key, value)
```

### Write-Behind模式
```python
def update_data_write_behind(cache, key, value):
    """Write-Behind模式：异步更新数据源"""
    cache.put(key, value)
    # 异步写入数据源
    background_write_to_database(key, value)
```

## 3. 缓存失效策略

### TTL (Time To Live)
```python
def put_with_ttl(cache, key, value, ttl=300):
    """设置过期时间的缓存写入"""
    cache.put(key, value, ttl)
```

### LRU (Least Recently Used)
```python
class LRUCache:
    """LRU缓存实现"""
    
    def __init__(self, capacity=100):
        self.capacity = capacity
        self.cache = {}
        self.order = []
    
    def get(self, key):
        if key in self.cache:
            self.order.remove(key)
            self.order.append(key)
            return self.cache[key]
        return None
    
    def put(self, key, value):
        if key in self.cache:
            self.order.remove(key)
        elif len(self.cache) >= self.capacity:
            oldest = self.order.pop(0)
            del self.cache[oldest]
        
        self.cache[key] = value
        self.order.append(key)
```

### 基于频率的缓存
```python
class LFUCache:
    """LFU缓存实现"""
    
    def __init__(self, capacity=100):
        self.capacity = capacity
        self.cache = {}
        self.freq_map = {}
    
    def get(self, key):
        if key not in self.cache:
            return None
        
        # 增加访问频率
        self._increase_frequency(key)
        return self.cache[key]
    
    def _increase_frequency(self, key):
        freq = self.freq_map[key]
        del self.freq_map[key]
        # 将key移到更高频率的桶
```

## 4. 缓存一致性保证

### 缓存失效策略
```python
def invalidate_cache_pattern(cache, key, data_source):
    """缓存失效模式：数据更新时删除缓存"""
    data_source.update(key, new_value)
    cache.delete(key)  # 删除缓存，迫使下次从数据源加载
```

### 写时更新策略
```python
def write_through_consistency(cache, key, value, data_source):
    """写透一致性：同时更新缓存和数据源"""
    data_source.update(key, value)
    cache.put(key, value)
```

### 事件驱动的缓存更新
```python
class CacheUpdateManager:
    """缓存更新管理器"""
    
    def __init__(self):
        self.subscribers = {}
    
    def subscribe(self, event_type, callback):
        if event_type not in self.subscribers:
            self.subscribers[event_type] = []
        self.subscribers[event_type].append(callback)
    
    def notify(self, event_type, data):
        if event_type in self.subscribers:
            for callback in self.subscribers[event_type]:
                callback(data)
```

## 5. 分布式缓存

### Redis缓存集成
```python
import redis

class RedisCache:
    """Redis缓存封装"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def get(self, key):
        value = self.redis.get(key)
        return json.loads(value) if value else None
    
    def put(self, key, value, ttl=300):
        serialized = json.dumps(value)
        self.redis.setex(key, ttl, serialized)
    
    def delete(self, key):
        self.redis.delete(key)
```

### 分布式缓存模式
```python
class DistributedCacheManager:
    """分布式缓存管理器"""
    
    def __init__(self, nodes):
        self.nodes = nodes
        self.consistent_hash = ConsistentHash(nodes)
    
    def get(self, key):
        node = self.consistent_hash.get_node(key)
        return node.cache.get(key)
    
    def put(self, key, value, ttl=300):
        node = self.consistent_hash.get_node(key)
        node.cache.put(key, value, ttl)
```

## 6. 缓存性能优化

### 缓存预热
```python
def warm_up_cache(cache, hot_keys, data_loader):
    """缓存预热：系统启动时加载热点数据"""
    for key in hot_keys:
        try:
            value = data_loader(key)
            cache.put(key, value)
        except Exception as e:
            print(f"Failed to warm cache for key {key}: {e}")
```

### 缓存监控
```python
def monitor_cache_performance(cache):
    """缓存性能监控"""
    hit_rate = cache.get_hit_rate()
    size = cache.size()
    
    print(f"Cache hit rate: {hit_rate:.2%}")
    print(f"Cache size: {size}")
    
    if hit_rate < 0.8:
        print("Warning: Low cache hit rate")
    
    return {
        'hit_rate': hit_rate,
        'size': size,
        'hits': cache.stats.get('hits', 0),
        'misses': cache.stats.get('misses', 0)
    }
```

## 7. 实践建议

### 缓存设计原则
1. **选择性缓存** - 只缓存频繁访问且计算代价高的数据
2. **合理设置TTL** - 根据数据特性设置合适的过期时间
3. **监控缓存效果** - 持续监控命中率和使用情况
4. **处理缓存穿透** - 使用布隆过滤器等方法防止恶意查询
5. **处理缓存雪崩** - 错开过期时间，避免同时失效

### 常见应用场景
1. **数据库查询缓存** - 缓存查询结果
2. **计算结果缓存** - 缓存昂贵的计算结果
3. **页面片段缓存** - 缓存页面渲染结果
4. **API响应缓存** - 缓存API调用结果

### 性能考量
- **内存使用** - 监控缓存占用内存
- **访问延迟** - 确保缓存访问延迟可接受
- **一致性** - 平衡一致性和性能
- **可用性** - 缓存系统本身的可用性

## 8. 总结

缓存是提升系统性能的重要手段。设计高效的缓存系统需要：

1. **选择合适的缓存策略** - 根据业务特点选择缓存模式
2. **合理配置缓存参数** - TTL、容量、淘汰策略等
3. **保证数据一致性** - 选择合适的一致性策略
4. **持续监控优化** - 监控性能指标，及时调优

成功的缓存系统能够显著提升系统响应速度和用户体验。

class CacheUpdateStrategy(Enum):
    WRITE_THROUGH = "write_through"    # 写透
    WRITE_AROUND = "write_around"      # 绕写
    WRITE_BACK = "write_back"          # 写回

class CacheConsistencyManager:
    """缓存一致性管理器"""
    
    def __init__(self, cache: MultiLevelCache, update_strategy: CacheUpdateStrategy):
        self.cache = cache
        self.update_strategy = update_strategy
        self.invalidation_listeners = []
        self.lock = threading.RLock()
    
    def read_through(self, key: str, data_loader: Callable[[str], Any]) -> Any:
        """读透模式"""
        # 先查缓存
        value = self.cache.get(key)
        if value is not None:
            return value
        
        # 缓存未命中，加载数据
        value = data_loader(key)
        if value is not None:
            self.cache.put(key, value)
        
        return value
    
    def write_through(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """写透模式"""
        # 写入数据源
        success = data_writer(key, value)
        
        if success:
            # 写入缓存
            self.cache.put(key, value)
            
            # 通知失效监听器
            self._notify_invalidation_listeners(key, value)
        
        return success
    
    def write_around(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """绕写模式"""
        # 只写入数据源，不写入缓存
        success = data_writer(key, value)
        
        if success:
            # 失效缓存中可能存在的旧数据
            self.cache.remove(key)
            
            # 通知失效监听器
            self._notify_invalidation_listeners(key, value)
        
        return success
    
    def write_back(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """写回模式"""
        # 先写入缓存
        self.cache.put(key, value)
        
        # 异步写入数据源
        def async_write():
            try:
                success = data_writer(key, value)
                if not success:
                    # 写入失败，需要回滚缓存
                    self.cache.remove(key)
            except Exception as e:
                print(f"Async write failed for key {key}: {e}")
                self.cache.remove(key)
        
        threading.Thread(target=async_write, daemon=True).start()
        
        return True
    
    def add_invalidation_listener(self, listener: Callable[[str, Any], None]):
        """添加缓存失效监听器"""
        with self.lock:
            self.invalidation_listeners.append(listener)
    
    def _notify_invalidation_listeners(self, key: str, value: Any):
        """通知所有失效监听器"""
        for listener in self.invalidation_listeners:
            try:
                listener(key, value)
            except Exception as e:
                print(f"Invalidation listener failed: {e}")

class DistributedCacheInvalidator:
    """分布式缓存失效器"""
    
    def __init__(self, cache: MultiLevelCache, redis_pub_sub):
        self.cache = cache
        self.redis_pub_sub = redis_pub_sub
        self.subscribers = {}
        self._start_listening()
    
    def _start_listening(self):
        """开始监听Redis发布订阅"""
        def listen():
            pubsub = self.redis_pub_sub.pubsub()
            pubsub.subscribe("cache_invalidation")
            
            for message in pubsub.listen():
                if message['type'] == 'message':
                    try:
                        data = eval(message['data'])  # 注意：生产环境应使用JSON
                        key = data['key']
                        self._handle_invalidation_message(key, data)
                    except Exception as e:
                        print(f"Failed to handle invalidation message: {e}")
        
        threading.Thread(target=listen, daemon=True).start()
    
    def invalidate_key(self, key: str, source: str = "unknown"):
        """失效指定key"""
        # 本地失效
        self.cache.remove(key)
        
        # 广播给其他节点
        try:
            self.redis_pub_sub.publish(
                "cache_invalidation", 
                str({'key': key, 'source': source, 'timestamp': time.time()})
            )
        except Exception as e:
            print(f"Failed to publish invalidation message: {e}")
    
    def _handle_invalidation_message(self, key: str, data: dict):
        """处理失效消息"""
        # 验证消息时间戳（避免过期消息）
        if time.time() - data.get('timestamp', 0) > 300:  # 5分钟过期
            return
        
        self.cache.remove(key)
    
    def batch_invalidate(self, keys: List[str], source: str = "unknown"):
        """批量失效"""
        for key in keys:
            self.invalidate_key(key, source)
```

## 3. 缓存失效策略

### TTL管理
```python
import heapq
from typing import Dict, List, Tuple

class TTLCacheManager:
    """TTL缓存管理器"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.expiry_heap = []  # (expire_time, key)
        self.key_expiry_map = {}  # key -> expire_time
        self.lock = threading.RLock()
        self._start_cleanup_thread()
    
    def put_with_ttl(self, key: str, value: Any, ttl: int):
        """带TTL的缓存写入"""
        expire_time = time.time() + ttl
        
        with self.lock:
            # 添加到过期堆
            heapq.heappush(self.expiry_heap, (expire_time, key))
            self.key_expiry_map[key] = expire_time
        
        # 写入缓存
        self.cache.put(key, value, ttl)
    
    def _start_cleanup_thread(self):
        """启动清理线程"""
        def cleanup_expired():
            while True:
                try:
                    self._cleanup_expired_entries()
                    time.sleep(60)  # 每分钟清理一次
                except Exception as e:
                    print(f"Cache cleanup failed: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_expired, daemon=True)
        cleanup_thread.start()
    
    def _cleanup_expired_entries(self):
        """清理过期条目"""
        current_time = time.time()
        expired_keys = []
        
        with self.lock:
            while self.expiry_heap and self.expiry_heap[0][0] <= current_time:
                expire_time, key = heapq.heappop(self.expiry_heap)
                
                # 验证这个key是否真的过期（可能有重复）
                if self.key_expiry_map.get(key) == expire_time:
                    expired_keys.append(key)
                    del self.key_expiry_map[key]
        
        # 批量清理过期key
        for key in expired_keys:
            self.cache.remove(key)
        
        if expired_keys:
            print(f"Cleaned up {len(expired_keys)} expired cache entries")
    
    def get_keys_expiring_soon(self, threshold_seconds: int = 300) -> List[str]:
        """获取即将过期的key"""
        current_time = time.time()
        threshold_time = current_time + threshold_seconds
        
        with self.lock:
            expiring_keys = []
            for expire_time, key in self.expiry_heap:
                if expire_time <= threshold_time:
                    if self.key_expiry_map.get(key) == expire_time:
                        expiring_keys.append(key)
                else:
                    break
        
        return expiring_keys
    
    def refresh_expiring_keys(self, data_loader: Callable[[str], Any], 
                            refresh_threshold: int = 300):
        """刷新即将过期的key"""
        expiring_keys = self.get_keys_expiring_soon(refresh_threshold)
        
        for key in expiring_keys:
            try:
                value = data_loader(key)
                if value is not None:
                    # 重新设置TTL
                    self.put_with_ttl(key, value, refresh_threshold * 2)
            except Exception as e:
                print(f"Failed to refresh key {key}: {e}")

class CacheTagManager:
    """缓存标签管理器"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.tag_index = {}  # tag -> set of keys
        self.key_tags = {}   # key -> set of tags
        self.lock = threading.RLock()
    
    def put_with_tags(self, key: str, value: Any, ttl: int, tags: List[str]):
        """带标签的缓存写入"""
        # 写入缓存
        self.cache.put(key, value, ttl)
        
        # 更新索引
        with self.lock:
            for tag in tags:
                if tag not in self.tag_index:
                    self.tag_index[tag] = set()
                self.tag_index[tag].add(key)
            
            self.key_tags[key] = set(tags)
    
    def invalidate_by_tag(self, tag: str):
        """按标签失效缓存"""
        with self.lock:
            if tag in self.tag_index:
                keys_to_invalidate = self.tag_index[tag].copy()
                
                # 失效所有相关key
                for key in keys_to_invalidate:
                    self.cache.remove(key)
                    # 清理key的标签记录
                    self.key_tags.pop(key, None)
                
                # 清理标签索引
                del self.tag_index[tag]
                
                print(f"Invalidated {len(keys_to_invalidate)} keys with tag: {tag}")
    
    def get_keys_by_tag(self, tag: str) -> List[str]:
        """获取指定标签的所有key"""
        with self.lock:
            return list(self.tag_index.get(tag, set()))
    
    def get_tags_for_key(self, key: str) -> List[str]:
        """获取key的所有标签"""
        with self.lock:
            return list(self.key_tags.get(key, set()))
```

## 4. 缓存性能优化

### 缓存性能监控
```python
import time
from collections import defaultdict, deque
from typing import Dict, List

class CachePerformanceMonitor:
    """缓存性能监控器"""
    
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.metrics = {
            'hits': deque(maxlen=window_size),
            'misses': deque(maxlen=window_size),
            'latencies': defaultdict(lambda: deque(maxlen=100)),
            'evictions': deque(maxlen=window_size)
        }
        self.lock = threading.RLock()
    
    def record_hit(self, key: str, latency: float):
        """记录缓存命中"""
        with self.lock:
            self.metrics['hits'].append({
                'timestamp': time.time(),
                'key': key,
                'latency': latency
            })
            self.metrics['latencies'][key].append(latency)
    
    def record_miss(self, key: str, latency: float):
        """记录缓存未命中"""
        with self.lock:
            self.metrics['misses'].append({
                'timestamp': time.time(),
                'key': key,
                'latency': latency
            })
    
    def record_eviction(self, key: str, reason: str):
        """记录缓存淘汰"""
        with self.lock:
            self.metrics['evictions'].append({
                'timestamp': time.time(),
                'key': key,
                'reason': reason
            })
    
    def get_hit_rate(self, window_minutes: int = 5) -> float:
        """计算命中率"""
        with self.lock:
            cutoff_time = time.time() - (window_minutes * 60)
            
            hits_in_window = [
                m for m in self.metrics['hits'] 
                if m['timestamp'] >= cutoff_time
            ]
            misses_in_window = [
                m for m in self.metrics['misses'] 
                if m['timestamp'] >= cutoff_time
            ]
            
            total_requests = len(hits_in_window) + len(misses_in_window)
            if total_requests == 0:
                return 0.0
            
            return len(hits_in_window) / total_requests
    
    def get_avg_latency(self, key: str = None) -> float:
        """获取平均延迟"""
        with self.lock:
            if key:
                latencies = self.metrics['latencies'][key]
                if not latencies:
                    return 0.0
                return sum(latencies) / len(latencies)
            else:
                # 全局平均延迟
                all_latencies = []
                for key_latencies in self.metrics['latencies'].values():
                    all_latencies.extend(key_latencies)
                
                if not all_latencies:
                    return 0.0
                return sum(all_latencies) / len(all_latencies)
    
    def get_slowest_keys(self, limit: int = 10) -> List[tuple]:
        """获取最慢的key"""
        with self.lock:
            key_latencies = []
            for key, latencies in self.metrics['latencies'].items():
                if latencies:
                    avg_latency = sum(latencies) / len(latencies)
                    key_latencies.append((key, avg_latency, len(latencies)))
            
            return sorted(key_latencies, key=lambda x: x[1], reverse=True)[:limit]
    
    def get_eviction_stats(self) -> Dict:
        """获取淘汰统计"""
        with self.lock:
            eviction_counts = defaultdict(int)
            recent_evictions = self.metrics['evictions'][-100:]  # 最近100次
            
            for eviction in recent_evictions:
                eviction_counts[eviction['reason']] += 1
            
            return dict(eviction_counts)

class AdaptiveCacheSizer:
    """自适应缓存大小调整器"""
    
    def __init__(self, cache: MultiLevelCache, min_size: int = 100, max_size: int = 10000):
        self.cache = cache
        self.min_size = min_size
        self.max_size = max_size
        self.performance_monitor = CachePerformanceMonitor()
        self.current_size = min_size
        self.adjustment_history = []
    
    def analyze_and_adjust(self):
        """分析并调整缓存大小"""
        hit_rate = self.performance_monitor.get_hit_rate()
        avg_latency = self.performance_monitor.get_avg_latency()
        
        # 性能评分算法
        performance_score = hit_rate * 100 - avg_latency * 10
        
        # 根据性能评分调整大小
        new_size = self.current_size
        
        if performance_score < 70:  # 性能较差
            if self.current_size < self.max_size:
                new_size = int(self.current_size * 1.2)  # 增加20%
        elif performance_score > 90:  # 性能很好
            if self.current_size > self.min_size:
                new_size = int(self.current_size * 0.9)  # 减少10%
        
        # 应用调整
        if new_size != self.current_size:
            self._adjust_cache_size(new_size)
            self.adjustment_history.append({
                'timestamp': time.time(),
                'old_size': self.current_size,
                'new_size': new_size,
                'hit_rate': hit_rate,
                'avg_latency': avg_latency,
                'performance_score': performance_score
            })
    
    def _adjust_cache_size(self, new_size: int):
        """调整缓存大小"""
        self.current_size = max(self.min_size, min(self.max_size, new_size))
        print(f"Cache size adjusted to {self.current_size}")
        # 这里应该实际调整缓存的大小
        # 具体的调整逻辑取决于缓存实现

class CacheHealthChecker:
    """缓存健康检查器"""
    
    def __init__(self, cache: MultiLevelCache, performance_monitor: CachePerformanceMonitor):
        self.cache = cache
        self.performance_monitor = performance_monitor
        self.health_metrics = {}
    
    def check_health(self) -> Dict:
        """健康检查"""
        health_status = {
            'status': 'healthy',
            'issues': [],
            'recommendations': [],
            'metrics': {}
        }
        
        # 检查命中率
        hit_rate = self.performance_monitor.get_hit_rate()
        health_status['metrics']['hit_rate'] = hit_rate
        
        if hit_rate < 0.7:
            health_status['issues'].append(f"Low hit rate: {hit_rate:.2%}")
            health_status['recommendations'].append("Consider increasing cache size")
        
        # 检查延迟
        avg_latency = self.performance_monitor.get_avg_latency()
        health_status['metrics']['avg_latency'] = avg_latency
        
        if avg_latency > 100:  # 100ms
            health_status['issues'].append(f"High latency: {avg_latency:.2f}ms")
            health_status['recommendations'].append("Check cache performance and consider optimization")
        
        # 检查淘汰率
        eviction_stats = self.performance_monitor.get_eviction_stats()
        total_evictions = sum(eviction_stats.values())
        health_status['metrics']['evictions'] = eviction_stats
        
        if total_evictions > 100:
            health_status['issues'].append(f"High eviction rate: {total_evictions}")
            health_status['recommendations'].append("Consider increasing cache capacity")
        
        # 确定整体状态
        if len(health_status['issues']) > 3:
            health_status['status'] = 'critical'
        elif len(health_status['issues']) > 0:
            health_status['status'] = 'warning'
        
        return health_status
```

## 5. 高级缓存模式

### 缓存旁路模式
```python
class CacheAsidePattern:
    """缓存旁路模式（Cache-Aside Pattern）"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
    
    def get(self, key: str, data_loader: Callable[[str], Any]) -> Any:
        """获取数据，缓存旁路模式"""
        # 1. 先查缓存
        cached_value = self.cache.get(key)
        if cached_value is not None:
            return cached_value
        
        # 2. 缓存未命中，从数据源加载
        value = data_loader(key)
        
        # 3. 将数据放入缓存
        if value is not None:
            self.cache.put(key, value)
        
        return value
    
    def set(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]) -> bool:
        """设置数据，缓存旁路模式"""
        # 1. 先写入数据源
        success = data_writer(key, value)
        
        if success:
            # 2. 成功后更新缓存
            self.cache.put(key, value)
        
        return success
    
    def delete(self, key: str, data_deleter: Callable[[str], bool]) -> bool:
        """删除数据，缓存旁路模式"""
        # 1. 先从数据源删除
        success = data_deleter(key)
        
        # 2. 无论数据源删除是否成功，都删除缓存
        self.cache.remove(key)
        
        return success

class ReadThroughCache:
    """读透模式缓存"""
    
    def __init__(self, cache: MultiLevelCache, data_loader: Callable[[str], Any]):
        self.cache = cache
        self.data_loader = data_loader
        self.lock = threading.RLock()
        self.pending_loads = {}
    
    def get(self, key: str) -> Any:
        """读透获取"""
        # 先查缓存
        cached_value = self.cache.get(key)
        if cached_value is not None:
            return cached_value
        
        with self.lock:
            # 检查是否已有其他线程在加载这个key
            if key in self.pending_loads:
                # 等待正在进行的加载
                return self.pending_loads[key].result()
            
            # 开始新的加载
            future = threading.Future()
            self.pending_loads[key] = future
        
        try:
            # 加载数据
            value = self.data_loader(key)
            
            # 放入缓存
            if value is not None:
                self.cache.put(key, value)
            
            # 通知等待的线程
            future.set_result(value)
            
            # 清理待加载记录
            with self.lock:
                del self.pending_loads[key]
            
            return value
        
        except Exception as e:
            # 处理异常
            future.set_exception(e)
            with self.lock:
                del self.pending_loads[key]
            raise

class WriteBehindCache:
    """写回模式缓存"""
    
    def __init__(self, cache: MultiLevelCache, batch_size: int = 100):
        self.cache = cache
        self.batch_size = batch_size
        self.write_queue = asyncio.Queue()
        self.pending_writes = []
        self.is_running = False
        self._start_write_worker()
    
    def _start_write_worker(self):
        """启动异步写入工作线程"""
        self.is_running = True
        
        def write_worker():
            while self.is_running:
                try:
                    # 收集一批写入
                    batch = []
                    for _ in range(self.batch_size):
                        try:
                            item = self.write_queue.get_nowait()
                            batch.append(item)
                        except asyncio.QueueEmpty:
                            break
                    
                    if batch:
                        self._flush_batch(batch)
                    
                    time.sleep(1)  # 等待新数据
                
                except Exception as e:
                    print(f"Write worker error: {e}")
        
        threading.Thread(target=write_worker, daemon=True).start()
    
    def set(self, key: str, value: Any, data_writer: Callable[[str, Any], bool]):
        """写回设置"""
        # 立即写入缓存
        self.cache.put(key, value)
        
        # 异步写入数据源
        asyncio.run_coroutine_threadsafe(
            self.write_queue.put((key, value, data_writer)),
            asyncio.get_event_loop()
        )
    
    def _flush_batch(self, batch: List[tuple]):
        """批量写入数据源"""
        for key, value, data_writer in batch:
            try:
                success = data_writer(key, value)
                if not success:
                    print(f"Failed to write data for key {key}")
            except Exception as e:
                print(f"Error writing data for key {key}: {e}")
    
    def flush_all(self):
        """刷新所有待写入数据"""
        batch = []
        while not self.write_queue.empty():
            try:
                item = self.write_queue.get_nowait()
                batch.append(item)
            except asyncio.QueueEmpty:
                break
        
        if batch:
            self._flush_batch(batch)
```

## 3. 高级缓存模式

### 缓存旁路模式
```python
class CacheAsidePattern:
    """缓存旁路模式（Cache-Aside Pattern）"""
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
    
    def get(self, key, data_loader):
        """获取数据，缓存旁路模式"""
        # 1. 先查缓存
        cached_value = self.cache.get(key)
        if cached_value is not None:
            return cached_value
        
        # 2. 缓存未命中，从数据源加载
        value = data_loader(key)
        
        # 3. 将数据放入缓存
        if value is not None:
            self.cache.put(key, value)
        
        return value
    
    def set(self, key, value, data_writer):
        """设置数据，缓存旁路模式"""
        # 1. 先写入数据源
        success = data_writer(key, value)
        
        if success:
            # 2. 成功后更新缓存
            self.cache.put(key, value)
        
        return success
```

## 4. 缓存架构最佳实践

### 缓存设计原则
1. **层次化设计**: L1/L2多层缓存，根据访问频率分层
2. **一致性保障**: 选择合适的缓存一致性策略
3. **性能监控**: 监控命中率、延迟、淘汰率等指标
4. **故障恢复**: 建立完善的缓存故障恢复机制
5. **安全防护**: 防止缓存穿透、击穿、雪崩等问题

### 缓存部署建议
- **容量规划**: 根据业务数据量和访问模式规划缓存容量
- **节点分布**: 在分布式环境中合理分布缓存节点
- **监控告警**: 设置完善的监控和告警机制

### 性能优化要点
- **批量操作**: 优化批量读取和写入操作
- **压缩存储**: 对大对象使用压缩技术
- **异步处理**: 对非关键路径使用异步处理

通过以上高级缓存策略的设计和实现，可以显著提升系统的性能、可用性和可扩展性。