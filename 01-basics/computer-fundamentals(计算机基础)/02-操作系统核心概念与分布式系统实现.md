# 操作系统核心概念与分布式系统实现

## 概述

操作系统是计算机系统的核心，它管理硬件资源、提供服务接口，并确保系统的安全性和可靠性。在分布式系统架构中，操作系统的许多核心概念得到了扩展和应用。本文档将探讨操作系统原理及其在现代分布式系统中的实现和应用。

## 进程管理

### 进程调度算法

```python
import threading
import time
from typing import List, Dict, Optional
from enum import Enum
from dataclasses import dataclass
from queue import PriorityQueue

class ProcessState(Enum):
    NEW = "new"
    READY = "ready"
    RUNNING = "running"
    WAITING = "waiting"
    TERMINATED = "terminated"

class Priority(Enum):
    LOW = 3
    NORMAL = 2
    HIGH = 1
    CRITICAL = 0

@dataclass
class Process:
    pid: int
    name: str
    priority: Priority
    burst_time: int
    arrival_time: int
    state: ProcessState = ProcessState.NEW
    start_time: Optional[int] = None
    completion_time: Optional[int] = None
    waiting_time: int = 0
    turnaround_time: int = 0
    remaining_time: int = 0
    
    def __post_init__(self):
        self.remaining_time = self.burst_time

class ProcessScheduler:
    """进程调度器 - 实现多种调度算法"""
    
    def __init__(self):
        self.processes: List[Process] = []
        self.ready_queue: List[Process] = []
        self.current_time = 0
        self.running_process: Optional[Process] = None
        self.completed_processes: List[Process] = []
        self.scheduler_stats = {
            'total_context_switches': 0,
            'cpu_utilization': 0,
            'avg_waiting_time': 0,
            'avg_turnaround_time': 0
        }
    
    def add_process(self, process: Process):
        """添加进程"""
        process.remaining_time = process.burst_time
        self.processes.append(process)
        if process.arrival_time <= self.current_time:
            self.ready_queue.append(process)
            process.state = ProcessState.READY
    
    def fcfs_scheduling(self) -> Dict:
        """先来先服务调度"""
        # 按到达时间排序
        ready_processes = sorted(self.processes, key=lambda p: p.arrival_time)
        
        for process in ready_processes:
            # 等待进程到达
            if process.arrival_time > self.current_time:
                self.current_time = process.arrival_time
            
            process.start_time = self.current_time
            process.state = ProcessState.RUNNING
            
            # 执行进程
            self.current_time += process.burst_time
            
            process.completion_time = self.current_time
            process.turnaround_time = process.completion_time - process.arrival_time
            process.waiting_time = process.turnaround_time - process.burst_time
            process.state = ProcessState.TERMINATED
            self.completed_processes.append(process)
        
        return self._calculate_metrics()
    
    def sjf_scheduling(self) -> Dict:
        """短作业优先调度"""
        completed = []
        
        while len(completed) < len(self.processes):
            # 选择到达时间<=当前时间且剩余时间最短的进程
            available = [p for p in self.processes if p.arrival_time <= self.current_time and p not in completed]
            
            if not available:
                # 没有可用进程，时间快进
                self.current_time = min(p.arrival_time for p in self.processes if p not in completed)
                continue
            
            # 选择最短作业
            shortest = min(available, key=lambda p: p.burst_time)
            shortest.start_time = self.current_time
            shortest.state = ProcessState.RUNNING
            
            # 执行进程
            self.current_time += shortest.burst_time
            
            shortest.completion_time = self.current_time
            shortest.turnaround_time = shortest.completion_time - shortest.arrival_time
            shortest.waiting_time = shortest.turnaround_time - shortest.burst_time
            shortest.state = ProcessState.TERMINATED
            completed.append(shortest)
        
        return self._calculate_metrics()
    
    def priority_scheduling(self, preemptive: bool = False) -> Dict:
        """优先级调度"""
        completed = []
        
        while len(completed) < len(self.processes):
            # 选择到达时间<=当前时间且优先级最高的进程
            available = [p for p in self.processes if p.arrival_time <= self.current_time and p not in completed]
            
            if not available:
                self.current_time = min(p.arrival_time for p in self.processes if p not in completed)
                continue
            
            # 选择最高优先级（数值越小优先级越高）
            highest_priority = min(available, key=lambda p: p.priority.value)
            
            if preemptive and self.running_process:
                # 抢占式：检查是否有更高优先级的进程到达
                if highest_priority.priority.value < self.running_process.priority.value:
                    self.running_process.state = ProcessState.READY
                    self.ready_queue.append(self.running_process)
                    self.running_process = highest_priority
                else:
                    highest_priority = self.running_process
            else:
                self.running_process = highest_priority
            
            self.running_process.start_time = self.current_time
            self.running_process.state = ProcessState.RUNNING
            
            # 执行一个时间单位（简化实现）
            self.current_time += 1
            self.running_process.remaining_time -= 1
            
            if self.running_process.remaining_time == 0:
                self.running_process.completion_time = self.current_time
                self.running_process.turnaround_time = self.running_process.completion_time - self.running_process.arrival_time
                self.running_process.waiting_time = self.running_process.turnaround_time - self.running_process.burst_time
                self.running_process.state = ProcessState.TERMINATED
                completed.append(self.running_process)
                self.running_process = None
        
        return self._calculate_metrics()
    
    def round_robin_scheduling(self, time_quantum: int = 4) -> Dict:
        """时间片轮转调度"""
        completed = []
        ready_queue = []
        
        # 初始化就绪队列
        self.current_time = 0
        for process in self.processes:
            if process.arrival_time == 0:
                ready_queue.append(process)
                process.state = ProcessState.READY
        
        while len(completed) < len(self.processes):
            if ready_queue:
                current_process = ready_queue.pop(0)
                
                if current_process.state == ProcessState.TERMINATED:
                    completed.append(current_process)
                    continue
                
                # 执行时间片或剩余时间（取较小值）
                execution_time = min(time_quantum, current_process.remaining_time)
                current_process.start_time = self.current_time
                current_process.state = ProcessState.RUNNING
                
                self.current_time += execution_time
                current_process.remaining_time -= execution_time
                
                # 检查是否有新进程到达
                for process in self.processes:
                    if (process.arrival_time <= self.current_time and 
                        process not in ready_queue and 
                        process not in completed and
                        process != current_process):
                        process.state = ProcessState.READY
                        ready_queue.append(process)
                
                if current_process.remaining_time > 0:
                    current_process.state = ProcessState.READY
                    ready_queue.append(current_process)
                else:
                    current_process.completion_time = self.current_time
                    current_process.turnaround_time = current_process.completion_time - current_process.arrival_time
                    current_process.waiting_time = current_process.turnaround_time - current_process.burst_time
                    current_process.state = ProcessState.TERMINATED
                    completed.append(current_process)
            else:
                # 就绪队列为空，时间快进
                next_arrival = min(p.arrival_time for p in self.processes if p not in completed)
                self.current_time = next_arrival
        
        return self._calculate_metrics()
    
    def _calculate_metrics(self) -> Dict:
        """计算调度指标"""
        if not self.processes:
            return {}
        
        total_waiting_time = sum(p.waiting_time for p in self.processes)
        total_turnaround_time = sum(p.turnaround_time for p in self.processes)
        
        avg_waiting_time = total_waiting_time / len(self.processes)
        avg_turnaround_time = total_turnaround_time / len(self.processes)
        
        # 计算CPU利用率（简化）
        total_burst_time = sum(p.burst_time for p in self.processes)
        total_time = self.current_time
        cpu_utilization = (total_burst_time / total_time * 100) if total_time > 0 else 0
        
        return {
            'avg_waiting_time': avg_waiting_time,
            'avg_turnaround_time': avg_turnaround_time,
            'cpu_utilization': cpu_utilization,
            'total_time': total_time,
            'throughput': len(self.processes) / total_time if total_time > 0 else 0
        }

class DistributedProcessManager:
    """分布式进程管理器"""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.scheduler = ProcessScheduler()
        self.process_registry: Dict[int, Dict] = {}
        self.remote_processes: Dict[str, List[int]] = {}  # 节点ID -> 进程ID列表
        self.load_balancer = LoadBalancer()
        self.heartbeat_interval = 5
    
    def create_process(self, name: str, priority: Priority, burst_time: int, 
                      estimated_memory: int) -> int:
        """创建进程"""
        # 估算资源需求
        cpu_cores_needed = 1
        memory_needed = estimated_memory
        
        # 使用负载均衡器选择最佳节点
        target_node = self.load_balancer.select_optimal_node(
            self.get_cluster_info(), cpu_cores_needed, memory_needed
        )
        
        # 生成进程ID
        pid = self._generate_pid()
        
        process_info = {
            'pid': pid,
            'name': name,
            'node_id': target_node,
            'priority': priority,
            'state': ProcessState.NEW,
            'resources': {
                'cpu_cores': cpu_cores_needed,
                'memory': memory_needed
            },
            'created_at': time.time()
        }
        
        self.process_registry[pid] = process_info
        
        # 如果目标节点是当前节点，创建本地进程
        if target_node == self.node_id:
            process = Process(pid, name, priority, burst_time, 0)
            self.scheduler.add_process(process)
        else:
            # 远程进程，通过消息传递
            self._deploy_remote_process(target_node, process_info, burst_time)
        
        return pid
    
    def migrate_process(self, pid: int, target_node: str) -> bool:
        """迁移进程到目标节点"""
        if pid not in self.process_registry:
            return False
        
        process_info = self.process_registry[pid]
        
        if process_info['node_id'] == target_node:
            return True  # 已在目标节点
        
        # 检查目标节点资源
        if not self._check_resources(target_node, process_info['resources']):
            return False
        
        # 执行迁移
        success = self._transfer_process_state(process_info, target_node)
        if success:
            # 更新进程注册信息
            process_info['node_id'] = target_node
            self._update_cluster_info(pid, target_node)
        
        return success
    
    def _generate_pid(self) -> int:
        """生成唯一进程ID"""
        # 简单的PID生成策略，实际实现应考虑集群范围的唯一性
        import uuid
        return abs(hash(f"{self.node_id}_{time.time()}_{uuid.uuid4()}")) % 100000
    
    def _deploy_remote_process(self, target_node: str, process_info: Dict, burst_time: int):
        """部署远程进程"""
        # 这里应该实现真正的分布式消息传递
        print(f"部署进程 {process_info['pid']} 到节点 {target_node}")
    
    def _check_resources(self, node_id: str, required_resources: Dict) -> bool:
        """检查节点资源"""
        # 简化实现，实际需要查询节点的实时资源状态
        return True
    
    def _transfer_process_state(self, process_info: Dict, target_node: str) -> bool:
        """传输进程状态"""
        # 简化实现，实际需要实现状态序列化和传输
        return True
    
    def _update_cluster_info(self, pid: int, new_node: str):
        """更新集群信息"""
        # 更新进程在新节点的位置信息
        pass
    
    def get_cluster_info(self) -> Dict:
        """获取集群信息"""
        # 简化实现，实际需要从集群管理系统获取
        return {
            'nodes': [
                {'id': 'node-1', 'cpu_cores': 8, 'memory': 16*1024, 'load': 0.3},
                {'id': 'node-2', 'cpu_cores': 4, 'memory': 8*1024, 'load': 0.7},
                {'id': 'node-3', 'cpu_cores': 16, 'memory': 32*1024, 'load': 0.1}
            ]
        }

class LoadBalancer:
    """负载均衡器"""
    
    def select_optimal_node(self, cluster_info: Dict, required_cores: int, 
                           required_memory: int) -> str:
        """选择最优节点"""
        available_nodes = []
        
        for node in cluster_info['nodes']:
            if (node['cpu_cores'] >= required_cores and 
                node['memory'] >= required_memory and
                node['load'] < 0.8):  # 负载阈值
                available_nodes.append(node)
        
        if not available_nodes:
            # 如果没有满足要求的节点，选择负载最低的
            return min(cluster_info['nodes'], key=lambda n: n['load'])['id']
        
        # 选择负载最低的可用节点
        optimal_node = min(available_nodes, key=lambda n: n['load'])
        return optimal_node['id']
```

### 线程池管理

```python
import threading
import queue
import time
from concurrent.futures import ThreadPoolExecutor, Future
from typing import Callable, Any, Optional
from enum import Enum

class ThreadState(Enum):
    IDLE = "idle"
    BUSY = "busy"
    SHUTDOWN = "shutdown"

class WorkItem:
    """工作项"""
    
    def __init__(self, func: Callable, args: tuple, kwargs: dict, future: Future):
        self.func = func
        self.args = args
        self.kwargs = kwargs
        self.future = future
        self.submitted_at = time.time()
        self.started_at: Optional[float] = None
        self.completed_at: Optional[float] = None

class WorkerThread(threading.Thread):
    """工作线程"""
    
    def __init__(self, worker_id: int, work_queue: queue.Queue, 
                 shutdown_event: threading.Event, stats_collector: 'StatsCollector'):
        super().__init__()
        self.worker_id = worker_id
        self.work_queue = work_queue
        self.shutdown_event = shutdown_event
        self.stats_collector = stats_collector
        self.current_state = ThreadState.IDLE
        self.current_task: Optional[WorkItem] = None
    
    def run(self):
        """工作线程主循环"""
        while not self.shutdown_event.is_set():
            try:
                # 等待工作项
                work_item = self.work_queue.get(timeout=1.0)
                
                if work_item is None:  # 收到停止信号
                    break
                
                self.current_state = ThreadState.BUSY
                self.current_task = work_item
                work_item.started_at = time.time()
                
                # 执行任务
                try:
                    result = work_item.func(*work_item.args, **work_item.kwargs)
                    work_item.future.set_result(result)
                except Exception as e:
                    work_item.future.set_exception(e)
                
                work_item.completed_at = time.time()
                
                # 记录统计信息
                execution_time = work_item.completed_at - work_item.started_at
                self.stats_collector.record_task_completion(
                    self.worker_id, execution_time
                )
                
            except queue.Empty:
                # 队列为空，继续等待
                continue
            except Exception as e:
                print(f"Worker {self.worker_id} error: {e}")
            finally:
                self.current_state = ThreadState.IDLE
                self.current_task = None

class StatsCollector:
    """统计信息收集器"""
    
    def __init__(self):
        self.stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'total_execution_time': 0,
            'worker_stats': {}
        }
        self.lock = threading.Lock()
    
    def record_task_completion(self, worker_id: int, execution_time: float):
        """记录任务完成"""
        with self.lock:
            self.stats['total_tasks'] += 1
            self.stats['completed_tasks'] += 1
            self.stats['total_execution_time'] += execution_time
            
            if worker_id not in self.stats['worker_stats']:
                self.stats['worker_stats'][worker_id] = {
                    'tasks_completed': 0,
                    'total_time': 0,
                    'avg_time': 0
                }
            
            worker_stat = self.stats['worker_stats'][worker_id]
            worker_stat['tasks_completed'] += 1
            worker_stat['total_time'] += execution_time
            worker_stat['avg_time'] = worker_stat['total_time'] / worker_stat['tasks_completed']
    
    def record_task_failure(self):
        """记录任务失败"""
        with self.lock:
            self.stats['total_tasks'] += 1
            self.stats['failed_tasks'] += 1
    
    def get_stats(self) -> Dict:
        """获取统计信息"""
        with self.lock:
            stats_copy = self.stats.copy()
            stats_copy['avg_execution_time'] = (
                stats_copy['total_execution_time'] / stats_copy['completed_tasks']
                if stats_copy['completed_tasks'] > 0 else 0
            )
            return stats_copy

class ThreadPoolManager:
    """线程池管理器"""
    
    def __init__(self, min_workers: int = 4, max_workers: int = 16, 
                 work_queue_size: int = 1000):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.work_queue_size = work_queue_size
        
        self.work_queue = queue.Queue(maxsize=work_queue_size)
        self.shutdown_event = threading.Event()
        self.workers: List[WorkerThread] = []
        self.stats_collector = StatsCollector()
        
        self._current_workers = 0
        self._lock = threading.Lock()
    
    def start(self):
        """启动线程池"""
        # 启动最小工作线程数
        for i in range(self.min_workers):
            self._add_worker()
    
    def _add_worker(self) -> WorkerThread:
        """添加工作线程"""
        with self._lock:
            if self._current_workers >= self.max_workers:
                raise RuntimeError("Maximum workers reached")
            
            worker_id = self._current_workers
            self._current_workers += 1
            
            worker = WorkerThread(
                worker_id, self.work_queue, self.shutdown_event, self.stats_collector
            )
            worker.daemon = True
            worker.start()
            
            self.workers.append(worker)
            return worker
    
    def submit_task(self, func: Callable, *args, **kwargs) -> Future:
        """提交任务"""
        if self.shutdown_event.is_set():
            raise RuntimeError("Thread pool is shutting down")
        
        # 创建Future
        future = Future()
        work_item = WorkItem(func, args, kwargs, future)
        
        try:
            self.work_queue.put(work_item, timeout=1.0)
        except queue.Full:
            # 队列满，尝试增加工作线程
            try:
                self._add_worker()
                self.work_queue.put(work_item, timeout=1.0)
            except (queue.Full, RuntimeError):
                future.set_exception(RuntimeError("Work queue is full and cannot add more workers"))
        
        return future
    
    def execute_with_timeout(self, func: Callable, timeout: float, *args, **kwargs) -> Any:
        """带超时的任务执行"""
        future = self.submit_task(func, *args, **kwargs)
        return future.result(timeout=timeout)
    
    def scale_workers(self, target_count: int):
        """动态调整工作线程数"""
        with self._lock:
            if target_count < self.min_workers:
                target_count = self.min_workers
            elif target_count > self.max_workers:
                target_count = self.max_workers
            
            current_count = len(self.workers)
            
            if target_count > current_count:
                # 增加工作线程
                for _ in range(target_count - current_count):
                    self._add_worker()
            elif target_count < current_count:
                # 减少工作线程（发送停止信号）
                for _ in range(current_count - target_count):
                    if self.workers:
                        worker = self.workers.pop()
                        # 在工作线程中添加停止机制
                        self.work_queue.put(None)  # 停止信号
    
    def get_queue_status(self) -> Dict:
        """获取队列状态"""
        return {
            'queue_size': self.work_queue.qsize(),
            'max_queue_size': self.work_queue.maxsize,
            'current_workers': len(self.workers),
            'min_workers': self.min_workers,
            'max_workers': self.max_workers
        }
    
    def shutdown(self, wait: bool = True, timeout: float = 30.0):
        """关闭线程池"""
        self.shutdown_event.set()
        
        # 发送停止信号给所有工作线程
        for _ in range(len(self.workers)):
            try:
                self.work_queue.put_nowait(None)
            except queue.Full:
                break
        
        if wait:
            end_time = time.time() + timeout
            for worker in self.workers:
                remaining_time = end_time - time.time()
                if remaining_time > 0:
                    worker.join(timeout=remaining_time)
        
        # 清理资源
        self.workers.clear()

# 使用示例
def demo_thread_pool():
    """线程池演示"""
    def cpu_intensive_task(n):
        """CPU密集型任务"""
        result = sum(i * i for i in range(n))
        time.sleep(0.1)  # 模拟I/O
        return result
    
    def io_intensive_task(duration):
        """I/O密集型任务"""
        time.sleep(duration)
        return f"Task completed after {duration} seconds"
    
    # 创建线程池
    pool = ThreadPoolManager(min_workers=2, max_workers=8)
    pool.start()
    
    # 提交任务
    futures = []
    for i in range(10):
        if i % 2 == 0:
            future = pool.submit_task(cpu_intensive_task, 1000)
        else:
            future = pool.submit_task(io_intensive_task, 0.1)
        futures.append(future)
    
    # 等待结果
    results = []
    for i, future in enumerate(futures):
        try:
            result = future.result(timeout=2.0)
            results.append(result)
            print(f"Task {i}: {result}")
        except Exception as e:
            print(f"Task {i} failed: {e}")
    
    # 获取统计信息
    stats = pool.stats_collector.get_stats()
    print(f"Thread Pool Stats: {stats}")
    
    # 获取队列状态
    queue_status = pool.get_queue_status()
    print(f"Queue Status: {queue_status}")
    
    # 关闭线程池
    pool.shutdown()

# 如果作为主模块运行
if __name__ == "__main__":
    demo_thread_pool()
```

## 内存管理

### 虚拟内存系统

```python
import mmap
import struct
from typing import Dict, List, Optional, Set
from enum import Enum

class PageState(Enum):
    FREE = "free"
    ALLOCATED = "allocated"
    IN_MEMORY = "in_memory"
    ON_DISK = "on_disk"

class MemoryPage:
    """内存页"""
    
    def __init__(self, page_id: int, size: int = 4096):
        self.page_id = page_id
        self.size = size
        self.state = PageState.FREE
        self.data = bytearray(size)
        self.disk_location: Optional[str] = None
        self.last_accessed = 0.0
        self.access_count = 0
        self.dirty = False  # 页面是否被修改

class VirtualMemoryManager:
    """虚拟内存管理器"""
    
    def __init__(self, total_memory: int, page_size: int = 4096, 
                 swap_file: str = "swap.bin"):
        self.page_size = page_size
        self.total_pages = total_memory // page_size
        self.pages: List[MemoryPage] = []
        self.free_pages: List[int] = []
        self.page_table: Dict[int, int] = {}  # 虚拟页号 -> 物理页号
        
        # 分页统计
        self.stats = {
            'page_faults': 0,
            'page_hits': 0,
            'swaps_in': 0,
            'swaps_out': 0,
            'total_allocated': 0
        }
        
        # 初始化页面
        self._initialize_pages()
        
        # 页面替换算法状态
        self.clock_hand = 0  # 时钟算法的指针
    
    def _initialize_pages(self):
        """初始化内存页面"""
        for i in range(self.total_pages):
            page = MemoryPage(i, self.page_size)
            self.pages.append(page)
            self.free_pages.append(i)
    
    def allocate_pages(self, num_pages: int) -> List[int]:
        """分配页面"""
        if len(self.free_pages) < num_pages:
            # 需要页面替换
            self._evict_pages(num_pages - len(self.free_pages))
        
        allocated_pages = []
        for _ in range(num_pages):
            if self.free_pages:
                page_id = self.free_pages.pop(0)
                page = self.pages[page_id]
                page.state = PageState.ALLOCATED
                allocated_pages.append(page_id)
                self.stats['total_allocated'] += 1
        
        return allocated_pages
    
    def _evict_pages(self, num_pages: int):
        """页面置换"""
        evicted_count = 0
        
        while evicted_count < num_pages and self.free_pages:
            # 使用时钟算法进行页面置换
            page_id = self.clock_hand
            page = self.pages[page_id]
            
            if page.access_count > 0:
                # 减少访问计数，继续寻找
                page.access_count //= 2
            else:
                # 页面没有被引用，可以置换
                self._swap_out_page(page_id)
                evicted_count += 1
            
            self.clock_hand = (self.clock_hand + 1) % self.total_pages
    
    def _swap_out_page(self, page_id: int):
        """将页面置换到磁盘"""
        page = self.pages[page_id]
        
        if page.state == PageState.IN_MEMORY:
            page.state = PageState.ON_DISK
            self.stats['swaps_out'] += 1
            # 实际实现中应该写入磁盘
            page.disk_location = f"page_{page_id}.dat"
            
            # 从页表中移除映射
            virtual_page = self._find_virtual_page(page_id)
            if virtual_page is not None:
                del self.page_table[virtual_page]
    
    def _find_virtual_page(self, physical_page_id: int) -> Optional[int]:
        """查找虚拟页号"""
        for virtual_page, physical_page in self.page_table.items():
            if physical_page == physical_page_id:
                return virtual_page
        return None
    
    def map_virtual_page(self, virtual_page: int, physical_page: int) -> bool:
        """映射虚拟页面到物理页面"""
        if physical_page < len(self.pages):
            page = self.pages[physical_page]
            
            if page.state in [PageState.ALLOCATED, PageState.IN_MEMORY]:
                self.page_table[virtual_page] = physical_page
                page.state = PageState.IN_MEMORY
                return True
        
        return False
    
    def access_page(self, virtual_page: int, read_only: bool = False) -> Optional[bytearray]:
        """访问页面"""
        if virtual_page in self.page_table:
            # 页面在内存中
            physical_page = self.page_table[virtual_page]
            page = self.pages[physical_page]
            page.last_accessed = time.time()
            page.access_count += 1
            self.stats['page_hits'] += 1
            
            if not read_only:
                page.dirty = True
            
            return page.data
        else:
            # 页面错误，需要换入
            self.stats['page_faults'] += 1
            return self._handle_page_fault(virtual_page)
    
    def _handle_page_fault(self, virtual_page: int) -> Optional[bytearray]:
        """处理页面错误"""
        # 找到空闲页面或置换页面
        if self.free_pages:
            physical_page = self.free_pages.pop(0)
        else:
            # 需要置换页面
            physical_page = self._select_victim_page()
        
        # 换入页面
        self._swap_in_page(virtual_page, physical_page)
        
        page = self.pages[physical_page]
        page.last_accessed = time.time()
        page.access_count = 1
        
        return page.data
    
    def _swap_in_page(self, virtual_page: int, physical_page: int):
        """将页面从磁盘换入内存"""
        page = self.pages[physical_page]
        
        if page.state == PageState.ON_DISK:
            # 从磁盘读取页面数据
            self.stats['swaps_in'] += 1
            page.state = PageState.IN_MEMORY
        
        self.page_table[virtual_page] = physical_page
    
    def _select_victim_page(self) -> int:
        """选择被置换的页面"""
        # 简化的时钟算法
        while True:
            candidate_page = self.pages[self.clock_hand]
            
            if candidate_page.access_count == 0:
                if candidate_page.dirty:
                    # 如果页面被修改，需要写回磁盘
                    self._write_page_to_disk(self.clock_hand)
                self.clock_hand = (self.clock_hand + 1) % self.total_pages
                return self.clock_hand - 1
            else:
                candidate_page.access_count = 0
                self.clock_hand = (self.clock_hand + 1) % self.total_pages
    
    def _write_page_to_disk(self, page_id: int):
        """将页面写入磁盘"""
        page = self.pages[page_id]
        if page.state == PageState.IN_MEMORY:
            # 实际实现中应该写入磁盘文件
            page.state = PageState.ON_DISK
            page.dirty = False

class DistributedCache:
    """分布式缓存系统 - 模拟虚拟内存在分布式环境中的应用"""
    
    def __init__(self, node_id: str, memory_limit: int = 100 * 1024 * 1024):  # 100MB
        self.node_id = node_id
        self.memory_limit = memory_limit
        self.cache_data: Dict[str, any] = {}
        self.access_history: List[tuple] = []  # (key, timestamp, size)
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'memory_used': 0
        }
        self.lock = threading.RLock()
    
    def get(self, key: str) -> Optional[any]:
        """获取缓存项"""
        with self.lock:
            if key in self.cache_data:
                self.stats['hits'] += 1
                
                # 更新访问历史
                value, size = self.cache_data[key]
                self.access_history.append((key, time.time(), size))
                
                # 保持历史记录在合理范围内
                if len(self.access_history) > 1000:
                    self.access_history = self.access_history[-500:]
                
                return value
            else:
                self.stats['misses'] += 1
                return None
    
    def put(self, key: str, value: any, size: int = None):
        """存储缓存项"""
        with self.lock:
            if size is None:
                size = len(str(value))
            
            # 检查内存限制
            current_usage = sum(item[2] for item in self.cache_data.values())
            if current_usage + size > self.memory_limit:
                self._evict_items(size)
            
            self.cache_data[key] = (value, size)
            self.stats['memory_used'] = sum(item[2] for item in self.cache_data.values())
    
    def _evict_items(self, needed_space: int):
        """使用LRU算法驱逐项目"""
        if not self.access_history:
            return
        
        # 按时间排序
        sorted_history = sorted(self.access_history, key=lambda x: x[1])
        
        evicted_size = 0
        evicted_keys = []
        
        for key, timestamp, size in sorted_history:
            if key in self.cache_data:
                del self.cache_data[key]
                evicted_size += size
                evicted_keys.append(key)
                self.stats['evictions'] += 1
                
                if evicted_size >= needed_space:
                    break
        
        # 更新访问历史
        self.access_history = [item for item in self.access_history 
                             if item[0] not in evicted_keys]
        
        self.stats['memory_used'] = sum(item[2] for item in self.cache_data.values())
    
    def get_stats(self) -> Dict:
        """获取缓存统计信息"""
        with self.lock:
            hit_rate = self.stats['hits'] / (self.stats['hits'] + self.stats['misses']) if (self.stats['hits'] + self.stats['misses']) > 0 else 0
            return {
                'hits': self.stats['hits'],
                'misses': self.stats['misses'],
                'evictions': self.stats['evictions'],
                'hit_rate': hit_rate,
                'memory_used': self.stats['memory_used'],
                'memory_limit': self.memory_limit,
                'memory_utilization': self.stats['memory_used'] / self.memory_limit
            }
```

### 垃圾回收算法

```python
import weakref
import gc
from typing import Set, Dict, List, Optional
from enum import Enum

class ObjectState(Enum):
    UNMARKED = "unmarked"
    MARKED = "marked"
    SCANNED = "scanned"

class MemoryObject:
    """内存对象"""
    
    def __init__(self, obj_id: int, size: int = 1024):
        self.obj_id = obj_id
        self.size = size
        self.state = ObjectState.UNMARKED
        self.references: Set[int] = set()  # 引用其他对象的ID
        self.referenced_by: Set[int] = set()  # 被其他对象引用的ID
        self.finalize_callback: Optional[callable] = None
        self.creation_time = time.time()
        self.last_access_time = time.time()

class GarbageCollector:
    """垃圾回收器"""
    
    def __init__(self):
        self.objects: Dict[int, MemoryObject] = {}
        self.roots: Set[int] = set()  # 根对象
        self.stats = {
            'total_collections': 0,
            'objects_collected': 0,
            'memory_freed': 0,
            'collection_time': 0
        }
        self.finalization_queue: List[MemoryObject] = []
    
    def create_object(self, obj_id: int, size: int = 1024) -> MemoryObject:
        """创建新对象"""
        obj = MemoryObject(obj_id, size)
        self.objects[obj_id] = obj
        return obj
    
    def add_reference(self, from_obj_id: int, to_obj_id: int):
        """添加对象引用"""
        if from_obj_id in self.objects and to_obj_id in self.objects:
            from_obj = self.objects[from_obj_id]
            to_obj = self.objects[to_obj_id]
            
            from_obj.references.add(to_obj_id)
            to_obj.referenced_by.add(from_obj_id)
    
    def remove_reference(self, from_obj_id: int, to_obj_id: int):
        """移除对象引用"""
        if from_obj_id in self.objects and to_obj_id in self.objects:
            from_obj = self.objects[from_obj_id]
            to_obj = self.objects[to_obj_id]
            
            from_obj.references.discard(to_obj_id)
            to_obj.referenced_by.discard(from_obj_id)
    
    def mark_roots(self):
        """标记根对象"""
        for root_id in self.roots:
            if root_id in self.objects:
                self.objects[root_id].state = ObjectState.MARKED
    
    def mark_reachable_objects(self):
        """标记可达对象"""
        # 使用深度优先搜索
        stack = [obj for obj in self.objects.values() if obj.state == ObjectState.MARKED]
        
        while stack:
            current = stack.pop()
            
            for ref_id in current.references:
                if ref_id in self.objects:
                    ref_obj = self.objects[ref_id]
                    if ref_obj.state == ObjectState.UNMARKED:
                        ref_obj.state = ObjectState.MARKED
                        stack.append(ref_obj)
        
        # 将已标记的对象标记为已扫描
        for obj in self.objects.values():
            if obj.state == ObjectState.MARKED:
                obj.state = ObjectState.SCANNED
    
    def sweep_unreachable_objects(self) -> int:
        """清扫不可达对象"""
        unreachable = []
        
        for obj_id, obj in self.objects.items():
            if obj.state == ObjectState.UNMARKED:
                unreachable.append(obj_id)
        
        for obj_id in unreachable:
            obj = self.objects[obj_id]
            
            # 执行清理回调
            if obj.finalize_callback:
                try:
                    obj.finalize_callback(obj_id)
                except Exception as e:
                    print(f"Finalization callback error for object {obj_id}: {e}")
            
            self.stats['objects_collected'] += 1
            self.stats['memory_freed'] += obj.size
            
            # 移除引用关系
            for ref_id in list(obj.references):
                self.remove_reference(obj_id, ref_id)
            
            # 移除被引用关系
            for ref_by_id in list(obj.referenced_by):
                self.remove_reference(ref_by_id, obj_id)
            
            del self.objects[obj_id]
        
        return len(unreachable)
    
    def collect_garbage(self) -> Dict:
        """执行垃圾回收"""
        start_time = time.time()
        
        # 重置所有对象状态
        for obj in self.objects.values():
            obj.state = ObjectState.UNMARKED
        
        # 标记根对象和可达对象
        self.mark_roots()
        self.mark_reachable_objects()
        
        # 清扫不可达对象
        collected_count = self.sweep_unreachable_objects()
        
        # 更新统计信息
        collection_time = time.time() - start_time
        self.stats['total_collections'] += 1
        self.stats['collection_time'] += collection_time
        
        return {
            'collected_objects': collected_count,
            'memory_freed': self.stats['memory_freed'],
            'collection_time': collection_time,
            'remaining_objects': len(self.objects)
        }

class GenerationalGarbageCollector:
    """分代垃圾回收器"""
    
    def __init__(self, generations: int = 3):
        self.generations = generations
        self.generation_objects = [set() for _ in range(generations)]
        self.generation_thresholds = [100, 500, 1000]  # 每代触发GC的对象数量阈值
        self.collections = 0
        
        # 每代的统计信息
        self.gen_stats = [
            {'collections': 0, 'objects_collected': 0} for _ in range(generations)
        ]
    
    def allocate_object(self, obj_id: int, generation: int = 0) -> bool:
        """在指定代分配对象"""
        if 0 <= generation < self.generations:
            self.generation_objects[generation].add(obj_id)
            
            # 检查是否需要触发GC
            if len(self.generation_objects[generation]) >= self.generation_thresholds[generation]:
                return self.collect_generation(generation)
            return True
        return False
    
    def collect_generation(self, generation: int) -> bool:
        """对指定代进行垃圾回收"""
        if generation >= self.generations:
            return False
        
        objects_to_check = self.generation_objects[generation].copy()
        marked_objects = set()
        
        # 标记阶段
        self._mark_objects(objects_to_check, marked_objects)
        
        # 清理阶段
        unreachable = objects_to_check - marked_objects
        collected = len(unreachable)
        
        # 更新代的对象集合
        self.generation_objects[generation] = marked_objects
        
        # 升级幸存对象到下一代
        if generation < self.generations - 1:
            for obj_id in marked_objects:
                self.generation_objects[generation + 1].add(obj_id)
        
        # 更新统计
        self.gen_stats[generation]['collections'] += 1
        self.gen_stats[generation]['objects_collected'] += collected
        self.collections += 1
        
        return True
    
    def _mark_objects(self, objects: Set[int], marked: Set[int]):
        """标记可达对象（简化实现）"""
        # 简化的标记实现，实际需要跟踪引用关系
        marked.update(objects)
    
    def promote_survivors(self, generation: int) -> Set[int]:
        """提升幸存对象到下一代"""
        if generation >= self.generations - 1:
            return set()
        
        survivors = self.generation_objects[generation].copy()
        self.generation_objects[generation].clear()
        self.generation_objects[generation + 1].update(survivors)
        
        return survivors
    
    def get_generation_stats(self) -> List[Dict]:
        """获取分代统计信息"""
        stats = []
        for i in range(self.generations):
            stats.append({
                'generation': i,
                'objects': len(self.generation_objects[i]),
                'collections': self.gen_stats[i]['collections'],
                'objects_collected': self.gen_stats[i]['objects_collected'],
                'threshold': self.generation_thresholds[i]
            })
        return stats

# 使用示例
def demo_memory_management():
    """内存管理演示"""
    print("=== 虚拟内存管理演示 ===")
    
    # 创建虚拟内存管理器
    vmm = VirtualMemoryManager(total_memory=64 * 1024)  # 64KB
    print(f"虚拟内存系统初始化完成")
    print(f"总页数: {vmm.total_pages}")
    
    # 分配页面
    allocated_pages = vmm.allocate_pages(8)
    print(f"分配了 {len(allocated_pages)} 个页面: {allocated_pages}")
    
    # 映射虚拟页面
    for i, physical_page in enumerate(allocated_pages):
        vmm.map_virtual_page(i, physical_page)
    
    # 访问页面
    for i in range(len(allocated_pages)):
        data = vmm.access_page(i)
        if data:
            print(f"成功访问虚拟页面 {i}")
    
    print(f"页面错误统计: {vmm.stats}")
    
    print("\n=== 分布式缓存演示 ===")
    
    # 创建分布式缓存
    cache = DistributedCache("node-1", memory_limit=1024)  # 1KB限制
    
    # 添加数据
    cache.put("user:1", {"name": "Alice", "age": 30}, size=512)
    cache.put("user:2", {"name": "Bob", "age": 25}, size=512)
    cache.put("config", {"debug": True}, size=256)
    
    # 访问数据
    user1 = cache.get("user:1")
    user2 = cache.get("user:2")
    config = cache.get("config")
    
    print(f"用户1: {user1}")
    print(f"用户2: {user2}")
    print(f"配置: {config}")
    print(f"缓存统计: {cache.get_stats()}")
    
    print("\n=== 垃圾回收演示 ===")
    
    # 创建垃圾回收器
    gc = GarbageCollector()
    
    # 创建对象
    obj1 = gc.create_object(1, 1024)
    obj2 = gc.create_object(2, 2048)
    obj3 = gc.create_object(3, 1024)
    
    # 设置引用关系
    gc.add_reference(1, 2)  # obj1 -> obj2
    gc.add_reference(2, 3)  # obj2 -> obj3
    
    # 设置根对象
    gc.roots.add(1)
    
    # 移除obj1的引用，obj1和obj2将成为垃圾
    gc.remove_reference(1, 2)
    
    # 执行垃圾回收
    result = gc.collect_garbage()
    print(f"垃圾回收结果: {result}")
    print(f"剩余对象: {len(gc.objects)}")
    
    print("\n=== 分代垃圾回收演示 ===")
    
    # 创建分代垃圾回收器
    gen_gc = GenerationalGarbageCollector()
    
    # 分配对象到不同代
    for i in range(10):
        gen_gc.allocate_object(i, 0)  # 分配到第0代
    
    # 获取分代统计
    stats = gen_gc.get_generation_stats()
    print(f"分代统计: {stats}")

if __name__ == "__main__":
    demo_memory_management()
```

## 文件系统

### 分布式文件系统

```python
import os
import hashlib
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class FileType(Enum):
    REGULAR = "regular"
    DIRECTORY = "directory"
    SYMBOLIC_LINK = "symbolic_link"

class ReplicationStrategy(Enum):
    ROUND_ROBIN = "round_robin"
    CONSISTENT_HASHING = "consistent_hashing"
    RACK_AWARE = "rack_aware"

@dataclass
class FileMetadata:
    """文件元数据"""
    file_id: str
    name: str
    file_type: FileType
    size: int
    created_at: float
    modified_at: float
    owner: str
    permissions: str
    checksum: str
    replication_factor: int = 3
    storage_nodes: List[str] = None
    
    def __post_init__(self):
        if self.storage_nodes is None:
            self.storage_nodes = []

@dataclass
class StorageNode:
    """存储节点"""
    node_id: str
    ip_address: str
    port: int
    capacity: int  # GB
    used_space: int = 0
    rack: str = "default"
    status: str = "active"  # active, inactive, maintenance
    
    def is_available(self) -> bool:
        return self.status == "active" and self.used_space < self.capacity
    
    def can_store(self, size: int) -> bool:
        return self.is_available() and (self.used_space + size) <= self.capacity
    
    def store_data(self, size: int) -> bool:
        if self.can_store(size):
            self.used_space += size
            return True
        return False

class ConsistentHashRing:
    """一致性哈希环"""
    
    def __init__(self, virtual_nodes: int = 150):
        self.virtual_nodes = virtual_nodes
        self.ring = {}
        self.nodes = {}
    
    def add_node(self, node_id: str):
        """添加节点"""
        self.nodes[node_id] = node_id
        for i in range(self.virtual_nodes):
            virtual_key = f"{node_id}:{i}"
            hash_value = int(hashlib.md5(virtual_key.encode()).hexdigest(), 16)
            self.ring[hash_value] = node_id
    
    def remove_node(self, node_id: str):
        """移除节点"""
        if node_id in self.nodes:
            del self.nodes[node_id]
            for i in range(self.virtual_nodes):
                virtual_key = f"{node_id}:{i}"
                hash_value = int(hashlib.md5(virtual_key.encode()).hexdigest(), 16)
                if hash_value in self.ring:
                    del self.ring[hash_value]
    
    def get_nodes(self, key: str, count: int) -> List[str]:
        """获取存储key的节点列表"""
        if not self.ring:
            return []
        
        key_hash = int(hashlib.md5(key.encode()).hexdigest(), 16)
        sorted_keys = sorted(self.ring.keys())
        
        # 找到第一个大于等于key_hash的节点
        nodes = []
        for virtual_key in sorted_keys:
            if virtual_key >= key_hash:
                node = self.ring[virtual_key]
                if node not in nodes:
                    nodes.append(node)
                if len(nodes) >= count:
                    break
        
        # 如果还没找到足够的节点，循环到开头
        for virtual_key in sorted_keys:
            if len(nodes) >= count:
                break
            node = self.ring[virtual_key]
            if node not in nodes:
                nodes.append(node)
        
        return nodes

class DistributedFileSystem:
    """分布式文件系统"""
    
    def __init__(self, replication_strategy: ReplicationStrategy = ReplicationStrategy.CONSISTENT_HASHING):
        self.storage_nodes: Dict[str, StorageNode] = {}
        self.file_metadata: Dict[str, FileMetadata] = {}
        self.replication_strategy = replication_strategy
        self.hash_ring = ConsistentHashRing()
        
        # 统计信息
        self.stats = {
            'total_files': 0,
            'total_storage_used': 0,
            'read_operations': 0,
            'write_operations': 0,
            'replication_operations': 0
        }
        
        # 加载均衡器
        self.load_balancer = StorageLoadBalancer()
    
    def add_storage_node(self, node_id: str, ip: str, port: int, capacity: int, rack: str = "default"):
        """添加存储节点"""
        node = StorageNode(node_id, ip, port, capacity, rack=rack)
        self.storage_nodes[node_id] = node
        self.hash_ring.add_node(node_id)
    
    def remove_storage_node(self, node_id: str):
        """移除存储节点"""
        if node_id in self.storage_nodes:
            # 处理节点上的文件重新分配
            self._handle_node_removal(node_id)
            del self.storage_nodes[node_id]
            self.hash_ring.remove_node(node_id)
    
    def upload_file(self, file_path: str, content: bytes, owner: str, replication_factor: int = 3) -> str:
        """上传文件"""
        file_id = self._generate_file_id(file_path)
        
        # 计算校验和
        checksum = hashlib.md5(content).hexdigest()
        size = len(content)
        
        # 创建文件元数据
        metadata = FileMetadata(
            file_id=file_id,
            name=os.path.basename(file_path),
            file_type=FileType.REGULAR,
            size=size,
            created_at=time.time(),
            modified_at=time.time(),
            owner=owner,
            permissions="rw-r--r--",
            checksum=checksum,
            replication_factor=replication_factor
        )
        
        # 选择存储节点
        storage_nodes = self._select_storage_nodes(file_id, replication_factor)
        metadata.storage_nodes = storage_nodes
        
        # 存储文件内容
        success_count = 0
        for node_id in storage_nodes:
            if node_id in self.storage_nodes:
                node = self.storage_nodes[node_id]
                if node.can_store(size):
                    if self._store_on_node(node_id, file_id, content):
                        success_count += 1
                        node.store_data(size)
        
        if success_count < replication_factor:
            # 存储失败，需要重新选择节点
            additional_nodes = self._select_additional_nodes(file_id, replication_factor - success_count)
            for node_id in additional_nodes:
                if node_id in self.storage_nodes:
                    node = self.storage_nodes[node_id]
                    if node.can_store(size):
                        if self._store_on_node(node_id, file_id, content):
                            success_count += 1
                            node.store_data(size)
                            metadata.storage_nodes.append(node_id)
        
        if success_count >= 1:  # 至少需要一个副本
            self.file_metadata[file_id] = metadata
            self.stats['total_files'] += 1
            self.stats['total_storage_used'] += size * success_count
            self.stats['write_operations'] += 1
            return file_id
        else:
            raise Exception("Failed to store file on any node")
    
    def download_file(self, file_id: str) -> Optional[bytes]:
        """下载文件"""
        if file_id not in self.file_metadata:
            return None
        
        metadata = self.file_metadata[file_id]
        content = None
        
        # 尝试从第一个可用的存储节点下载
        for node_id in metadata.storage_nodes:
            if node_id in self.storage_nodes:
                node = self.storage_nodes[node_id]
                if node.status == "active":
                    content = self._read_from_node(node_id, file_id)
                    if content:
                        break
        
        if content:
            self.stats['read_operations'] += 1
            
            # 验证校验和
            expected_checksum = metadata.checksum
            actual_checksum = hashlib.md5(content).hexdigest()
            
            if expected_checksum != actual_checksum:
                # 校验和验证失败，可能需要从其他副本恢复
                content = self._restore_from_replicas(file_id, metadata)
        
        return content
    
    def delete_file(self, file_id: str) -> bool:
        """删除文件"""
        if file_id not in self.file_metadata:
            return False
        
        metadata = self.file_metadata[file_id]
        
        # 从所有存储节点删除文件
        for node_id in metadata.storage_nodes:
            if node_id in self.storage_nodes:
                self._delete_from_node(node_id, file_id)
                node = self.storage_nodes[node_id]
                node.used_space -= metadata.size
        
        # 删除元数据
        del self.file_metadata[file_id]
        self.stats['total_files'] -= 1
        self.stats['total_storage_used'] -= metadata.size * len(metadata.storage_nodes)
        
        return True
    
    def _generate_file_id(self, file_path: str) -> str:
        """生成文件ID"""
        return hashlib.md5(f"{file_path}_{time.time()}".encode()).hexdigest()
    
    def _select_storage_nodes(self, file_id: str, count: int) -> List[str]:
        """选择存储节点"""
        if self.replication_strategy == ReplicationStrategy.CONSISTENT_HASHING:
            return self.hash_ring.get_nodes(file_id, count)
        elif self.replication_strategy == ReplicationStrategy.ROUND_ROBIN:
            return self._round_robin_selection(count)
        elif self.replication_strategy == ReplicationStrategy.RACK_AWARE:
            return self._rack_aware_selection(file_id, count)
        else:
            return []
    
    def _round_robin_selection(self, count: int) -> List[str]:
        """轮询选择"""
        active_nodes = [node_id for node_id, node in self.storage_nodes.items() if node.is_available()]
        selected = []
        
        for i in range(count):
            index = i % len(active_nodes) if active_nodes else 0
            if active_nodes:
                selected.append(active_nodes[index])
        
        return selected
    
    def _rack_aware_selection(self, file_id: str, count: int) -> List[str]:
        """机架感知选择"""
        # 按机架分组存储节点
        racks = {}
        for node_id, node in self.storage_nodes.items():
            if node.is_available():
                if node.rack not in racks:
                    racks[node.rack] = []
                racks[node.rack].append(node_id)
        
        selected = []
        racks_used = set()
        
        # 首先从不同机架选择节点
        for rack, nodes in racks.items():
            if len(selected) >= count:
                break
            
            if rack not in racks_used:
                racks_used.add(rack)
                for node_id in nodes:
                    if node_id not in selected:
                        selected.append(node_id)
                        if len(selected) >= count:
                            break
        
        # 如果还需要更多节点，从已有机架补充
        if len(selected) < count:
            all_nodes = [node_id for nodes in racks.values() for node_id in nodes]
            for node_id in all_nodes:
                if node_id not in selected:
                    selected.append(node_id)
                    if len(selected) >= count:
                        break
        
        return selected
    
    def _select_additional_nodes(self, file_id: str, count: int) -> List[str]:
        """选择额外的存储节点"""
        all_available_nodes = [node_id for node_id, node in self.storage_nodes.items() 
                             if node.is_available()]
        
        # 排除已有副本的节点
        used_nodes = []
        if file_id in self.file_metadata:
            used_nodes = self.file_metadata[file_id].storage_nodes
        
        additional_nodes = [node_id for node_id in all_available_nodes 
                          if node_id not in used_nodes]
        
        # 使用一致性哈希选择
        return self.hash_ring.get_nodes(f"{file_id}_additional", count)
    
    def _store_on_node(self, node_id: str, file_id: str, content: bytes) -> bool:
        """在指定节点存储文件"""
        # 简化实现，实际应该通过网络传输到节点
        node = self.storage_nodes[node_id]
        print(f"存储文件 {file_id} 到节点 {node_id} ({len(content)} bytes)")
        return True
    
    def _read_from_node(self, node_id: str, file_id: str) -> Optional[bytes]:
        """从指定节点读取文件"""
        # 简化实现，实际应该从网络节点读取
        print(f"从节点 {node_id} 读取文件 {file_id}")
        # 返回模拟内容
        return f"Content of {file_id}".encode()
    
    def _delete_from_node(self, node_id: str, file_id: str) -> bool:
        """从指定节点删除文件"""
        # 简化实现，实际应该通过网络删除
        print(f"从节点 {node_id} 删除文件 {file_id}")
        return True
    
    def _restore_from_replicas(self, file_id: str, metadata: FileMetadata) -> Optional[bytes]:
        """从副本恢复文件"""
        # 寻找有效副本并恢复
        for node_id in metadata.storage_nodes:
            if node_id in self.storage_nodes:
                content = self._read_from_node(node_id, file_id)
                if content:
                    # 更新校验和
                    metadata.checksum = hashlib.md5(content).hexdigest()
                    # 恢复损坏的副本
                    self._restore_corrupted_replicas(file_id, content, metadata)
                    return content
        return None
    
    def _restore_corrupted_replicas(self, file_id: str, content: bytes, metadata: FileMetadata):
        """恢复损坏的副本"""
        # 重新同步所有副本
        for node_id in metadata.storage_nodes:
            if node_id in self.storage_nodes:
                self._store_on_node(node_id, file_id, content)
                self.stats['replication_operations'] += 1
    
    def _handle_node_removal(self, node_id: str):
        """处理节点移除"""
        # 找到需要重新分配的文件
        affected_files = []
        for file_id, metadata in self.file_metadata.items():
            if node_id in metadata.storage_nodes:
                affected_files.append((file_id, metadata))
        
        # 重新分配这些文件
        for file_id, metadata in affected_files:
            # 从元数据中移除失效节点
            metadata.storage_nodes.remove(node_id)
            
            # 计算需要多少个新副本
            needed_replicas = max(0, metadata.replication_factor - len(metadata.storage_nodes))
            
            if needed_replicas > 0:
                # 选择新的存储节点
                new_nodes = self._select_storage_nodes(file_id, needed_replicas)
                
                # 从有效副本读取内容
                content = None
                for existing_node in metadata.storage_nodes:
                    content = self._read_from_node(existing_node, file_id)
                    if content:
                        break
                
                # 在新节点上存储副本
                if content:
                    for new_node_id in new_nodes:
                        if self._store_on_node(new_node_id, file_id, content):
                            metadata.storage_nodes.append(new_node_id)
                            self.stats['replication_operations'] += 1

class StorageLoadBalancer:
    """存储负载均衡器"""
    
    def select_optimal_node(self, available_nodes: List[str], file_size: int) -> str:
        """选择最优节点"""
        # 简单的负载均衡策略：选择剩余空间最大的节点
        return max(available_nodes, key=lambda node_id: 
                  self.storage_nodes[node_id].capacity - self.storage_nodes[node_id].used_space)
    
    def get_cluster_health(self) -> Dict:
        """获取集群健康状态"""
        total_capacity = sum(node.capacity for node in self.storage_nodes.values())
        total_used = sum(node.used_space for node in self.storage_nodes.values())
        active_nodes = sum(1 for node in self.storage_nodes.values() if node.status == "active")
        
        return {
            'total_nodes': len(self.storage_nodes),
            'active_nodes': active_nodes,
            'total_capacity_gb': total_capacity,
            'used_capacity_gb': total_used,
            'utilization_rate': total_used / total_capacity if total_capacity > 0 else 0,
            'cluster_health': 'healthy' if total_used / total_capacity < 0.8 else 'high_usage'
        }

# 使用示例
def demo_distributed_file_system():
    """分布式文件系统演示"""
    print("=== 分布式文件系统演示 ===")
    
    # 创建分布式文件系统
    dfs = DistributedFileSystem(replication_strategy=ReplicationStrategy.CONSISTENT_HASHING)
    
    # 添加存储节点
    nodes = [
        ("node1", "192.168.1.1", 8001, 100, "rack1"),
        ("node2", "192.168.1.2", 8002, 100, "rack1"),
        ("node3", "192.168.1.3", 8003, 100, "rack2"),
        ("node4", "192.168.1.4", 8004, 100, "rack2"),
    ]
    
    for node_id, ip, port, capacity, rack in nodes:
        dfs.add_storage_node(node_id, ip, port, capacity, rack)
    
    print(f"添加了 {len(nodes)} 个存储节点")
    
    # 上传文件
    files = [
        ("document1.pdf", b"This is document 1 content", "user1"),
        ("image1.jpg", b"JPEG image data for image1", "user2"),
        ("data1.csv", b"CSV data with some records", "user1"),
        ("video1.mp4", b"Video file data", "user3")
    ]
    
    file_ids = []
    for file_path, content, owner in files:
        file_id = dfs.upload_file(file_path, content, owner, replication_factor=3)
        file_ids.append(file_id)
        print(f"上传文件: {file_path} (ID: {file_id})")
    
    # 下载文件
    for i, file_id in enumerate(file_ids):
        content = dfs.download_file(file_id)
        if content:
            print(f"下载文件成功: {files[i][0]} ({len(content)} bytes)")
        else:
            print(f"下载文件失败: {files[i][0]}")
    
    # 模拟节点故障
    print("\n模拟节点故障...")
    dfs.remove_storage_node("node1")
    
    # 尝试下载文件（应该能够从副本恢复）
    content = dfs.download_file(file_ids[0])
    if content:
        print(f"节点故障后仍能下载文件: {files[0][0]}")
    
    # 获取集群统计
    print(f"\n文件系统统计: {dfs.stats}")
    
    # 获取存储节点状态
    for node_id, node in dfs.storage_nodes.items():
        print(f"节点 {node_id}: 使用 {node.used_space}GB / {node.capacity}GB (状态: {node.status})")

if __name__ == "__main__":
    demo_distributed_file_system()
```

## 总结

操作系统原理在现代分布式系统中的应用非常广泛：

### 核心概念映射

1. **进程管理**
   - 分布式调度器：在多节点间分配和调度任务
   - 负载均衡：动态调整工作负载分配
   - 容错机制：处理节点故障和任务迁移

2. **内存管理**
   - 虚拟内存：分布式缓存和存储系统
   - 垃圾回收：自动内存管理和资源回收
   - 分代管理：分层存储和生命周期管理

3. **文件系统**
   - 分布式存储：多节点数据冗余和一致性
   - 一致性哈希：高效的分布式数据定位
   - 副本管理：数据可靠性和可用性保证

### 实际应用场景

1. **云原生平台**
   - Kubernetes中的资源调度和管理
   - 容器运行时和镜像管理
   - 服务发现和负载均衡

2. **大数据处理**
   - Hadoop和Spark的资源管理
   - 数据分片和并行处理
   - 任务调度和监控

3. **微服务架构**
   - 服务编排和治理
   - 配置管理和发现
   - 监控和日志系统

4. **边缘计算**
   - 资源编排和调度
   - 数据同步和一致性
   - 分布式缓存和存储

理解和应用这些操作系统原理对于构建高效、可靠、可扩展的分布式系统至关重要。现代系统架构需要综合考虑资源管理、并发控制、容错处理等多个方面的挑战。