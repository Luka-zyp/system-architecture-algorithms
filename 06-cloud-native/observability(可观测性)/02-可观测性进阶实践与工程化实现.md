# å¯è§‚æµ‹æ€§è¿›é˜¶å®è·µä¸å·¥ç¨‹åŒ–å®ç°

## æ¦‚è¿°

åœ¨ç°ä»£äº‘åŸç”Ÿç¯å¢ƒä¸­ï¼Œå¯è§‚æµ‹æ€§ä¸ä»…ä»…æ˜¯ç›‘æ§å’Œæ—¥å¿—çš„ç®€å•æ”¶é›†ï¼Œè€Œæ˜¯æˆä¸ºäº†ç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œã€å¿«é€Ÿå®šä½é—®é¢˜ã€ä¼˜åŒ–æ€§èƒ½çš„å…³é”®èƒ½åŠ›ã€‚æœ¬æ–‡æ¡£æ·±å…¥æ¢è®¨å¯è§‚æµ‹æ€§çš„é«˜çº§å®è·µï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼è¿½è¸ªçš„æ·±åº¦ä¼˜åŒ–ã€æ™ºèƒ½åŒ–å‘Šè­¦ç³»ç»Ÿã€åŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹ã€ä»¥åŠå¯è§‚æµ‹æ€§æ•°æ®çš„æ²»ç†å’Œä»·å€¼æŒ–æ˜ã€‚

## é«˜çº§åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ

### ä¸šåŠ¡åœºæ™¯ï¼šå¤§å‹åˆ†å¸ƒå¼ç”µå•†ç³»ç»Ÿ

åœ¨åƒä¸‡çº§ç”¨æˆ·çš„ç”µå•†ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬æœ‰è¶…è¿‡500ä¸ªå¾®æœåŠ¡ï¼Œæ¯æ—¥å¤„ç†10äº¿æ¬¡è¯·æ±‚ã€‚éœ€è¦æ„å»ºä¸€ä¸ªèƒ½å¤Ÿè¿½è¸ªä»»æ„è¯·æ±‚çš„å®Œæ•´é“¾è·¯ï¼Œå¹¶åœ¨æ¯«ç§’çº§å®šä½æ€§èƒ½ç“¶é¢ˆçš„å¯è§‚æµ‹æ€§ç³»ç»Ÿã€‚

### åˆ†å¸ƒå¼è¿½è¸ªæ¶æ„è®¾è®¡

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
import asyncio
import json
import time
from datetime import datetime, timezone
from enum import Enum
import threading
from dataclasses import dataclass, asdict
from collections import defaultdict
import uuid

# è¿½è¸ªæ•°æ®æ¨¡å‹
@dataclass
class TraceContext:
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    baggage: Dict[str, Any]
    start_time: float
    end_time: Optional[float] = None
    duration_ms: Optional[float] = None

class SpanKind(Enum):
    SERVER = "server"
    CLIENT = "client"
    PRODUCER = "producer"
    CONSUMER = "internal"

class TraceStatus(Enum):
    OK = "ok"
    ERROR = "error"
    TIMEOUT = "timeout"

@dataclass
class Span:
    trace_context: TraceContext
    operation_name: str
    span_kind: SpanKind
    service_name: str
    tags: Dict[str, Any]
    logs: List[Dict[str, Any]]
    status: TraceStatus
    resource_attributes: Dict[str, Any]

# è¿½è¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨
class TraceContextManager:
    def __init__(self):
        self._local = threading.local()
    
    def get_current_context(self) -> Optional[TraceContext]:
        """è·å–å½“å‰è¿½è¸ªä¸Šä¸‹æ–‡"""
        return getattr(self._local, 'context', None)
    
    def set_current_context(self, context: TraceContext):
        """è®¾ç½®å½“å‰è¿½è¸ªä¸Šä¸‹æ–‡"""
        self._local.context = context
    
    def clear_current_context(self):
        """æ¸…é™¤å½“å‰è¿½è¸ªä¸Šä¸‹æ–‡"""
        if hasattr(self._local, 'context'):
            delattr(self._local, 'context')

# è¿½è¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨
trace_context_manager = TraceContextManager()

# è¿½è¸ªæ•°æ®å¤„ç†å™¨æ¥å£
class TraceDataProcessor(ABC):
    @abstractmethod
    async def process_span(self, span: Span) -> bool:
        """å¤„ç†å•ä¸ªSpan"""
        pass
    
    @abstractmethod
    async def process_trace(self, trace_id: str, spans: List[Span]) -> bool:
        """å¤„ç†å®Œæ•´è¿½è¸ª"""
        pass

# å†…å­˜è¿½è¸ªæ•°æ®å¤„ç†å™¨
class MemoryTraceDataProcessor(TraceDataProcessor):
    def __init__(self):
        self.traces: Dict[str, List[Span]] = defaultdict(list)
        self.spans_count = 0
    
    async def process_span(self, span: Span) -> bool:
        """å¤„ç†å•ä¸ªSpan"""
        self.traces[span.trace_context.trace_id].append(span)
        self.spans_count += 1
        return True
    
    async def process_trace(self, trace_id: str, spans: List[Span]) -> bool:
        """å¤„ç†å®Œæ•´è¿½è¸ª"""
        self.traces[trace_id] = spans
        return True
    
    def get_trace(self, trace_id: str) -> Optional[List[Span]]:
        """è·å–è¿½è¸ªæ•°æ®"""
        return self.traces.get(trace_id)
    
    def get_all_traces(self) -> Dict[str, List[Span]]:
        """è·å–æ‰€æœ‰è¿½è¸ªæ•°æ®"""
        return dict(self.traces)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ•°æ®"""
        return {
            'total_traces': len(self.traces),
            'total_spans': self.spans_count,
            'avg_spans_per_trace': self.spans_count / max(len(self.traces), 1)
        }

# Zipkinå¯¼å‡ºå™¨
class ZipkinTraceExporter(TraceDataProcessor):
    def __init__(self, zipkin_endpoint: str):
        self.zipkin_endpoint = zipkin_endpoint
        self.spans_buffer = []
        self.buffer_size = 100
        self.flush_interval = 5.0  # ç§’
    
    async def process_span(self, span: Span) -> bool:
        """å°†Spanå¯¼å‡ºåˆ°Zipkin"""
        zipkin_span = self._convert_to_zipkin_span(span)
        self.spans_buffer.append(zipkin_span)
        
        if len(self.spans_buffer) >= self.buffer_size:
            await self._flush_spans()
        
        return True
    
    async def _flush_spans(self):
        """æ‰¹é‡å¯¼å‡ºSpansåˆ°Zipkin"""
        if not self.spans_buffer:
            return
        
        # æ¨¡æ‹Ÿå‘é€åˆ°Zipkin
        print(f"å¯¼å‡º {len(self.spans_buffer)} ä¸ªSpansåˆ°Zipkin: {self.zipkin_endpoint}")
        
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€HTTPè¯·æ±‚åˆ°Zipkin
        # async with aiohttp.ClientSession() as session:
        #     await session.post(f"{self.zipkin_endpoint}/api/v2/spans", json=self.spans_buffer)
        
        self.spans_buffer.clear()
    
    def _convert_to_zipkin_span(self, span: Span) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºZipkinæ ¼å¼"""
        context = span.trace_context
        duration_microseconds = int((context.duration_ms or 0) * 1000)
        
        return {
            'traceId': context.trace_id,
            'spanId': context.span_id,
            'parentSpanId': context.parent_span_id,
            'name': span.operation_name,
            'kind': span.span_kind.value.upper(),
            'timestamp': int(context.start_time * 1000000),  # è½¬æ¢ä¸ºå¾®ç§’
            'duration': duration_microseconds,
            'localEndpoint': {
                'serviceName': span.service_name
            },
            'tags': span.tags,
            'annotations': self._convert_logs_to_annotations(span.logs)
        }
    
    def _convert_logs_to_annotations(self, logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†æ—¥å¿—è½¬æ¢ä¸ºæ³¨è§£"""
        annotations = []
        for log in logs:
            annotations.append({
                'timestamp': int(log['timestamp'] * 1000000),
                'value': log['message']
            })
        return annotations
    
    async def flush(self):
        """æ‰‹åŠ¨åˆ·æ–°ç¼“å†²åŒº"""
        await self._flush_spans()

# Jaegerå¯¼å‡ºå™¨
class JaegerTraceExporter(TraceDataProcessor):
    def __init__(self, jaeger_endpoint: str):
        self.jaeger_endpoint = jaeger_endpoint
        self.spans_buffer = []
        self.buffer_size = 50
        self.flush_interval = 3.0
    
    async def process_span(self, span: Span) -> bool:
        """å°†Spanå¯¼å‡ºåˆ°Jaeger"""
        jaeger_span = self._convert_to_jaeger_span(span)
        self.spans_buffer.append(jaeger_span)
        
        if len(self.spans_buffer) >= self.buffer_size:
            await self._flush_spans()
        
        return True
    
    async def _flush_spans(self):
        """æ‰¹é‡å¯¼å‡ºSpansåˆ°Jaeger"""
        if not self.spans_buffer:
            return
        
        # æ¨¡æ‹Ÿå‘é€åˆ°Jaeger
        print(f"å¯¼å‡º {len(self.spans_buffer)} ä¸ªSpansåˆ°Jaeger: {self.jaeger_endpoint}")
        self.spans_buffer.clear()
    
    def _convert_to_jaeger_span(self, span: Span) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºJaegeræ ¼å¼"""
        context = span.trace_context
        
        return {
            'traceId': context.trace_id,
            'spanId': context.span_id,
            'parentSpanId': context.parent_span_id,
            'operationName': span.operation_name,
            'references': [],
            'flags': 1,  # é‡‡æ ·æ ‡å¿—
            'startTime': int(context.start_time * 1000000),
            'duration': int((context.duration_ms or 0) * 1000000),
            'tags': [{'key': k, 'type': 'STRING', 'value': v} for k, v in span.tags.items()],
            'logs': [{'timestamp': int(log['timestamp'] * 1000000), 'fields': [{'key': 'event', 'type': 'STRING', 'value': log['message']}]} for log in span.logs]
        }

# é«˜çº§è¿½è¸ªå™¨
class AdvancedTracer:
    def __init__(self, service_name: str, processors: List[TraceDataProcessor]):
        self.service_name = service_name
        self.processors = processors
        self.active_spans = {}
        self.span_counter = 0
    
    def generate_trace_id(self) -> str:
        """ç”Ÿæˆè¿½è¸ªID"""
        return str(uuid.uuid4()).replace('-', '')
    
    def generate_span_id(self) -> str:
        """ç”ŸæˆSpan ID"""
        self.span_counter += 1
        return f"span_{self.span_counter}"
    
    def start_span(self, 
                   operation_name: str, 
                   span_kind: SpanKind = SpanKind.INTERNAL,
                   baggage: Dict[str, Any] = None,
                   parent_span_id: str = None) -> Span:
        """å¼€å§‹è¿½è¸ªSpan"""
        # è·å–æˆ–åˆ›å»ºè¿½è¸ªä¸Šä¸‹æ–‡
        current_context = trace_context_manager.get_current_context()
        if not current_context:
            trace_id = self.generate_trace_id()
            context = TraceContext(
                trace_id=trace_id,
                span_id=self.generate_span_id(),
                parent_span_id=None,
                baggage=baggage or {},
                start_time=time.time()
            )
        else:
            context = TraceContext(
                trace_id=current_context.trace_id,
                span_id=self.generate_span_id(),
                parent_span_id=current_context.span_id,
                baggage=baggage or {},
                start_time=time.time()
            )
        
        # åˆ›å»ºSpan
        span = Span(
            trace_context=context,
            operation_name=operation_name,
            span_kind=span_kind,
            service_name=self.service_name,
            tags={},
            logs=[],
            status=TraceStatus.OK,
            resource_attributes={
                'service.name': self.service_name,
                'service.version': '1.0.0'
            }
        )
        
        # è®¾ç½®å½“å‰ä¸Šä¸‹æ–‡
        trace_context_manager.set_current_context(context)
        self.active_spans[context.span_id] = span
        
        return span
    
    def finish_span(self, span: Span, status: TraceStatus = TraceStatus.OK):
        """å®ŒæˆSpan"""
        span.trace_context.end_time = time.time()
        span.trace_context.duration_ms = (span.trace_context.end_time - span.trace_context.start_time) * 1000
        span.status = status
        
        # ç§»é™¤æ´»è·ƒSpan
        if span.trace_context.span_id in self.active_spans:
            del self.active_spans[span.trace_context.span_id]
        
        # å‘é€åˆ°å¤„ç†å™¨
        asyncio.create_task(self._process_span(span))
        
        # æ¢å¤çˆ¶çº§ä¸Šä¸‹æ–‡
        if span.trace_context.parent_span_id:
            # æ¢å¤çˆ¶çº§Spançš„ä¸Šä¸‹æ–‡
            parent_span = self.active_spans.get(span.trace_context.parent_span_id)
            if parent_span:
                parent_context = TraceContext(
                    trace_id=parent_span.trace_context.trace_id,
                    span_id=parent_span.trace_context.span_id,
                    parent_span_id=parent_span.trace_context.parent_span_id,
                    baggage=parent_span.trace_context.baggage,
                    start_time=parent_span.trace_context.start_time,
                    end_time=parent_span.trace_context.end_time,
                    duration_ms=parent_span.trace_context.duration_ms
                )
                trace_context_manager.set_current_context(parent_context)
        else:
            trace_context_manager.clear_current_context()
    
    async def _process_span(self, span: Span):
        """å¤„ç†Span"""
        for processor in self.processors:
            try:
                await processor.process_span(span)
            except Exception as e:
                print(f"å¤„ç†å™¨å¤„ç†Spanå¤±è´¥: {e}")
    
    def add_tag(self, span: Span, key: str, value: Any):
        """æ·»åŠ æ ‡ç­¾"""
        span.tags[key] = str(value)
    
    def add_log(self, span: Span, message: str, fields: Dict[str, Any] = None):
        """æ·»åŠ æ—¥å¿—"""
        log_entry = {
            'timestamp': time.time(),
            'message': message
        }
        if fields:
            log_entry.update(fields)
        span.logs.append(log_entry)
    
    def set_baggage_item(self, span: Span, key: str, value: Any):
        """è®¾ç½®ä¸Šä¸‹æ–‡é¡¹ç›®"""
        span.trace_context.baggage[key] = value
    
    def get_baggage_item(self, span: Span, key: str) -> Any:
        """è·å–ä¸Šä¸‹æ–‡é¡¹ç›®"""
        return span.trace_context.baggage.get(key)

# è¿½è¸ªè£…é¥°å™¨
def trace(operation_name: str, span_kind: SpanKind = SpanKind.INTERNAL):
    """è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        async def async_wrapper(*args, **kwargs):
            tracer = kwargs.get('tracer')
            if not tracer:
                return await func(*args, **kwargs)
            
            span = tracer.start_span(operation_name, span_kind)
            try:
                # ä¼ é€’tracerç»™å‡½æ•°
                if 'tracer' not in kwargs:
                    kwargs['tracer'] = tracer
                
                result = await func(*args, **kwargs)
                tracer.finish_span(span, TraceStatus.OK)
                return result
                
            except Exception as e:
                tracer.add_log(span, "Exception occurred", {'error': str(e)})
                tracer.finish_span(span, TraceStatus.ERROR)
                raise
        
        def sync_wrapper(*args, **kwargs):
            tracer = kwargs.get('tracer')
            if not tracer:
                return func(*args, **kwargs)
            
            span = tracer.start_span(operation_name, span_kind)
            try:
                # ä¼ é€’tracerç»™å‡½æ•°
                if 'tracer' not in kwargs:
                    kwargs['tracer'] = tracer
                
                result = func(*args, **kwargs)
                tracer.finish_span(span, TraceStatus.OK)
                return result
                
            except Exception as e:
                tracer.add_log(span, "Exception occurred", {'error': str(e)})
                tracer.finish_span(span, TraceStatus.ERROR)
                raise
        
        # å¦‚æœæ˜¯å¼‚æ­¥å‡½æ•°ï¼Œä½¿ç”¨asyncåŒ…è£…å™¨
        import asyncio
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

# åˆ†å¸ƒå¼è¿½è¸ªå®¢æˆ·ç«¯
class TracingClient:
    def __init__(self, tracer: AdvancedTracer):
        self.tracer = tracer
    
    async def make_traced_request(self, service_name: str, operation: str, request_data: Dict) -> Dict:
        """å‘èµ·å¸¦è¿½è¸ªçš„è¯·æ±‚"""
        # å¼€å§‹å®¢æˆ·ç«¯Span
        span = self.tracer.start_span(operation, SpanKind.CLIENT)
        self.tracer.add_tag(span, 'peer.service', service_name)
        self.tracer.add_tag(span, 'http.method', 'POST')
        
        try:
            # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
            self.tracer.add_log(span, f"Making request to {service_name}")
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            
            # æ¨¡æ‹Ÿå“åº”
            response = {
                'status': 'success',
                'data': request_data,
                'timestamp': time.time()
            }
            
            self.tracer.add_tag(span, 'http.status_code', 200)
            self.tracer.add_log(span, "Request completed successfully")
            
            return response
            
        except Exception as e:
            self.tracer.add_log(span, f"Request failed: {str(e)}")
            self.tracer.add_tag(span, 'error', True)
            self.tracer.add_tag(span, 'error.type', type(e).__name__)
            raise
            
        finally:
            self.tracer.finish_span(span)

# ä½¿ç”¨ç¤ºä¾‹
async def demonstrate_advanced_tracing():
    """æ¼”ç¤ºé«˜çº§åˆ†å¸ƒå¼è¿½è¸ª"""
    
    # åˆ›å»ºå¤„ç†å™¨
    memory_processor = MemoryTraceDataProcessor()
    zipkin_exporter = ZipkinTraceExporter("http://localhost:9411")
    jaeger_exporter = JaegerTraceExporter("http://localhost:14268")
    
    # åˆ›å»ºè¿½è¸ªå™¨
    tracer = AdvancedTracer("ecommerce-service", [memory_processor, zipkin_exporter, jaeger_exporter])
    tracing_client = TracingClient(tracer)
    
    # å¼€å§‹è¿½è¸ª
    print("=== ç”µå•†è®¢å•å¤„ç†è¿½è¸ªæ¼”ç¤º ===")
    
    async def process_order(order_id: str, user_id: str, product_ids: List[str]):
        """å¤„ç†è®¢å•"""
        order_span = tracer.start_span("process_order", SpanKind.SERVER)
        tracer.add_tag(order_span, "order.id", order_id)
        tracer.add_tag(order_span, "order.user_id", user_id)
        
        try:
            # éªŒè¯åº“å­˜
            inventory_span = tracer.start_span("validate_inventory", SpanKind.INTERNAL)
            await asyncio.sleep(0.05)
            tracer.add_log(inventory_span, "Inventory validated")
            tracer.finish_span(inventory_span)
            
            # å¤„ç†æ”¯ä»˜
            payment_span = tracer.start_span("process_payment", SpanKind.INTERNAL)
            tracer.add_tag(payment_span, "payment.method", "credit_card")
            tracer.add_tag(payment_span, "payment.amount", "199.99")
            
            await asyncio.sleep(0.1)
            
            # è°ƒç”¨å¤–éƒ¨æ”¯ä»˜æœåŠ¡
            payment_response = await tracing_client.make_traced_request(
                "payment-service", 
                "charge_credit_card", 
                {"amount": 199.99, "currency": "USD"}
            )
            
            tracer.add_log(payment_span, "Payment processed successfully")
            tracer.finish_span(payment_span)
            
            # å‘é€ç¡®è®¤é€šçŸ¥
            notification_span = tracer.start_span("send_notification", SpanKind.INTERNAL)
            await asyncio.sleep(0.03)
            tracer.add_log(notification_span, "Notification sent to user")
            tracer.finish_span(notification_span)
            
            tracer.add_log(order_span, "Order processed successfully")
            tracer.finish_span(order_span, TraceStatus.OK)
            
            return {"order_id": order_id, "status": "processed"}
            
        except Exception as e:
            tracer.add_log(order_span, f"Order processing failed: {str(e)}")
            tracer.finish_span(order_span, TraceStatus.ERROR)
            raise
    
    # æ‰§è¡Œè¿½è¸ªçš„è®¢å•å¤„ç†
    result = await process_order("order_12345", "user_67890", ["product_1", "product_2"])
    print(f"è®¢å•å¤„ç†ç»“æœ: {result}")
    
    # æ˜¾ç¤ºè¿½è¸ªç»Ÿè®¡
    stats = memory_processor.get_stats()
    print(f"è¿½è¸ªç»Ÿè®¡: {stats}")
    
    # æ˜¾ç¤ºè¿½è¸ªè¯¦æƒ…
    traces = memory_processor.get_all_traces()
    for trace_id, spans in traces.items():
        print(f"\nè¿½è¸ª {trace_id}:")
        for span in spans:
            context = span.trace_context
            print(f"  - {span.operation_name}: {context.duration_ms:.2f}ms ({span.span_kind.value})")
```

## æ™ºèƒ½åŒ–å‘Šè­¦ç³»ç»Ÿ

### ä¸šåŠ¡åœºæ™¯ï¼šåŠ¨æ€é˜ˆå€¼ä¸å¼‚å¸¸æ£€æµ‹

ä¼ ç»Ÿçš„é™æ€é˜ˆå€¼å‘Šè­¦æ— æ³•é€‚åº”ä¸šåŠ¡çš„åŠ¨æ€å˜åŒ–ã€‚éœ€è¦æ„å»ºä¸€ä¸ªåŸºäºå†å²æ•°æ®å’Œæœºå™¨å­¦ä¹ çš„æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š

1. åŠ¨æ€å­¦ä¹ æ­£å¸¸åŸºçº¿
2. æ£€æµ‹å¼‚å¸¸æ¨¡å¼
3. å‡å°‘è¯¯æŠ¥å’Œæ¼æŠ¥
4. æä¾›æ ¹å› åˆ†æå»ºè®®

### æ™ºèƒ½å‘Šè­¦æ¶æ„

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Tuple
import numpy as np
import asyncio
import time
from datetime import datetime, timedelta
from collections import deque
import math

# å‘Šè­¦çº§åˆ«æšä¸¾
class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"

# å‘Šè­¦ç±»å‹æšä¸¾
class AlertType(Enum):
    THRESHOLD = "threshold"
    ANOMALY = "anomaly"
    TREND = "trend"
    PATTERN = "pattern"

@dataclass
class Alert:
    alert_id: str
    alert_type: AlertType
    severity: AlertSeverity
    title: str
    description: str
    metric_name: str
    metric_value: float
    threshold_value: Optional[float]
    timestamp: float
    labels: Dict[str, str]
    recommended_actions: List[str]

# æŒ‡æ ‡æ•°æ®æ¨¡å‹
@dataclass
class MetricData:
    name: str
    value: float
    timestamp: float
    labels: Dict[str, str]
    
    def __post_init__(self):
        if isinstance(self.timestamp, datetime):
            self.timestamp = self.timestamp.timestamp()

# å¼‚å¸¸æ£€æµ‹å™¨æ¥å£
class AnomalyDetector(ABC):
    @abstractmethod
    async def train(self, historical_data: List[MetricData]):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        pass
    
    @abstractmethod
    async def detect(self, current_data: MetricData) -> Tuple[bool, float, str]:
        """æ£€æµ‹å¼‚å¸¸ï¼Œè¿”å›(æ˜¯å¦å¼‚å¸¸, å¼‚å¸¸åˆ†æ•°, åŸå› )"""
        pass

# åŸºäºç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹å™¨
class StatisticalAnomalyDetector(AnomalyDetector):
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.historical_values = deque(maxlen=window_size)
        self.mean = 0.0
        self.std = 0.0
        self.is_trained = False
    
    async def train(self, historical_data: List[MetricData]):
        """è®­ç»ƒç»Ÿè®¡æ¨¡å‹"""
        self.historical_values.extend([data.value for data in historical_data])
        
        if len(self.historical_values) > 0:
            values = list(self.historical_values)
            self.mean = np.mean(values)
            self.std = np.std(values)
            self.is_trained = True
            
            print(f"ç»Ÿè®¡æ¨¡å‹è®­ç»ƒå®Œæˆ: å‡å€¼={self.mean:.4f}, æ ‡å‡†å·®={self.std:.4f}")
    
    async def detect(self, current_data: MetricData) -> Tuple[bool, float, str]:
        """åŸºäº3-sigmaè§„åˆ™æ£€æµ‹å¼‚å¸¸"""
        if not self.is_trained or self.std == 0:
            return False, 0.0, "æ¨¡å‹æœªè®­ç»ƒæˆ–æ–¹å·®ä¸º0"
        
        z_score = abs((current_data.value - self.mean) / self.std)
        is_anomaly = z_score > 3.0  # 3-sigmaè§„åˆ™
        
        reason = f"Z-score={z_score:.2f}, é˜ˆå€¼=3.0"
        return is_anomaly, z_score, reason

# åŸºäºæ—¶é—´åºåˆ—çš„å¼‚å¸¸æ£€æµ‹å™¨
class TimeSeriesAnomalyDetector(AnomalyDetector):
    def __init__(self, window_size: int = 24):  # 24å°æ—¶çš„æ»‘åŠ¨çª—å£
        self.window_size = window_size
        self.time_series_data = deque(maxlen=window_size)
        self.seasonal_patterns = {}
        self.trend_model = None
        self.is_trained = False
    
    async def train(self, historical_data: List[MetricData]):
        """è®­ç»ƒæ—¶é—´åºåˆ—æ¨¡å‹"""
        # æŒ‰æ—¶é—´æ’åº
        sorted_data = sorted(historical_data, key=lambda x: x.timestamp)
        self.time_series_data.extend(sorted_data)
        
        # æå–æ—¶é—´ç‰¹å¾
        values = [data.value for data in self.time_series_data]
        timestamps = [data.timestamp for data in self.time_series_data]
        
        # ç®€å•çš„è¶‹åŠ¿åˆ†æ
        if len(values) > 1:
            self.trend_model = self._calculate_trend(timestamps, values)
        
        # å­£èŠ‚æ€§æ¨¡å¼åˆ†æï¼ˆç®€åŒ–ï¼‰
        self.seasonal_patterns = self._analyze_seasonality(sorted_data)
        
        self.is_trained = True
        print("æ—¶é—´åºåˆ—æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def _calculate_trend(self, timestamps: List[float], values: List[float]) -> Dict[str, float]:
        """è®¡ç®—è¶‹åŠ¿"""
        n = len(values)
        if n < 2:
            return {"slope": 0.0, "intercept": values[0] if values else 0.0}
        
        # çº¿æ€§å›å½’
        x_mean = np.mean(timestamps)
        y_mean = np.mean(values)
        
        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(timestamps, values))
        denominator = sum((x - x_mean) ** 2 for x in timestamps)
        
        if denominator == 0:
            slope = 0.0
        else:
            slope = numerator / denominator
        
        intercept = y_mean - slope * x_mean
        
        return {"slope": slope, "intercept": intercept}
    
    def _analyze_seasonality(self, data: List[MetricData]) -> Dict[str, float]:
        """åˆ†æå­£èŠ‚æ€§æ¨¡å¼"""
        patterns = {}
        
        # æŒ‰å°æ—¶åˆ†ç»„åˆ†æ
        hourly_values = {}
        for item in data:
            dt = datetime.fromtimestamp(item.timestamp)
            hour = dt.hour
            if hour not in hourly_values:
                hourly_values[hour] = []
            hourly_values[hour].append(item.value)
        
        # è®¡ç®—æ¯å°æ—¶å¹³å‡å€¼
        for hour, values in hourly_values.items():
            patterns[f"hour_{hour}"] = np.mean(values)
        
        return patterns
    
    async def detect(self, current_data: MetricData) -> Tuple[bool, float, str]:
        """åŸºäºæ—¶é—´åºåˆ—æ¨¡å‹æ£€æµ‹å¼‚å¸¸"""
        if not self.is_trained:
            return False, 0.0, "æ¨¡å‹æœªè®­ç»ƒ"
        
        expected_value = self._predict_value(current_data.timestamp)
        actual_value = current_data.value
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        if expected_value != 0:
            relative_error = abs(actual_value - expected_value) / abs(expected_value)
        else:
            relative_error = abs(actual_value) if actual_value != 0 else 0
        
        # æ£€æµ‹å¼‚å¸¸ï¼ˆç›¸å¯¹è¯¯å·®è¶…è¿‡50%ï¼‰
        is_anomaly = relative_error > 0.5
        reason = f"æœŸæœ›å€¼={expected_value:.2f}, å®é™…å€¼={actual_value:.2f}, ç›¸å¯¹è¯¯å·®={relative_error:.2%}"
        
        return is_anomaly, relative_error, reason
    
    def _predict_value(self, timestamp: float) -> float:
        """é¢„æµ‹ç‰¹å®šæ—¶é—´çš„å€¼"""
        predicted_value = self.trend_model["intercept"] + self.trend_model["slope"] * timestamp
        
        # åŠ ä¸Šå­£èŠ‚æ€§è°ƒæ•´
        dt = datetime.fromtimestamp(timestamp)
        hour = dt.hour
        seasonal_adjustment = self.seasonal_patterns.get(f"hour_{hour}", 0)
        
        if seasonal_adjustment != 0:
            # ç®€å•çš„æ··åˆé¢„æµ‹
            predicted_value = 0.7 * predicted_value + 0.3 * seasonal_adjustment
        
        return predicted_value

# åŠ¨æ€é˜ˆå€¼ç®¡ç†å™¨
class DynamicThresholdManager:
    def __init__(self, anomaly_detector: AnomalyDetector):
        self.anomaly_detector = anomaly_detector
        self.threshold_cache = {}
        self.adaptation_rate = 0.1  # é˜ˆå€¼é€‚åº”é€Ÿç‡
    
    async def update_thresholds(self, metric_name: str, historical_data: List[MetricData]):
        """æ›´æ–°æŒ‡æ ‡çš„åŠ¨æ€é˜ˆå€¼"""
        # ä½¿ç”¨å¼‚å¸¸æ£€æµ‹å™¨åˆ†æå†å²æ•°æ®
        await self.anomaly_detector.train(historical_data)
        
        # è®¡ç®—åŠ¨æ€é˜ˆå€¼
        if hasattr(self.anomaly_detector, 'mean') and hasattr(self.anomaly_detector, 'std'):
            mean = self.anomaly_detector.mean
            std = self.anomaly_detector.std
            
            # è®¡ç®—ä¸åŒçº§åˆ«çš„é˜ˆå€¼
            thresholds = {
                'info': mean + 1.5 * std,
                'warning': mean + 2.0 * std,
                'critical': mean + 2.5 * std,
                'emergency': mean + 3.0 * std
            }
            
            self.threshold_cache[metric_name] = thresholds
            print(f"æ›´æ–°æŒ‡æ ‡ {metric_name} çš„åŠ¨æ€é˜ˆå€¼: {thresholds}")
    
    def get_threshold(self, metric_name: str, severity: AlertSeverity) -> Optional[float]:
        """è·å–ç‰¹å®šçº§åˆ«çš„é˜ˆå€¼"""
        thresholds = self.threshold_cache.get(metric_name)
        return thresholds.get(severity.value) if thresholds else None
    
    def check_threshold(self, metric_name: str, value: float) -> Optional[AlertSeverity]:
        """æ£€æŸ¥å€¼æ˜¯å¦è¶…è¿‡é˜ˆå€¼"""
        thresholds = self.threshold_cache.get(metric_name)
        if not thresholds:
            return None
        
        # æ£€æŸ¥å„ä¸ªçº§åˆ«çš„é˜ˆå€¼
        if value >= thresholds['emergency']:
            return AlertSeverity.EMERGENCY
        elif value >= thresholds['critical']:
            return AlertSeverity.CRITICAL
        elif value >= thresholds['warning']:
            return AlertSeverity.WARNING
        elif value >= thresholds['info']:
            return AlertSeverity.INFO
        
        return None

# å‘Šè­¦è§„åˆ™ç®¡ç†å™¨
class AlertRuleManager:
    def __init__(self):
        self.rules = {}
        self.rule_evaluations = defaultdict(int)  # è§„åˆ™è¯„ä¼°æ¬¡æ•°
        self.alert_history = deque(maxlen=1000)   # å‘Šè­¦å†å²
    
    def add_rule(self, rule_id: str, rule_config: Dict[str, Any]):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self.rules[rule_id] = rule_config
        print(f"æ·»åŠ å‘Šè­¦è§„åˆ™: {rule_id}")
    
    def remove_rule(self, rule_id: str):
        """ç§»é™¤å‘Šè­¦è§„åˆ™"""
        if rule_id in self.rules:
            del self.rules[rule_id]
            print(f"ç§»é™¤å‘Šè­¦è§„åˆ™: {rule_id}")
    
    async def evaluate_rules(self, metric_data: MetricData) -> List[Alert]:
        """è¯„ä¼°å‘Šè­¦è§„åˆ™"""
        alerts = []
        
        for rule_id, rule_config in self.rules.items():
            if self._metric_matches_rule(metric_data, rule_config):
                alert = await self._create_alert(rule_id, metric_data, rule_config)
                if alert:
                    alerts.append(alert)
                    self.alert_history.append(alert)
            
            self.rule_evaluations[rule_id] += 1
        
        return alerts
    
    def _metric_matches_rule(self, metric_data: MetricData, rule_config: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦åŒ¹é…è§„åˆ™"""
        # æ£€æŸ¥æŒ‡æ ‡åç§°
        if 'metric_name' in rule_config:
            if metric_data.name != rule_config['metric_name']:
                return False
        
        # æ£€æŸ¥æ ‡ç­¾åŒ¹é…
        if 'labels' in rule_config:
            for key, expected_value in rule_config['labels'].items():
                if key not in metric_data.labels or metric_data.labels[key] != expected_value:
                    return False
        
        return True
    
    async def _create_alert(self, rule_id: str, metric_data: MetricData, rule_config: Dict[str, Any]) -> Optional[Alert]:
        """åˆ›å»ºå‘Šè­¦"""
        # è¿™é‡Œå¯ä»¥é›†æˆé˜ˆå€¼æ£€æŸ¥ã€å¼‚å¸¸æ£€æµ‹ç­‰
        # ç®€åŒ–å®ç°ï¼šæ ¹æ®é…ç½®åˆ›å»ºå‘Šè­¦
        
        if 'threshold' in rule_config:
            threshold_value = rule_config['threshold']
            if metric_data.value >= threshold_value:
                severity = rule_config.get('severity', AlertSeverity.WARNING)
                
                alert = Alert(
                    alert_id=f"alert_{rule_id}_{int(time.time())}",
                    alert_type=AlertType.THRESHOLD,
                    severity=severity,
                    title=f"æŒ‡æ ‡ {metric_data.name} è¶…è¿‡é˜ˆå€¼",
                    description=f"æŒ‡æ ‡ {metric_data.name} å½“å‰å€¼ {metric_data.value} è¶…è¿‡é˜ˆå€¼ {threshold_value}",
                    metric_name=metric_data.name,
                    metric_value=metric_data.value,
                    threshold_value=threshold_value,
                    timestamp=time.time(),
                    labels=metric_data.labels.copy(),
                    recommended_actions=rule_config.get('actions', ["æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"])
                )
                
                return alert
        
        return None

# å‘Šè­¦é™å™ªç®¡ç†å™¨
class AlertDeduplicationManager:
    def __init__(self, dedup_window: int = 300):  # 5åˆ†é’Ÿçª—å£
        self.dedup_window = dedup_window
        self.alert_hashes = {}  # å‘Šè­¦å“ˆå¸Œ -> æ—¶é—´æˆ³
        self.similar_alerts = defaultdict(list)  # ç›¸ä¼¼å‘Šè­¦ç»„
    
    def should_suppress_alert(self, alert: Alert) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æŠ‘åˆ¶å‘Šè­¦"""
        alert_hash = self._calculate_alert_hash(alert)
        current_time = time.time()
        
        # æ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰ç›¸ä¼¼å‘Šè­¦
        for stored_hash, timestamp in self.alert_hashes.items():
            if current_time - timestamp <= self.dedup_window:
                if self._alerts_similar(alert_hash, stored_hash):
                    self.similar_alerts[stored_hash].append(alert.alert_id)
                    print(f"æŠ‘åˆ¶ç›¸ä¼¼å‘Šè­¦: {alert.alert_id}")
                    return True
        
        # è®°å½•æ–°çš„å‘Šè­¦å“ˆå¸Œ
        self.alert_hashes[alert_hash] = current_time
        return False
    
    def _calculate_alert_hash(self, alert: Alert) -> str:
        """è®¡ç®—å‘Šè­¦å“ˆå¸Œ"""
        # åŸºäºæŒ‡æ ‡åç§°ã€æ ‡ç­¾ç­‰è®¡ç®—å“ˆå¸Œ
        key_parts = [alert.metric_name]
        key_parts.extend(sorted(alert.labels.items()))
        key_string = "|".join(key_parts)
        return str(hash(key_string))
    
    def _alerts_similar(self, hash1: str, hash2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå‘Šè­¦æ˜¯å¦ç›¸ä¼¼"""
        return hash1 == hash2

# æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ
class IntelligentAlertSystem:
    def __init__(self, 
                 threshold_manager: DynamicThresholdManager,
                 rule_manager: AlertRuleManager,
                 dedup_manager: AlertDeduplicationManager):
        self.threshold_manager = threshold_manager
        self.rule_manager = rule_manager
        self.dedup_manager = dedup_manager
        self.alert_callbacks = []
        self.alert_stats = {
            'total_alerts': 0,
            'suppressed_alerts': 0,
            'sent_alerts': 0
        }
    
    def add_alert_callback(self, callback):
        """æ·»åŠ å‘Šè­¦å›è°ƒå‡½æ•°"""
        self.alert_callbacks.append(callback)
    
    async def process_metric(self, metric_data: MetricData):
        """å¤„ç†æŒ‡æ ‡æ•°æ®"""
        # 1. è¯„ä¼°å‘Šè­¦è§„åˆ™
        rule_alerts = await self.rule_manager.evaluate_rules(metric_data)
        
        # 2. æ£€æŸ¥åŠ¨æ€é˜ˆå€¼
        threshold_alerts = await self._check_dynamic_thresholds(metric_data)
        
        # 3. åˆå¹¶æ‰€æœ‰å‘Šè­¦
        all_alerts = rule_alerts + threshold_alerts
        
        # 4. é™å™ªå¤„ç†
        for alert in all_alerts:
            self.alert_stats['total_alerts'] += 1
            
            if self.dedup_manager.should_suppress_alert(alert):
                self.alert_stats['suppressed_alerts'] += 1
                continue
            
            # å‘é€å‘Šè­¦
            await self._send_alert(alert)
            self.alert_stats['sent_alerts'] += 1
    
    async def _check_dynamic_thresholds(self, metric_data: MetricData) -> List[Alert]:
        """æ£€æŸ¥åŠ¨æ€é˜ˆå€¼"""
        alerts = []
        
        severity = self.threshold_manager.check_threshold(metric_data.name, metric_data.value)
        if severity:
            threshold_value = self.threshold_manager.get_threshold(metric_data.name, severity)
            
            alert = Alert(
                alert_id=f"dynamic_threshold_{int(time.time())}",
                alert_type=AlertType.THRESHOLD,
                severity=severity,
                title=f"æŒ‡æ ‡ {metric_data.name} è¶…è¿‡åŠ¨æ€é˜ˆå€¼",
                description=f"æŒ‡æ ‡ {metric_data.name} å½“å‰å€¼ {metric_data.value} è¶…è¿‡åŠ¨æ€é˜ˆå€¼ {threshold_value}",
                metric_name=metric_data.name,
                metric_value=metric_data.value,
                threshold_value=threshold_value,
                timestamp=time.time(),
                labels=metric_data.labels.copy(),
                recommended_actions=["æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½", "æŸ¥çœ‹ç›¸å…³æŒ‡æ ‡", "è€ƒè™‘æ‰©å®¹"]
            )
            
            alerts.append(alert)
        
        return alerts
    
    async def _send_alert(self, alert: Alert):
        """å‘é€å‘Šè­¦"""
        print(f"å‘é€å‘Šè­¦: {alert.title} - {alert.description}")
        
        # è°ƒç”¨æ‰€æœ‰æ³¨å†Œçš„å›è°ƒå‡½æ•°
        for callback in self.alert_callbacks:
            try:
                await callback(alert)
            except Exception as e:
                print(f"å‘Šè­¦å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘Šè­¦ç³»ç»Ÿç»Ÿè®¡"""
        total = self.alert_stats['total_alerts']
        suppression_rate = (self.alert_stats['suppressed_alerts'] / max(total, 1)) * 100
        
        return {
            'total_alerts': total,
            'suppressed_alerts': self.alert_stats['suppressed_alerts'],
            'sent_alerts': self.alert_stats['sent_alerts'],
            'suppression_rate': f"{suppression_rate:.1f}%"
        }

# ä½¿ç”¨ç¤ºä¾‹
async def demonstrate_intelligent_alerting():
    """æ¼”ç¤ºæ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ"""
    
    # åˆ›å»ºç»„ä»¶
    anomaly_detector = StatisticalAnomalyDetector()
    threshold_manager = DynamicThresholdManager(anomaly_detector)
    rule_manager = AlertRuleManager()
    dedup_manager = AlertDeduplicationManager()
    
    # åˆ›å»ºæ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ
    alert_system = IntelligentAlertSystem(threshold_manager, rule_manager, dedup_manager)
    
    # æ·»åŠ å‘Šè­¦å›è°ƒ
    async def send_email_alert(alert: Alert):
        print(f"ğŸ“§ å‘é€é‚®ä»¶å‘Šè­¦: {alert.title}")
    
    async def send_sms_alert(alert: Alert):
        print(f"ğŸ“± å‘é€çŸ­ä¿¡å‘Šè­¦: {alert.title}")
    
    alert_system.add_alert_callback(send_email_alert)
    alert_system.add_alert_callback(send_sms_alert)
    
    # ç”Ÿæˆå†å²æ•°æ®ç”¨äºè®­ç»ƒ
    historical_data = []
    base_time = time.time() - 24 * 3600  # 24å°æ—¶å‰
    
    for i in range(100):
        timestamp = base_time + i * 900  # æ¯15åˆ†é’Ÿä¸€ä¸ªæ•°æ®ç‚¹
        # æ¨¡æ‹ŸCPUä½¿ç”¨ç‡ï¼Œæ­£å¸¸èŒƒå›´åœ¨50-80%
        value = 65 + 10 * math.sin(i * 0.1) + np.random.normal(0, 5)
        historical_data.append(MetricData(
            name="cpu_usage_percent",
            value=max(0, min(100, value)),
            timestamp=timestamp,
            labels={"host": "web-server-1"}
        ))
    
    # è®­ç»ƒåŠ¨æ€é˜ˆå€¼
    await threshold_manager.update_thresholds("cpu_usage_percent", historical_data)
    
    # æ·»åŠ é™æ€å‘Šè­¦è§„åˆ™
    rule_manager.add_rule("high_memory", {
        'metric_name': 'memory_usage_percent',
        'threshold': 90.0,
        'severity': AlertSeverity.CRITICAL,
        'actions': ['æ£€æŸ¥å†…å­˜æ³„æ¼', 'é‡å¯ç›¸å…³æœåŠ¡']
    })
    
    # å¤„ç†å®æ—¶æŒ‡æ ‡æ•°æ®
    print("\n=== æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿæ¼”ç¤º ===")
    
    # æ­£å¸¸çš„CPUä½¿ç”¨ç‡
    normal_metric = MetricData(
        name="cpu_usage_percent",
        value=70.0,
        timestamp=time.time(),
        labels={"host": "web-server-1"}
    )
    await alert_system.process_metric(normal_metric)
    
    # å¼‚å¸¸çš„CPUä½¿ç”¨ç‡
    high_metric = MetricData(
        name="cpu_usage_percent",
        value=95.0,
        timestamp=time.time(),
        labels={"host": "web-server-1"}
    )
    await alert_system.process_metric(high_metric)
    
    # é«˜å†…å­˜ä½¿ç”¨ç‡
    memory_metric = MetricData(
        name="memory_usage_percent",
        value=92.0,
        timestamp=time.time(),
        labels={"host": "web-server-1"}
    )
    await alert_system.process_metric(memory_metric)
    
    # æ˜¾ç¤ºå‘Šè­¦ç»Ÿè®¡
    stats = alert_system.get_stats()
    print(f"\nå‘Šè­¦ç³»ç»Ÿç»Ÿè®¡: {stats}")
```

## å¯è§‚æµ‹æ€§æ•°æ®æ²»ç†ä¸ä»·å€¼æŒ–æ˜

### æ•°æ®æ²»ç†æ¶æ„

ç°ä»£å¯è§‚æµ‹æ€§ç³»ç»Ÿäº§ç”Ÿæµ·é‡æ•°æ®ï¼Œéœ€è¦æœ‰æ•ˆçš„æ•°æ®æ²»ç†æ¥é™ä½æˆæœ¬å¹¶æŒ–æ˜ä»·å€¼ï¼š

1. **æ•°æ®åˆ†å±‚å­˜å‚¨**ï¼šçƒ­æ•°æ®ã€æ¸©æ•°æ®ã€å†·æ•°æ®çš„åˆ†å±‚ç®¡ç†
2. **æ•°æ®é‡‡æ ·**ï¼šåŸºäºé‡è¦æ€§çš„æ™ºèƒ½é‡‡æ ·
3. **æ•°æ®å‹ç¼©**ï¼šé«˜æ•ˆçš„å‹ç¼©ç®—æ³•
4. **æ•°æ®ä»·å€¼æŒ–æ˜**ï¼šåŸºäºæœºå™¨å­¦ä¹ çš„ä¸šåŠ¡æ´å¯Ÿ

### æ•°æ®æ²»ç†å®ç°

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Iterator
import json
import gzip
import pickle
from datetime import datetime, timedelta
import asyncio

# æ•°æ®å­˜å‚¨åˆ†å±‚
class StorageTier(Enum):
    HOT = "hot"      # SSD, æœ€è¿‘1å°æ—¶
    WARM = "warm"    # æ™®é€šç£ç›˜, æœ€è¿‘7å¤©
    COLD = "cold"    # å¯¹è±¡å­˜å‚¨, é•¿æœŸå½’æ¡£

# æ•°æ®è®¿é—®çº§åˆ«
class AccessLevel(Enum):
    REALTIME = "realtime"    # å®æ—¶æŸ¥è¯¢
    ANALYTICAL = "analytical" # åˆ†ææŸ¥è¯¢
    ARCHIVAL = "archival"     # å½’æ¡£æŸ¥è¯¢

@dataclass
class ObservabilityData:
    data_id: str
    data_type: str  # trace, metric, log
    content: Dict[str, Any]
    timestamp: float
    size_bytes: int
    access_level: AccessLevel
    importance_score: float  # 0-1ï¼Œè¶Šé«˜è¶Šé‡è¦
    labels: Dict[str, str]

# å­˜å‚¨ç­–ç•¥æ¥å£
class StorageStrategy(ABC):
    @abstractmethod
    async def store(self, data: ObservabilityData) -> bool:
        """å­˜å‚¨æ•°æ®"""
        pass
    
    @abstractmethod
    async def retrieve(self, data_id: str) -> Optional[ObservabilityData]:
        """æ£€ç´¢æ•°æ®"""
        pass
    
    @abstractmethod
    async def delete(self, data_id: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        pass
    
    @abstractmethod
    def get_storage_cost_per_gb(self) -> float:
        """è·å–æ¯GBå­˜å‚¨æˆæœ¬"""
        pass

# çƒ­å­˜å‚¨ç­–ç•¥
class HotStorageStrategy(StorageStrategy):
    def __init__(self):
        self.data_store = {}  # å†…å­˜å­˜å‚¨
        self.storage_limit_gb = 1  # 1GBé™åˆ¶
    
    async def store(self, data: ObservabilityData) -> bool:
        """å­˜å‚¨åˆ°å†…å­˜"""
        if self._get_total_size() + data.size_bytes > self.storage_limit_gb * 1024 * 1024 * 1024:
            # æ¸…ç†æœ€è€çš„æ•°æ®
            await self._cleanup_oldest_data()
        
        self.data_store[data.data_id] = data
        print(f"å­˜å‚¨åˆ°çƒ­å­˜å‚¨: {data.data_id}")
        return True
    
    async def retrieve(self, data_id: str) -> Optional[ObservabilityData]:
        """ä»å†…å­˜æ£€ç´¢"""
        return self.data_store.get(data_id)
    
    async def delete(self, data_id: str) -> bool:
        """ä»å†…å­˜åˆ é™¤"""
        if data_id in self.data_store:
            del self.data_store[data_id]
            return True
        return False
    
    def get_storage_cost_per_gb(self) -> float:
        return 10.0  # å†…å­˜å­˜å‚¨æ¯GB 10å…ƒ
    
    def _get_total_size(self) -> int:
        return sum(data.size_bytes for data in self.data_store.values())
    
    async def _cleanup_oldest_data(self):
        """æ¸…ç†æœ€è€çš„æ•°æ®"""
        if not self.data_store:
            return
        
        oldest_id = min(self.data_store.keys(), key=lambda x: self.data_store[x].timestamp)
        del self.data_store[oldest_id]
        print(f"æ¸…ç†æœ€è€æ•°æ®: {oldest_id}")

# æ¸©å­˜å‚¨ç­–ç•¥
class WarmStorageStrategy(StorageStrategy):
    def __init__(self):
        self.data_index = {}  # æ•°æ®ç´¢å¼•
        self.file_storage = "warm_storage/"  # æ–‡ä»¶å­˜å‚¨ç›®å½•
    
    async def store(self, data: ObservabilityData) -> bool:
        """å­˜å‚¨åˆ°æ–‡ä»¶"""
        # å‹ç¼©æ•°æ®
        compressed_data = gzip.compress(json.dumps(data.content).encode('utf-8'))
        
        # å†™å…¥æ–‡ä»¶
        filename = f"{self.file_storage}/{data.data_id}.gz"
        with open(filename, 'wb') as f:
            f.write(compressed_data)
        
        # æ›´æ–°ç´¢å¼•
        self.data_index[data.data_id] = {
            'filename': filename,
            'size_bytes': len(compressed_data),
            'timestamp': data.timestamp
        }
        
        print(f"å­˜å‚¨åˆ°æ¸©å­˜å‚¨: {data.data_id}")
        return True
    
    async def retrieve(self, data_id: str) -> Optional[ObservabilityData]:
        """ä»æ–‡ä»¶æ£€ç´¢"""
        if data_id not in self.data_index:
            return None
        
        index_info = self.data_index[data_id]
        filename = index_info['filename']
        
        try:
            with open(filename, 'rb') as f:
                compressed_data = f.read()
            
            decompressed_data = gzip.decompress(compressed_data).decode('utf-8')
            content = json.loads(decompressed_data)
            
            # é‡å»ºObservabilityDataå¯¹è±¡ï¼ˆç®€åŒ–ï¼‰
            return ObservabilityData(
                data_id=data_id,
                data_type="unknown",
                content=content,
                timestamp=index_info['timestamp'],
                size_bytes=index_info['size_bytes'],
                access_level=AccessLevel.ANALYTICAL,
                importance_score=0.5,
                labels={}
            )
        except Exception as e:
            print(f"ä»æ¸©å­˜å‚¨æ£€ç´¢å¤±è´¥: {e}")
            return None
    
    async def delete(self, data_id: str) -> bool:
        """ä»æ–‡ä»¶åˆ é™¤"""
        if data_id in self.data_index:
            index_info = self.data_index[data_id]
            filename = index_info['filename']
            
            try:
                import os
                if os.path.exists(filename):
                    os.remove(filename)
                
                del self.data_index[data_id]
                return True
            except Exception as e:
                print(f"ä»æ¸©å­˜å‚¨åˆ é™¤å¤±è´¥: {e}")
                return False
        
        return False
    
    def get_storage_cost_per_gb(self) -> float:
        return 1.0  # ç£ç›˜å­˜å‚¨æ¯GB 1å…ƒ

# å†·å­˜å‚¨ç­–ç•¥
class ColdStorageStrategy(StorageStrategy):
    def __init__(self):
        self.archive_index = {}  # å½’æ¡£ç´¢å¼•
    
    async def store(self, data: ObservabilityData) -> bool:
        """å½’æ¡£åˆ°å¯¹è±¡å­˜å‚¨"""
        # æ¨¡æ‹Ÿå½’æ¡£åˆ°äº‘å­˜å‚¨
        archive_key = f"archive/{data.data_type}/{data.data_id}.json.gz"
        
        # æ¨¡æ‹Ÿä¸Šä¼ è¿‡ç¨‹
        print(f"å½’æ¡£åˆ°å†·å­˜å‚¨: {archive_key}")
        
        # æ›´æ–°ç´¢å¼•
        self.archive_index[data.data_id] = {
            'archive_key': archive_key,
            'size_bytes': data.size_bytes,
            'timestamp': data.timestamp,
            'retention_period_days': 365  # ä¿ç•™1å¹´
        }
        
        return True
    
    async def retrieve(self, data_id: str) -> Optional[ObservabilityData]:
        """ä»å½’æ¡£æ£€ç´¢"""
        if data_id not in self.archive_index:
            return None
        
        # æ¨¡æ‹Ÿä»äº‘å­˜å‚¨ä¸‹è½½
        archive_info = self.archive_index[data_id]
        print(f"ä»å†·å­˜å‚¨ä¸‹è½½: {archive_info['archive_key']}")
        
        # æ¨¡æ‹Ÿä¸‹è½½çš„æ•°æ®
        return ObservabilityData(
            data_id=data_id,
            data_type="archived",
            content={"archived": True},
            timestamp=archive_info['timestamp'],
            size_bytes=archive_info['size_bytes'],
            access_level=AccessLevel.ARCHIVAL,
            importance_score=0.1,
            labels={}
        )
    
    async def delete(self, data_id: str) -> bool:
        """ä»å½’æ¡£åˆ é™¤"""
        if data_id in self.archive_index:
            del self.archive_index[data_id]
            return True
        return False
    
    def get_storage_cost_per_gb(self) -> float:
        return 0.1  # å¯¹è±¡å­˜å‚¨æ¯GB 0.1å…ƒ

# æ•°æ®ç®¡ç†å™¨
class DataGovernanceManager:
    def __init__(self):
        self.hot_storage = HotStorageStrategy()
        self.warm_storage = WarmStorageStrategy()
        self.cold_storage = ColdStorageStrategy()
        self.retention_policies = {}  # ä¿ç•™ç­–ç•¥
        self.data_stats = {
            'total_stored': 0,
            'storage_costs': 0.0,
            'data_by_tier': {tier: 0 for tier in StorageTier}
        }
    
    def set_retention_policy(self, data_type: str, policy: Dict[str, Any]):
        """è®¾ç½®æ•°æ®ä¿ç•™ç­–ç•¥"""
        self.retention_policies[data_type] = policy
        print(f"è®¾ç½®æ•°æ®ä¿ç•™ç­–ç•¥: {data_type} -> {policy}")
    
    async def store_observability_data(self, data: ObservabilityData) -> bool:
        """å­˜å‚¨å¯è§‚æµ‹æ€§æ•°æ®"""
        # æ ¹æ®é‡è¦æ€§åˆ†æ•°å’Œä¿ç•™ç­–ç•¥é€‰æ‹©å­˜å‚¨å±‚
        storage_tier = self._determine_storage_tier(data)
        
        # æ ¹æ®è®¿é—®çº§åˆ«é€‰æ‹©å­˜å‚¨ç­–ç•¥
        storage_strategy = self._get_storage_strategy(data.access_level, storage_tier)
        
        # å­˜å‚¨æ•°æ®
        success = await storage_strategy.store(data)
        
        if success:
            self.data_stats['total_stored'] += 1
            self.data_stats['storage_costs'] += (data.size_bytes / (1024 * 1024 * 1024)) * storage_strategy.get_storage_cost_per_gb()
            self.data_stats['data_by_tier'][storage_tier] += 1
        
        return success
    
    def _determine_storage_tier(self, data: ObservabilityData) -> StorageTier:
        """ç¡®å®šå­˜å‚¨å±‚"""
        # åŸºäºé‡è¦æ€§åˆ†æ•°å’Œæ—¶é—´å†³å®šå­˜å‚¨å±‚
        current_time = time.time()
        age_hours = (current_time - data.timestamp) / 3600
        
        if data.importance_score > 0.8 and age_hours < 1:
            return StorageTier.HOT
        elif data.importance_score > 0.5 and age_hours < 24:
            return StorageTier.WARM
        else:
            return StorageTier.COLD
    
    def _get_storage_strategy(self, access_level: AccessLevel, storage_tier: StorageTier) -> StorageStrategy:
        """è·å–å­˜å‚¨ç­–ç•¥"""
        if access_level == AccessLevel.REALTIME:
            return self.hot_storage
        elif access_level == AccessLevel.ANALYTICAL:
            return self.warm_storage
        else:
            return self.cold_storage
    
    async def query_data(self, query: Dict[str, Any]) -> Iterator[ObservabilityData]:
        """æŸ¥è¯¢æ•°æ®"""
        # ç®€åŒ–å®ç°ï¼šæŸ¥è¯¢æ‰€æœ‰å­˜å‚¨å±‚
        all_data = []
        
        # ä»çƒ­å­˜å‚¨æŸ¥è¯¢
        for data_id, data in self.hot_storage.data_store.items():
            if self._matches_query(data, query):
                all_data.append(data)
        
        # ä»æ¸©å­˜å‚¨æŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿï¼‰
        print(f"ä»æ¸©å­˜å‚¨æŸ¥è¯¢: {len(self.warm_storage.data_index)} æ¡è®°å½•")
        
        # ä»å†·å­˜å‚¨æŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿï¼‰
        print(f"ä»å†·å­˜å‚¨æŸ¥è¯¢: {len(self.cold_storage.archive_index)} æ¡è®°å½•")
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        all_data.sort(key=lambda x: x.timestamp, reverse=True)
        
        for data in all_data:
            yield data
    
    def _matches_query(self, data: ObservabilityData, query: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ¹é…æŸ¥è¯¢"""
        if 'data_type' in query and data.data_type != query['data_type']:
            return False
        
        if 'importance_min' in query and data.importance_score < query['importance_min']:
            return False
        
        if 'time_range' in query:
            start_time, end_time = query['time_range']
            if not (start_time <= data.timestamp <= end_time):
                return False
        
        if 'labels' in query:
            for key, value in query['labels'].items():
                if key not in data.labels or data.labels[key] != value:
                    return False
        
        return True
    
    def get_storage_stats(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡"""
        return {
            **self.data_stats,
            'storage_tier_distribution': {tier.value: count for tier, count in self.data_stats['data_by_tier'].items()},
            'retention_policies': len(self.retention_policies)
        }

# æ•°æ®ä»·å€¼æŒ–æ˜
class DataValueExtractor:
    def __init__(self, data_manager: DataGovernanceManager):
        self.data_manager = data_manager
    
    async def analyze_performance_trends(self, time_range_hours: int = 24) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        end_time = time.time()
        start_time = end_time - (time_range_hours * 3600)
        
        query = {
            'data_type': 'metric',
            'time_range': (start_time, end_time)
        }
        
        metrics = []
        async for data in self.data_manager.query_data(query):
            metrics.append(data)
        
        if not metrics:
            return {'error': 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æŒ‡æ ‡æ•°æ®'}
        
        # æå–æ€§èƒ½æŒ‡æ ‡è¶‹åŠ¿
        cpu_metrics = [m for m in metrics if 'cpu' in m.labels.get('metric_name', '')]
        memory_metrics = [m for m in metrics if 'memory' in m.labels.get('metric_name', '')]
        
        trends = {
            'cpu_trend': self._calculate_trend(cpu_metrics),
            'memory_trend': self._calculate_trend(memory_metrics),
            'total_metrics_analyzed': len(metrics),
            'analysis_period_hours': time_range_hours
        }
        
        return trends
    
    def _calculate_trend(self, metrics: List[ObservabilityData]) -> Dict[str, Any]:
        """è®¡ç®—æŒ‡æ ‡è¶‹åŠ¿"""
        if len(metrics) < 2:
            return {'trend': 'insufficient_data'}
        
        values = [m.content.get('value', 0) for m in metrics]
        timestamps = [m.timestamp for m in metrics]
        
        # ç®€å•çš„çº¿æ€§è¶‹åŠ¿è®¡ç®—
        if len(values) >= 2:
            slope = (values[-1] - values[0]) / (timestamps[-1] - timestamps[0])
            
            if slope > 0.01:
                trend = 'increasing'
            elif slope < -0.01:
                trend = 'decreasing'
            else:
                trend = 'stable'
            
            return {
                'trend': trend,
                'slope': slope,
                'start_value': values[0],
                'end_value': values[-1],
                'change_percent': ((values[-1] - values[0]) / max(values[0], 1)) * 100
            }
        
        return {'trend': 'unknown'}
    
    async def extract_business_insights(self) -> Dict[str, Any]:
        """æå–ä¸šåŠ¡æ´å¯Ÿ"""
        # æŸ¥è¯¢æœ€è¿‘çš„ä¸šåŠ¡æŒ‡æ ‡
        query = {
            'data_type': 'metric',
            'time_range': (time.time() - 24 * 3600, time.time())
        }
        
        business_metrics = []
        async for data in self.data_manager.query_data(query):
            if data.labels.get('category') == 'business':
                business_metrics.append(data)
        
        insights = {
            'total_business_metrics': len(business_metrics),
            'key_metrics': [],
            'recommendations': []
        }
        
        # åˆ†æå…³é”®ä¸šåŠ¡æŒ‡æ ‡
        for metric in business_metrics:
            insights['key_metrics'].append({
                'metric_name': metric.labels.get('metric_name'),
                'current_value': metric.content.get('value'),
                'importance_score': metric.importance_score
            })
        
        # ç”Ÿæˆå»ºè®®
        if len(business_metrics) > 0:
            insights['recommendations'].extend([
                'å…³æ³¨é«˜é‡è¦æ€§åˆ†æ•°çš„ä¸šåŠ¡æŒ‡æ ‡',
                'è€ƒè™‘å®æ–½A/Bæµ‹è¯•ä¼˜åŒ–å…³é”®æŒ‡æ ‡',
                'å»ºç«‹ä¸šåŠ¡æŒ‡æ ‡å‘Šè­¦æœºåˆ¶'
            ])
        
        return insights

# ä½¿ç”¨ç¤ºä¾‹
async def demonstrate_data_governance():
    """æ¼”ç¤ºæ•°æ®æ²»ç†ç³»ç»Ÿ"""
    
    # åˆ›å»ºæ•°æ®æ²»ç†ç®¡ç†å™¨
    data_manager = DataGovernanceManager()
    value_extractor = DataValueExtractor(data_manager)
    
    # è®¾ç½®ä¿ç•™ç­–ç•¥
    data_manager.set_retention_policy('trace', {
        'hot_retention_hours': 1,
        'warm_retention_hours': 168,  # 7å¤©
        'archive_after_days': 30
    })
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("=== æ•°æ®æ²»ç†ç³»ç»Ÿæ¼”ç¤º ===")
    
    # åˆ›å»ºä¸åŒç±»å‹å’Œé‡è¦æ€§çš„æ•°æ®
    test_data = [
        ObservabilityData(
            data_id="trace_001",
            data_type="trace",
            content={"operation": "process_order", "duration": 150},
            timestamp=time.time(),
            size_bytes=1024,
            access_level=AccessLevel.REALTIME,
            importance_score=0.9,
            labels={"service": "order-service", "operation": "process_order"}
        ),
        ObservabilityData(
            data_id="metric_001",
            data_type="metric",
            content={"name": "cpu_usage", "value": 75.5},
            timestamp=time.time(),
            size_bytes=256,
            access_level=AccessLevel.ANALYTICAL,
            importance_score=0.7,
            labels={"host": "server-1", "metric_name": "cpu_usage"}
        ),
        ObservabilityData(
            data_id="log_001",
            data_type="log",
            content={"level": "INFO", "message": "Application started"},
            timestamp=time.time() - 3600,  # 1å°æ—¶å‰
            size_bytes=512,
            access_level=AccessLevel.ARCHIVAL,
            importance_score=0.3,
            labels={"service": "app-service"}
        )
    ]
    
    # å­˜å‚¨æ•°æ®
    for data in test_data:
        success = await data_manager.store_observability_data(data)
        print(f"å­˜å‚¨æ•°æ® {data.data_id}: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # æŸ¥è¯¢æ•°æ®
    print("\næŸ¥è¯¢æœ€è¿‘24å°æ—¶çš„æŒ‡æ ‡æ•°æ®:")
    query = {
        'data_type': 'metric',
        'time_range': (time.time() - 24 * 3600, time.time())
    }
    
    async for data in data_manager.query_data(query):
        print(f"  - {data.data_id}: {data.content}")
    
    # æå–ä¸šåŠ¡æ´å¯Ÿ
    print("\næå–ä¸šåŠ¡æ´å¯Ÿ:")
    insights = await value_extractor.extract_business_insights()
    print(f"æ´å¯Ÿç»“æœ: {insights}")
    
    # æ˜¾ç¤ºå­˜å‚¨ç»Ÿè®¡
    print("\nå­˜å‚¨ç»Ÿè®¡:")
    stats = data_manager.get_storage_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demonstrate_data_governance())
```

## æ€»ç»“

é€šè¿‡è¿™äº›é«˜çº§å¯è§‚æµ‹æ€§å®è·µï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç°ä»£åŒ–çš„å¯è§‚æµ‹æ€§ç³»ç»Ÿï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

### 1. é«˜çº§åˆ†å¸ƒå¼è¿½è¸ª

- **ç«¯åˆ°ç«¯è¿½è¸ª**ï¼šå®Œæ•´çš„è¯·æ±‚é“¾è·¯è¿½è¸ª
- **å¤šåè®®æ”¯æŒ**ï¼šé›†æˆJaegerã€Zipkinç­‰ä¸»æµè¿½è¸ªç³»ç»Ÿ
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæ‰¹é‡å¤„ç†ã€æ™ºèƒ½é‡‡æ ·
- **ä¸Šä¸‹æ–‡ä¼ æ’­**ï¼šè‡ªåŠ¨è·¨æœåŠ¡ä¼ æ’­è¿½è¸ªä¸Šä¸‹æ–‡

### 2. æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ

- **åŠ¨æ€é˜ˆå€¼**ï¼šåŸºäºå†å²æ•°æ®çš„è‡ªé€‚åº”é˜ˆå€¼
- **å¼‚å¸¸æ£€æµ‹**ï¼šå¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•
- **å‘Šè­¦é™å™ª**ï¼šæ™ºèƒ½å»é‡å’Œèšåˆ
- **æ ¹å› åˆ†æ**ï¼šæä¾›é—®é¢˜è¯Šæ–­å»ºè®®

### 3. æ•°æ®æ²»ç†ä¸ä»·å€¼æŒ–æ˜

- **åˆ†å±‚å­˜å‚¨**ï¼šçƒ­ã€æ¸©ã€å†·æ•°æ®çš„æ™ºèƒ½ç®¡ç†
- **æˆæœ¬ä¼˜åŒ–**ï¼šæ ¹æ®è®¿é—®æ¨¡å¼ä¼˜åŒ–å­˜å‚¨æˆæœ¬
- **ä»·å€¼æŒ–æ˜**ï¼šåŸºäºæœºå™¨å­¦ä¹ çš„ä¸šåŠ¡æ´å¯Ÿ
- **åˆè§„æ€§**ï¼šæ•°æ®ä¿ç•™å’Œåˆ é™¤ç­–ç•¥

### 4. å·¥ç¨‹åŒ–å®è·µ

- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•çš„æ¶æ„è®¾è®¡
- **é«˜å¯ç”¨æ€§**ï¼šå¤šå±‚æ¬¡çš„æ•…éšœæ¢å¤æœºåˆ¶
- **æ˜“ç»´æŠ¤æ€§**ï¼šæ¸…æ™°çš„æ¥å£å®šä¹‰å’Œç»„ä»¶åˆ†ç¦»
- **è§‚æµ‹æ€§**ï¼šç³»ç»Ÿè‡ªèº«çš„å¯è§‚æµ‹æ€§è®¾è®¡

è¿™äº›å®è·µä¸ä»…æå‡äº†ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§ï¼Œæ›´é‡è¦çš„æ˜¯ä¸ºä¼ä¸šæä¾›äº†æ•°æ®é©±åŠ¨çš„å†³ç­–æ”¯æŒï¼Œå®ç°äº†ä»è¢«åŠ¨å“åº”åˆ°ä¸»åŠ¨é¢„é˜²çš„è½¬å˜ã€‚