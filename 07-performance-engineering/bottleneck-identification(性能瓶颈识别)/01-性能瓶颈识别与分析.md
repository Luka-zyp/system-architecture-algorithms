# 性能瓶颈识别与分析

## 1. 性能瓶颈类型识别

### CPU密集型瓶颈
- 长时间高CPU使用率
- 计算密集型操作执行缓慢
- 线程池线程不足导致排队等待

### 内存密集型瓶颈  
- 内存泄漏导致内存耗尽
- 频繁GC导致性能下降
- 大对象缓存导致内存压力

### I/O密集型瓶颈
- 磁盘读写速度瓶颈
- 网络IO等待时间过长
- 数据库连接池耗尽

### 网络瓶颈
- 网络带宽不足
- 网络延迟过高
- 连接建立/断开频繁

## 2. 性能监控指标

### 系统层面指标
```python
import psutil
import time
from typing import Dict, List

class SystemMetrics:
    def __init__(self):
        self.metrics_history = []
    
    def collect_cpu_metrics(self) -> Dict:
        """收集CPU相关指标"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'cpu_count': psutil.cpu_count(),
            'load_average': psutil.getloadavg() if hasattr(psutil, 'getloadavg') else (0, 0, 0),
            'process_count': len(psutil.pids())
        }
    
    def collect_memory_metrics(self) -> Dict:
        """收集内存相关指标"""
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()
        return {
            'memory_percent': memory.percent,
            'memory_available': memory.available,
            'memory_used': memory.used,
            'memory_total': memory.total,
            'swap_percent': swap.percent,
            'swap_used': swap.used,
            'swap_total': swap.total
        }
    
    def collect_disk_metrics(self) -> Dict:
        """收集磁盘I/O指标"""
        disk_io = psutil.disk_io_counters()
        disk_usage = psutil.disk_usage('/')
        return {
            'disk_read_bytes': disk_io.read_bytes if disk_io else 0,
            'disk_write_bytes': disk_io.write_bytes if disk_io else 0,
            'disk_read_count': disk_io.read_count if disk_io else 0,
            'disk_write_count': disk_io.write_count if disk_io else 0,
            'disk_usage_percent': (disk_usage.used / disk_usage.total) * 100,
            'disk_free': disk_usage.free,
            'disk_total': disk_usage.total
        }
    
    def collect_network_metrics(self) -> Dict:
        """收集网络相关指标"""
        network_io = psutil.net_io_counters()
        network_connections = len(psutil.net_connections())
        return {
            'network_bytes_sent': network_io.bytes_sent if network_io else 0,
            'network_bytes_recv': network_io.bytes_recv if network_io else 0,
            'network_packets_sent': network_io.packets_sent if network_io else 0,
            'network_packets_recv': network_io.packets_recv if network_io else 0,
            'network_connections': network_connections
        }
```

### 应用层面指标
```python
import threading
import time
from collections import defaultdict, deque
from dataclasses import dataclass
from typing import Optional, Dict, List
import functools

@dataclass
class RequestMetric:
    timestamp: float
    response_time: float
    status_code: int
    endpoint: str
    method: str

class ApplicationMetrics:
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.request_metrics = deque(maxlen=window_size)
        self.active_requests = 0
        self.lock = threading.Lock()
        self.endpoint_latencies = defaultdict(lambda: deque(maxlen=100))
        self.error_counts = defaultdict(int)
        
    def record_request(self, response_time: float, status_code: int, 
                      endpoint: str, method: str):
        """记录请求指标"""
        metric = RequestMetric(
            timestamp=time.time(),
            response_time=response_time,
            status_code=status_code,
            endpoint=endpoint,
            method=method
        )
        
        with self.lock:
            self.request_metrics.append(metric)
            self.endpoint_latencies[endpoint].append(response_time)
            
            if status_code >= 400:
                self.error_counts[f"{method} {endpoint}"] += 1
    
    def get_performance_summary(self) -> Dict:
        """获取性能摘要"""
        with self.lock:
            if not self.request_metrics:
                return {}
            
            response_times = [m.response_time for m in self.request_metrics]
            error_rate = len([m for m in self.request_metrics if m.status_code >= 400]) / len(self.request_metrics)
            
            return {
                'avg_response_time': sum(response_times) / len(response_times),
                'p95_response_time': sorted(response_times)[int(len(response_times) * 0.95)],
                'p99_response_time': sorted(response_times)[int(len(response_times) * 0.99)],
                'min_response_time': min(response_times),
                'max_response_time': max(response_times),
                'total_requests': len(self.request_metrics),
                'error_rate': error_rate,
                'active_requests': self.active_requests
            }
    
    def get_endpoint_stats(self) -> Dict:
        """获取端点统计信息"""
        with self.lock:
            stats = {}
            for endpoint, latencies in self.endpoint_latencies.items():
                if latencies:
                    stats[endpoint] = {
                        'avg_latency': sum(latencies) / len(latencies),
                        'p95_latency': sorted(latencies)[int(len(latencies) * 0.95)],
                        'p99_latency': sorted(latencies)[int(len(latencies) * 0.99)],
                        'min_latency': min(latencies),
                        'max_latency': max(latencies),
                        'request_count': len(latencies),
                        'error_count': self.error_counts.get(endpoint, 0)
                    }
            return stats

def measure_performance(func):
    """性能测量装饰器"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            status_code = 200
        except Exception as e:
            status_code = 500
            raise
        finally:
            end_time = time.time()
            response_time = end_time - start_time
            
            # 这里应该集成到你的应用指标收集系统中
            print(f"{func.__name__} - Response time: {response_time:.4f}s, Status: {status_code}")
        
        return result
    return wrapper
```

## 3. 性能分析工具

### 火焰图生成
```python
import subprocess
import json
from typing import List, Dict

class FlameGraphGenerator:
    def __init__(self):
        self.stack_samples = []
    
    def collect_stack_samples(self, duration: int = 10) -> List[str]:
        """收集堆栈采样"""
        try:
            # 使用perf收集堆栈信息
            cmd = [
                'perf', 'record', '-F', '99', '-ag', '--', 
                'sleep', str(duration)
            ]
            subprocess.run(cmd, check=True)
            
            # 转换堆栈跟踪
            cmd = ['perf', 'script']
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            stack_traces = []
            for line in result.stdout.split('\n'):
                if line.strip() and '[' in line:
                    stack_traces.append(line.strip())
            
            return stack_traces
        except (subprocess.CalledProcessError, FileNotFoundError):
            return []
    
    def generate_flamegraph_data(self, stack_traces: List[str]) -> Dict:
        """生成火焰图数据"""
        stack_tree = {}
        
        for trace in stack_traces:
            functions = trace.split('->')
            current = stack_tree
            
            for func in functions:
                func = func.strip()
                if func not in current:
                    current[func] = {'count': 0, 'children': {}}
                current[func]['count'] += 1
                current = current[func]['children']
        
        return stack_tree
    
    def identify_hotspots(self, stack_tree: Dict, threshold: float = 0.05) -> List[tuple]:
        """识别性能热点"""
        hotspots = []
        total_count = self._get_total_count(stack_tree)
        
        def traverse(node, path, count):
            if count / total_count >= threshold:
                hotspots.append((path, count / total_count))
            
            for func, child in node.items():
                traverse(child['children'], path + [func], child['count'])
        
        traverse(stack_tree, [], total_count)
        return sorted(hotspots, key=lambda x: x[1], reverse=True)
    
    def _get_total_count(self, stack_tree: Dict) -> int:
        """获取总计数"""
        if not stack_tree:
            return 0
        return sum(child['count'] + self._get_total_count(child['children']) 
                  for child in stack_tree.values())
```

### 内存分析
```python
import tracemalloc
import gc
import sys
from typing import List, Dict, Tuple

class MemoryProfiler:
    def __init__(self):
        self.snapshots = []
        self.peak_usage = 0
        
    def start_memory_tracking(self):
        """开始内存跟踪"""
        tracemalloc.start()
        self.peak_usage = tracemalloc.get_traced_memory()[0]
    
    def stop_memory_tracking(self) -> Tuple[int, int]:
        """停止内存跟踪"""
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        return current, peak
    
    def take_snapshot(self) -> str:
        """拍摄内存快照"""
        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append(snapshot)
        return str(snapshot)
    
    def analyze_memory_usage(self) -> Dict:
        """分析内存使用情况"""
        if not self.snapshots:
            return {}
        
        stats = self.snapshots[-1].statistics('lineno')
        top_stats = stats[:10]
        
        memory_info = []
        total_allocated = 0
        
        for stat in top_stats:
            total_allocated += stat.size
            memory_info.append({
                'filename': stat.traceback.filename,
                'lineno': stat.traceback.lineno,
                'size': stat.size,
                'count': stat.count,
                'size_mb': stat.size / 1024 / 1024
            })
        
        return {
            'total_allocated_mb': total_allocated / 1024 / 1024,
            'top_memory_users': memory_info,
            'traces': len(stats)
        }
    
    def compare_snapshots(self, snapshot1_idx: int = 0, snapshot2_idx: int = -1) -> Dict:
        """比较两个内存快照"""
        if len(self.snapshots) < 2:
            return {}
        
        snapshot1 = self.snapshots[snapshot1_idx]
        snapshot2 = self.snapshots[snapshot2_idx]
        
        top_stats = snapshot2.compare_to(snapshot1, 'lineno')
        
        comparison = []
        for stat in top_stats[:10]:
            comparison.append({
                'filename': stat.traceback.filename,
                'lineno': stat.traceback.lineno,
                'size_diff': stat.size_diff,
                'count_diff': stat.count_diff,
                'size_diff_mb': stat.size_diff / 1024 / 1024
            })
        
        return {
            'memory_growth': comparison,
            'top_growth_areas': sorted(comparison, 
                                     key=lambda x: abs(x['size_diff']), 
                                     reverse=True)[:5]
        }
    
    def detect_memory_leaks(self) -> List[Dict]:
        """检测内存泄漏"""
        gc.collect()
        objects_before = len(gc.get_objects())
        
        # 运行一些操作
        test_data = list(range(100000))
        test_dict = {f"key_{i}": f"value_{i}" for i in range(100000)}
        
        # 清理
        del test_data, test_dict
        gc.collect()
        
        objects_after = len(gc.get_objects())
        objects_leaked = objects_after - objects_before
        
        return [
            {
                'objects_before': objects_before,
                'objects_after': objects_after,
                'objects_leaked': objects_leaked,
                'leak_detected': objects_leaked > 1000
            }
        ]
```

## 4. 瓶颈诊断流程

### 自动诊断系统
```python
from typing import List, Dict, Optional
import time
from dataclasses import dataclass

@dataclass
class DiagnosticResult:
    bottleneck_type: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    description: str
    recommendations: List[str]
    metrics: Dict

class PerformanceDiagnosticEngine:
    def __init__(self):
        self.system_metrics = SystemMetrics()
        self.app_metrics = ApplicationMetrics()
        self.thresholds = {
            'cpu_percent': 80,
            'memory_percent': 85,
            'disk_usage_percent': 90,
            'avg_response_time': 1.0,
            'error_rate': 0.01,
            'p95_response_time': 2.0
        }
    
    def run_diagnostic(self, duration: int = 60) -> List[DiagnosticResult]:
        """运行性能诊断"""
        results = []
        
        # 收集指标
        system_data = self._collect_system_metrics(duration)
        app_data = self._collect_app_metrics(duration)
        
        # CPU瓶颈诊断
        cpu_result = self._diagnose_cpu_bottleneck(system_data)
        if cpu_result:
            results.append(cpu_result)
        
        # 内存瓶颈诊断
        memory_result = self._diagnose_memory_bottleneck(system_data)
        if memory_result:
            results.append(memory_result)
        
        # 磁盘I/O瓶颈诊断
        disk_result = self._diagnose_disk_bottleneck(system_data)
        if disk_result:
            results.append(disk_result)
        
        # 应用响应时间瓶颈诊断
        response_result = self._diagnose_response_time_bottleneck(app_data)
        if response_result:
            results.append(response_result)
        
        # 错误率瓶颈诊断
        error_result = self._diagnose_error_rate_bottleneck(app_data)
        if error_result:
            results.append(error_result)
        
        return sorted(results, key=lambda x: self._severity_rank(x.severity), reverse=True)
    
    def _collect_system_metrics(self, duration: int) -> Dict:
        """收集系统指标"""
        cpu_samples = []
        memory_samples = []
        disk_samples = []
        
        for _ in range(duration):
            cpu_samples.append(self.system_metrics.collect_cpu_metrics())
            memory_samples.append(self.system_metrics.collect_memory_metrics())
            disk_samples.append(self.system_metrics.collect_disk_metrics())
            time.sleep(1)
        
        return {
            'cpu': cpu_samples,
            'memory': memory_samples,
            'disk': disk_samples
        }
    
    def _collect_app_metrics(self, duration: int) -> Dict:
        """收集应用指标"""
        time.sleep(duration)  # 等待指标积累
        return {
            'performance_summary': self.app_metrics.get_performance_summary(),
            'endpoint_stats': self.app_metrics.get_endpoint_stats()
        }
    
    def _diagnose_cpu_bottleneck(self, system_data: Dict) -> Optional[DiagnosticResult]:
        """诊断CPU瓶颈"""
        cpu_samples = system_data['cpu']
        avg_cpu = sum(sample['cpu_percent'] for sample in cpu_samples) / len(cpu_samples)
        
        if avg_cpu > self.thresholds['cpu_percent']:
            recommendations = [
                "分析CPU密集型操作，考虑算法优化",
                "增加CPU核心数量或使用更强的CPU",
                "检查是否有死循环或低效的计算",
                "考虑使用多线程/协程来提高并发度",
                "分析进程优先级设置"
            ]
            
            return DiagnosticResult(
                bottleneck_type="CPU密集型",
                severity="high" if avg_cpu > 95 else "medium",
                description=f"平均CPU使用率 {avg_cpu:.1f}% 超过阈值 {self.thresholds['cpu_percent']}%",
                recommendations=recommendations,
                metrics={'avg_cpu_percent': avg_cpu}
            )
        return None
    
    def _diagnose_memory_bottleneck(self, system_data: Dict) -> Optional[DiagnosticResult]:
        """诊断内存瓶颈"""
        memory_samples = system_data['memory']
        avg_memory = sum(sample['memory_percent'] for sample in memory_samples) / len(memory_samples)
        avg_swap = sum(sample['swap_percent'] for sample in memory_samples) / len(memory_samples)
        
        if avg_memory > self.thresholds['memory_percent'] or avg_swap > 10:
            recommendations = [
                "检查内存泄漏，释放不必要的对象引用",
                "增加系统内存容量",
                "优化缓存策略，减少内存占用",
                "使用内存池或对象池来减少内存分配",
                "考虑使用更内存高效的数据结构"
            ]
            
            severity = "high" if avg_memory > 95 else "medium"
            if avg_swap > 50:
                severity = "critical"
            
            return DiagnosticResult(
                bottleneck_type="内存密集型",
                severity=severity,
                description=f"平均内存使用率 {avg_memory:.1f}%, 交换分区使用率 {avg_swap:.1f}%",
                recommendations=recommendations,
                metrics={'avg_memory_percent': avg_memory, 'avg_swap_percent': avg_swap}
            )
        return None
    
    def _diagnose_disk_bottleneck(self, system_data: Dict) -> Optional[DiagnosticResult]:
        """诊断磁盘I/O瓶颈"""
        disk_samples = system_data['disk']
        avg_disk_usage = sum(sample['disk_usage_percent'] for sample in disk_samples) / len(disk_samples)
        
        if avg_disk_usage > self.thresholds['disk_usage_percent']:
            recommendations = [
                "清理磁盘空间，删除不必要的文件",
                "迁移数据到更大的磁盘或分布式存储",
                "优化磁盘I/O模式，减少随机读写",
                "使用SSD替代传统硬盘",
                "实现数据压缩以节省空间"
            ]
            
            return DiagnosticResult(
                bottleneck_type="磁盘I/O",
                severity="high" if avg_disk_usage > 95 else "medium",
                description=f"磁盘使用率 {avg_disk_usage:.1f}% 超过阈值 {self.thresholds['disk_usage_percent']}%",
                recommendations=recommendations,
                metrics={'avg_disk_usage_percent': avg_disk_usage}
            )
        return None
    
    def _diagnose_response_time_bottleneck(self, app_data: Dict) -> Optional[DiagnosticResult]:
        """诊断响应时间瓶颈"""
        perf_summary = app_data.get('performance_summary', {})
        avg_response_time = perf_summary.get('avg_response_time', 0)
        p95_response_time = perf_summary.get('p95_response_time', 0)
        
        if avg_response_time > self.thresholds['avg_response_time'] or p95_response_time > self.thresholds['p95_response_time']:
            recommendations = [
                "分析慢查询和API调用，优化数据库查询",
                "增加缓存层，减少数据库压力",
                "优化代码逻辑，减少不必要的计算",
                "增加服务器实例以分散负载",
                "使用CDN加速静态资源访问"
            ]
            
            severity = "high" if p95_response_time > 5 else "medium"
            
            return DiagnosticResult(
                bottleneck_type="响应时间",
                severity=severity,
                description=f"平均响应时间 {avg_response_time:.2f}s, P95响应时间 {p95_response_time:.2f}s",
                recommendations=recommendations,
                metrics=perf_summary
            )
        return None
    
    def _diagnose_error_rate_bottleneck(self, app_data: Dict) -> Optional[DiagnosticResult]:
        """诊断错误率瓶颈"""
        perf_summary = app_data.get('performance_summary', {})
        error_rate = perf_summary.get('error_rate', 0)
        
        if error_rate > self.thresholds['error_rate']:
            recommendations = [
                "检查错误日志，定位错误根源",
                "加强输入验证和异常处理",
                "优化数据库连接和查询语句",
                "检查外部依赖服务状态",
                "实现熔断器模式避免级联故障"
            ]
            
            return DiagnosticResult(
                bottleneck_type="高错误率",
                severity="high" if error_rate > 0.1 else "medium",
                description=f"错误率 {error_rate:.2%} 超过阈值 {self.thresholds['error_rate']:.2%}",
                recommendations=recommendations,
                metrics={'error_rate': error_rate}
            )
        return None
    
    def _severity_rank(self, severity: str) -> int:
        """获取严重性等级"""
        ranks = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}
        return ranks.get(severity, 0)
```

## 5. 性能优化建议

### 通用优化策略
1. **代码层面优化**
   - 算法复杂度分析
   - 数据结构选择
   - 循环和条件语句优化
   - 函数调用开销减少

2. **系统层面优化**
   - 内核参数调优
   - 文件系统优化
   - 网络栈调优
   - 内存管理优化

3. **架构层面优化**
   - 缓存策略设计
   - 数据库查询优化
   - 异步处理模式
   - 负载均衡配置

4. **监控和告警**
   - 实时性能监控
   - 趋势分析和预测
   - 阈值告警设置
   - 自动化诊断流程

### 性能调优检查清单
- [ ] CPU使用率和负载监控
- [ ] 内存使用情况和GC分析
- [ ] 磁盘I/O性能评估
- [ ] 网络延迟和吞吐量测试
- [ ] 数据库连接池和查询优化
- [ ] 缓存命中率和策略分析
- [ ] 线程池和连接池配置
- [ ] JVM参数和垃圾回收调优
- [ ] 应用程序代码性能分析
- [ ] 系统资源限制和配额设置

## 总结

性能瓶颈识别是系统性能优化的基础工作。通过系统化的监控、分析和诊断流程，能够快速定位性能问题并提供针对性的优化建议。关键是要建立完善的监控体系，结合自动化诊断工具，实现性能问题的早期发现和快速解决。