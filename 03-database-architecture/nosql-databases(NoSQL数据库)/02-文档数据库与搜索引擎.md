# 文档数据库与搜索引擎

## 概述

随着大数据时代的到来，传统的结构化数据库已无法满足现代应用对灵活性、扩展性和全文检索的需求。文档数据库和搜索引擎作为NoSQL数据库的两大重要分支，以其独特的数据模型和强大的检索能力，成为了现代应用架构中不可或缺的组件。本文档将深入探讨文档数据库和搜索引擎的架构设计、核心原理、实际实现和最佳实践。

## 1. 文档数据库架构设计

### 1.1 文档数据库核心概念

```python
from typing import Dict, List, Any, Optional, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
import json
import asyncio
import time
import hashlib
from datetime import datetime
import re
from collections import defaultdict

class DocumentType(Enum):
    """文档类型"""
    JSON = "json"
    BSON = "bson"
    XML = "xml"
    YAML = "yaml"

class IndexType(Enum):
    """索引类型"""
    BTREE = "btree"
    HASH = "hash"
    TEXT = "text"
    GEO = "geospatial"
    ARRAY = "array"

@dataclass
class FieldSchema:
    """字段模式定义"""
    field_name: str
    field_type: str  # "string", "number", "boolean", "date", "array", "object"
    is_indexed: bool = False
    is_required: bool = False
    default_value: Any = None
    validation_rules: List[str] = field(default_factory=list)

@dataclass
class DocumentSchema:
    """文档模式"""
    schema_id: str
    collection_name: str
    fields: List[FieldSchema]
    indexes: List[str] = field(default_factory=list)
    version: str = "1.0"

@dataclass
class Document:
    """文档对象"""
    doc_id: str
    collection: str
    data: Dict[str, Any]
    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: Optional[datetime] = None
    version: int = 1
    metadata: Dict[str, Any] = field(default_factory=dict)

class DocumentValidator:
    """文档验证器"""
    
    def __init__(self, schema: DocumentSchema):
        self.schema = schema
        self.field_schemas = {field.field_name: field for field in schema.fields}
    
    def validate(self, document: Document) -> Tuple[bool, List[str]]:
        """验证文档"""
        errors = []
        
        # 检查必需字段
        for field_schema in self.schema.fields:
            if field_schema.is_required:
                if field_schema.field_name not in document.data:
                    errors.append(f"Required field '{field_schema.field_name}' is missing")
        
        # 验证字段类型和规则
        for field_name, value in document.data.items():
            if field_name in self.field_schemas:
                field_schema = self.field_schemas[field_name]
                field_errors = self._validate_field(field_schema, value)
                errors.extend(field_errors)
            else:
                errors.append(f"Unknown field '{field_name}'")
        
        return len(errors) == 0, errors
    
    def _validate_field(self, field_schema: FieldSchema, value: Any) -> List[str]:
        """验证单个字段"""
        errors = []
        
        # 类型验证
        expected_type = field_schema.field_type
        if not self._check_type(value, expected_type):
            errors.append(f"Field '{field_schema.field_name}' type mismatch: expected {expected_type}")
        
        # 应用验证规则
        for rule in field_schema.validation_rules:
            rule_error = self._apply_validation_rule(field_schema.field_name, value, rule)
            if rule_error:
                errors.append(rule_error)
        
        return errors
    
    def _check_type(self, value: Any, expected_type: str) -> bool:
        """检查数据类型"""
        type_map = {
            "string": str,
            "number": (int, float),
            "boolean": bool,
            "date": datetime,
            "array": list,
            "object": dict
        }
        
        expected_python_type = type_map.get(expected_type)
        if expected_python_type:
            if isinstance(expected_python_type, tuple):
                return isinstance(value, expected_python_type)
            else:
                return isinstance(value, expected_python_type)
        
        return True
    
    def _apply_validation_rule(self, field_name: str, value: Any, rule: str) -> Optional[str]:
        """应用验证规则"""
        # 简化的规则验证实现
        if rule.startswith("min_length:"):
            min_length = int(rule.split(":")[1])
            if isinstance(value, str) and len(value) < min_length:
                return f"Field '{field_name}' must be at least {min_length} characters long"
        
        elif rule.startswith("max_length:"):
            max_length = int(rule.split(":")[1])
            if isinstance(value, str) and len(value) > max_length:
                return f"Field '{field_name}' must not exceed {max_length} characters"
        
        elif rule.startswith("min_value:"):
            min_value = float(rule.split(":")[1])
            if isinstance(value, (int, float)) and value < min_value:
                return f"Field '{field_name}' must be at least {min_value}"
        
        elif rule.startswith("pattern:"):
            pattern = rule.split(":", 1)[1]
            if isinstance(value, str) and not re.match(pattern, value):
                return f"Field '{field_name}' does not match pattern {pattern}"
        
        return None

class InvertedIndex:
    """倒排索引"""
    
    def __init__(self):
        self.index = defaultdict(set)  # term -> set of document IDs
        self.document_freq = defaultdict(int)  # term -> document frequency
        self.term_freq = defaultdict(lambda: defaultdict(int))  # doc_id -> term -> frequency
    
    def add_document(self, doc_id: str, terms: List[str]):
        """添加文档到索引"""
        unique_terms = list(set(terms))  # 去重
        
        for term in unique_terms:
            self.index[term].add(doc_id)
            self.document_freq[term] = len(self.index[term])
            self.term_freq[doc_id][term] += terms.count(term)
    
    def remove_document(self, doc_id: str):
        """从索引中移除文档"""
        for term in self.term_freq[doc_id]:
            self.index[term].discard(doc_id)
            self.document_freq[term] = len(self.index[term])
            
            # 如果词项没有文档了，清理索引
            if self.document_freq[term] == 0:
                del self.index[term]
                del self.document_freq[term]
        
        del self.term_freq[doc_id]
    
    def search(self, terms: List[str]) -> List[str]:
        """搜索文档"""
        if not terms:
            return []
        
        # 获取包含所有搜索词的文档集合
        result_sets = []
        for term in terms:
            result_sets.append(self.index.get(term, set()))
        
        # 计算词频权重
        doc_scores = defaultdict(float)
        
        for doc_id in set().union(*result_sets):
            score = 0
            for term in terms:
                if doc_id in self.index[term]:
                    tf = self.term_freq[doc_id][term]  # 词频
                    idf = len(self.index) / self.document_freq[term]  # 逆文档频率
                    score += tf * idf
            doc_scores[doc_id] = score
        
        # 按得分排序
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        return [doc_id for doc_id, score in sorted_docs if score > 0]

class DocumentDatabase:
    """文档数据库核心引擎"""
    
    def __init__(self):
        self.collections = {}  # collection_name -> list of documents
        self.schemas = {}  # collection_name -> DocumentSchema
        self.indexes = {}  # (collection_name, field_name) -> InvertedIndex
        self.validators = {}  # collection_name -> DocumentValidator
        self.statistics = {
            "total_documents": 0,
            "total_collections": 0,
            "total_indexes": 0
        }
        
    def create_collection(self, schema: DocumentSchema):
        """创建集合"""
        self.schemas[schema.collection_name] = schema
        self.collections[schema.collection_name] = []
        self.validators[schema.collection_name] = DocumentValidator(schema)
        self.statistics["total_collections"] += 1
        
        print(f"Created collection '{schema.collection_name}' with {len(schema.fields)} fields")
    
    def insert_document(self, collection: str, document: Document) -> bool:
        """插入文档"""
        if collection not in self.collections:
            raise ValueError(f"Collection '{collection}' does not exist")
        
        # 验证文档
        validator = self.validators[collection]
        is_valid, errors = validator.validate(document)
        
        if not is_valid:
            print(f"Document validation failed: {errors}")
            return False
        
        # 生成文档ID（如果未提供）
        if not document.doc_id:
            document.doc_id = self._generate_document_id(document)
        
        # 检查重复
        for existing_doc in self.collections[collection]:
            if existing_doc.doc_id == document.doc_id:
                print(f"Document with ID '{document.doc_id}' already exists")
                return False
        
        # 插入文档
        self.collections[collection].append(document)
        self.statistics["total_documents"] += 1
        
        # 更新索引
        self._update_indexes(collection, document)
        
        print(f"Inserted document '{document.doc_id}' into collection '{collection}'")
        return True
    
    def update_document(self, collection: str, doc_id: str, updates: Dict[str, Any]) -> bool:
        """更新文档"""
        if collection not in self.collections:
            return False
        
        # 查找文档
        document = None
        for doc in self.collections[collection]:
            if doc.doc_id == doc_id:
                document = doc
                break
        
        if not document:
            print(f"Document '{doc_id}' not found in collection '{collection}'")
            return False
        
        # 保存旧索引数据
        old_data = document.data.copy()
        
        # 更新文档
        document.data.update(updates)
        document.updated_at = datetime.utcnow()
        document.version += 1
        
        # 重新计算索引
        if self._indexes_affected(collection, old_data, updates):
            # 移除旧索引
            self._remove_from_indexes(collection, old_data, doc_id)
            # 添加新索引
            self._update_indexes(collection, document)
        
        print(f"Updated document '{doc_id}' in collection '{collection}'")
        return True
    
    def delete_document(self, collection: str, doc_id: str) -> bool:
        """删除文档"""
        if collection not in self.collections:
            return False
        
        # 查找并移除文档
        for i, doc in enumerate(self.collections[collection]):
            if doc.doc_id == doc_id:
                # 从索引中移除
                self._remove_from_indexes(collection, doc.data, doc_id)
                
                # 移除文档
                self.collections[collection].pop(i)
                self.statistics["total_documents"] -= 1
                
                print(f"Deleted document '{doc_id}' from collection '{collection}'")
                return True
        
        print(f"Document '{doc_id}' not found in collection '{collection}'")
        return False
    
    def find_document(self, collection: str, doc_id: str) -> Optional[Document]:
        """查找单个文档"""
        if collection not in self.collections:
            return None
        
        for document in self.collections[collection]:
            if document.doc_id == doc_id:
                return document
        
        return None
    
    def find_documents(self, collection: str, 
                      filter_criteria: Optional[Dict[str, Any]] = None,
                      limit: Optional[int] = None) -> List[Document]:
        """查找多个文档"""
        if collection not in self.collections:
            return []
        
        documents = self.collections[collection].copy()
        
        # 应用过滤条件
        if filter_criteria:
            documents = self._apply_filter(documents, filter_criteria)
        
        # 排序（简化：按创建时间降序）
        documents.sort(key=lambda x: x.created_at, reverse=True)
        
        # 限制结果数量
        if limit:
            documents = documents[:limit]
        
        return documents
    
    def search_documents(self, collection: str, 
                        search_terms: List[str],
                        limit: Optional[int] = None) -> List[Tuple[Document, float]]:
        """搜索文档（基于全文搜索）"""
        if collection not in self.collections:
            return []
        
        # 获取集合对应的文本索引
        text_index = self.indexes.get((collection, "_text"))
        if not text_index:
            print(f"No text index found for collection '{collection}'")
            return []
        
        # 使用倒排索引搜索
        matched_doc_ids = text_index.search(search_terms)
        
        # 获取匹配的文档及其得分
        results = []
        for doc_id in matched_doc_ids:
            document = self.find_document(collection, doc_id)
            if document:
                # 计算搜索得分
                score = self._calculate_search_score(document, search_terms)
                results.append((document, score))
        
        # 按得分排序
        results.sort(key=lambda x: x[1], reverse=True)
        
        # 限制结果数量
        if limit:
            results = results[:limit]
        
        return results
    
    def _generate_document_id(self, document: Document) -> str:
        """生成文档ID"""
        content = json.dumps(document.data, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()
    
    def _update_indexes(self, collection: str, document: Document):
        """更新索引"""
        schema = self.schemas[collection]
        
        # 为索引字段创建索引
        for field_schema in schema.fields:
            if field_schema.is_indexed:
                field_name = field_schema.field_name
                
                # 文本字段的倒排索引
                if field_schema.field_type in ["string", "array"]:
                    if field_name not in self.indexes:
                        self.indexes[(collection, field_name)] = InvertedIndex()
                    
                    # 分词（简化处理）
                    terms = self._tokenize_field(document.data.get(field_name, ""))
                    self.indexes[(collection, field_name)].add_document(document.doc_id, terms)
                
                # 全文搜索索引
                if field_name == "_all":
                    all_text = self._extract_all_text(document.data)
                    if "_text" not in self.indexes:
                        self.indexes[(collection, "_text")] = InvertedIndex()
                    terms = self._tokenize_field(all_text)
                    self.indexes[(collection, "_text")].add_document(document.doc_id, terms)
        
        self.statistics["total_indexes"] = len(self.indexes)
    
    def _remove_from_indexes(self, collection: str, data: Dict[str, Any], doc_id: str):
        """从索引中移除"""
        for (col, field), index in self.indexes.items():
            if col == collection:
                index.remove_document(doc_id)
    
    def _indexes_affected(self, collection: str, old_data: Dict[str, Any], updates: Dict[str, Any]) -> bool:
        """检查索引是否受影响"""
        schema = self.schemas[collection]
        indexed_fields = [f.field_name for f in schema.fields if f.is_indexed]
        
        return any(field in indexed_fields for field in updates.keys())
    
    def _apply_filter(self, documents: List[Document], criteria: Dict[str, Any]) -> List[Document]:
        """应用过滤条件"""
        filtered = []
        
        for document in documents:
            if self._matches_criteria(document.data, criteria):
                filtered.append(document)
        
        return filtered
    
    def _matches_criteria(self, data: Dict[str, Any], criteria: Dict[str, Any]) -> bool:
        """检查是否匹配过滤条件"""
        for field, condition in criteria.items():
            if field not in data:
                return False
            
            value = data[field]
            
            if isinstance(condition, dict):
                # 复杂条件
                if "eq" in condition and value != condition["eq"]:
                    return False
                if "ne" in condition and value == condition["ne"]:
                    return False
                if "gt" in condition and not (value > condition["gt"]):
                    return False
                if "gte" in condition and not (value >= condition["gte"]):
                    return False
                if "lt" in condition and not (value < condition["lt"]):
                    return False
                if "lte" in condition and not (value <= condition["lte"]):
                    return False
                if "in" in condition and value not in condition["in"]:
                    return False
                if "regex" in condition and not re.search(condition["regex"], str(value)):
                    return False
            else:
                # 简单相等条件
                if value != condition:
                    return False
        
        return True
    
    def _tokenize_field(self, value: Any) -> List[str]:
        """字段分词"""
        if isinstance(value, str):
            # 移除标点符号并转换为小写
            text = re.sub(r'[^\w\s]', ' ', value.lower())
            return text.split()
        elif isinstance(value, list):
            # 列表中的每个元素分词
            terms = []
            for item in value:
                if isinstance(item, str):
                    terms.extend(self._tokenize_field(item))
            return terms
        else:
            # 其他类型转换为字符串
            return self._tokenize_field(str(value))
    
    def _extract_all_text(self, data: Dict[str, Any]) -> str:
        """提取所有文本内容"""
        text_parts = []
        
        for key, value in data.items():
            if isinstance(value, str):
                text_parts.append(value)
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, str):
                        text_parts.append(item)
            elif isinstance(value, dict):
                text_parts.append(self._extract_all_text(value))
            else:
                text_parts.append(str(value))
        
        return ' '.join(text_parts)
    
    def _calculate_search_score(self, document: Document, search_terms: List[str]) -> float:
        """计算搜索得分"""
        # 简化的TF-IDF得分计算
        score = 0.0
        all_text = self._extract_all_text(document.data)
        
        for term in search_terms:
            # 计算词频
            term_count = all_text.lower().count(term.lower())
            if term_count > 0:
                score += term_count
        
        return score
    
    def get_collection_stats(self, collection: str) -> Dict[str, Any]:
        """获取集合统计信息"""
        if collection not in self.collections:
            return {}
        
        documents = self.collections[collection]
        
        # 计算字段统计
        field_stats = defaultdict(int)
        for document in documents:
            for field in document.data.keys():
                field_stats[field] += 1
        
        return {
            "collection_name": collection,
            "document_count": len(documents),
            "field_statistics": dict(field_stats),
            "indexes": [
                {
                    "field": field,
                    "type": "inverted_index",
                    "terms": len(self.indexes.get((collection, field), InvertedIndex()).index)
                }
                for (col, field) in self.indexes.keys()
                if col == collection
            ],
            "last_modified": max(doc.created_at for doc in documents) if documents else None
        }

# 使用示例
async def demo_document_database():
    """演示文档数据库"""
    
    print("=== Document Database Demo ===\n")
    
    # 创建数据库实例
    db = DocumentDatabase()
    
    # 创建用户集合模式
    user_schema = DocumentSchema(
        schema_id="user_schema_001",
        collection_name="users",
        fields=[
            FieldSchema("name", "string", is_indexed=True, is_required=True, validation_rules=["min_length:2"]),
            FieldSchema("email", "string", is_indexed=True, is_required=True, validation_rules=["pattern:^[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}$"]),
            FieldSchema("age", "number", validation_rules=["min_value:0", "max_value:150"]),
            FieldSchema("skills", "array", is_indexed=True, validation_rules=["min_length:1"]),
            FieldSchema("bio", "string", is_indexed=True),
            FieldSchema("_all", "string", is_indexed=True)  # 全文搜索字段
        ]
    )
    
    db.create_collection(user_schema)
    
    # 创建产品集合模式
    product_schema = DocumentSchema(
        schema_id="product_schema_001",
        collection_name="products",
        fields=[
            FieldSchema("name", "string", is_indexed=True, is_required=True),
            FieldSchema("description", "string", is_indexed=True),
            FieldSchema("price", "number", is_required=True, validation_rules=["min_value:0"]),
            FieldSchema("category", "string", is_indexed=True),
            FieldSchema("tags", "array", is_indexed=True),
            FieldSchema("specifications", "object"),
            FieldSchema("_all", "string", is_indexed=True)
        ]
    )
    
    db.create_collection(product_schema)
    
    # 插入用户文档
    print("\n1. Inserting User Documents")
    users_data = [
        {
            "name": "张三",
            "email": "zhangsan@example.com",
            "age": 28,
            "skills": ["Python", "JavaScript", "机器学习", "数据分析"],
            "bio": "资深软件工程师，专注于人工智能和大数据技术开发"
        },
        {
            "name": "李四",
            "email": "lisi@example.com",
            "age": 32,
            "skills": ["Java", "Spring", "微服务", "Docker"],
            "bio": "全栈开发工程师，在互联网公司工作5年，擅长后端架构设计"
        },
        {
            "name": "王五",
            "email": "wangwu@example.com",
            "age": 25,
            "skills": ["React", "Vue.js", "前端开发", "UI设计"],
            "bio": "前端工程师，热爱创造优雅的用户界面和良好的用户体验"
        }
    ]
    
    for user_data in users_data:
        user_doc = Document(
            doc_id="",  # 自动生成
            collection="users",
            data=user_data
        )
        
        success = db.insert_document("users", user_doc)
        if success:
            print(f"✓ Inserted user: {user_data['name']} ({user_doc.doc_id[:8]}...)")
        else:
            print(f"✗ Failed to insert user: {user_data['name']}")
    
    # 插入产品文档
    print("\n2. Inserting Product Documents")
    products_data = [
        {
            "name": "智能手表 X1",
            "description": "新一代智能手表，具备心率监测、运动追踪、长续航等功能",
            "price": 2999.0,
            "category": "电子产品",
            "tags": ["智能穿戴", "健康", "运动", "科技"]
        },
        {
            "name": "无线耳机 Pro",
            "description": "主动降噪无线耳机，高品质音质，30小时续航",
            "price": 1299.0,
            "category": "音频设备",
            "tags": ["音频", "降噪", "无线", "便携"]
        },
        {
            "name": "游戏键盘 RGB",
            "description": "机械游戏键盘，RGB背光，防水设计，专为游戏而生",
            "price": 899.0,
            "category": "电脑配件",
            "tags": ["游戏", "键盘", "RGB", "机械"]
        }
    ]
    
    for product_data in products_data:
        product_doc = Document(
            doc_id="",
            collection="products",
            data=product_data
        )
        
        success = db.insert_document("products", product_doc)
        if success:
            print(f"✓ Inserted product: {product_data['name']} ({product_doc.doc_id[:8]}...)")
        else:
            print(f"✗ Failed to insert product: {product_data['name']}")
    
    # 查询操作
    print("\n3. Query Operations")
    
    # 查找所有用户
    all_users = db.find_documents("users", limit=10)
    print(f"Total users: {len(all_users)}")
    for user in all_users:
        print(f"  - {user.data['name']} ({user.data['email']})")
    
    # 条件查询
    print("\n4. Conditional Queries")
    
    # 查找年龄大于30的用户
    filter_criteria = {"age": {"gt": 30}}
    filtered_users = db.find_documents("users", filter_criteria=filter_criteria)
    print(f"Users older than 30: {len(filtered_users)}")
    for user in filtered_users:
        print(f"  - {user.data['name']}, {user.data['age']}岁")
    
    # 全文搜索
    print("\n5. Full-Text Search")
    
    search_terms = ["工程师", "人工智能", "后端"]
    search_results = db.search_documents("users", search_terms, limit=5)
    
    print(f"Search results for {search_terms}:")
    for document, score in search_results:
        print(f"  - {document.data['name']} (score: {score:.2f})")
        print(f"    bio: {document.data['bio']}")
    
    # 更新文档
    print("\n6. Document Updates")
    
    if all_users:
        target_user = all_users[0]
        print(f"Updating user: {target_user.data['name']}")
        
        updates = {
            "age": 29,
            "bio": "资深软件工程师，专注于人工智能和大数据技术开发，在业界有丰富经验"
        }
        
        success = db.update_document("users", target_user.doc_id, updates)
        if success:
            print("✓ User updated successfully")
            # 验证更新
            updated_user = db.find_document("users", target_user.doc_id)
            print(f"  New age: {updated_user.data['age']}")
            print(f"  New bio: {updated_user.data['bio']}")
        else:
            print("✗ Failed to update user")
    
    # 集合统计信息
    print("\n7. Collection Statistics")
    
    users_stats = db.get_collection_stats("users")
    print("Users collection stats:")
    for key, value in users_stats.items():
        print(f"  {key}: {value}")
    
    products_stats = db.get_collection_stats("products")
    print("\nProducts collection stats:")
    for key, value in products_stats.items():
        print(f"  {key}: {value}")
    
    # 数据库统计
    print("\n8. Database Statistics")
    print(f"Total documents: {db.statistics['total_documents']}")
    print(f"Total collections: {db.statistics['total_collections']}")
    print(f"Total indexes: {db.statistics['total_indexes']}")
```

## 2. 搜索引擎架构设计

### 2.1 全文搜索引擎核心组件

```python
import math
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import re
import nltk
from collections import defaultdict
import json

class SearchEngineType(Enum):
    """搜索引擎类型"""
    INVERTED_INDEX = "inverted_index"
    VECTOR_SEARCH = "vector_search"
    ELASTICSEARCH = "elasticsearch_simulation"

@dataclass
class SearchQuery:
    """搜索查询"""
    query_text: str
    filters: Dict[str, Any] = field(default_factory=dict)
    sort_criteria: List[str] = field(default_factory=list)
    limit: int = 10
    offset: int = 0
    boost_fields: Dict[str, float] = field(default_factory=dict)

@dataclass
class SearchResult:
    """搜索结果"""
    document_id: str
    title: str
    content: str
    score: float
    highlights: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Token:
    """词项"""
    term: str
    position: int
    frequency: int = 1
    tf_weight: float = 1.0

class TextProcessor:
    """文本处理器"""
    
    def __init__(self):
        self.stopwords = {
            "的", "了", "在", "是", "我", "有", "和", "就", "不", "人", "都", "一", "一个",
            "the", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"
        }
        self.stemmer = None  # 简化实现
        self.tokenizer = re.compile(r'\b\w+\b')
    
    def tokenize(self, text: str) -> List[str]:
        """分词"""
        # 转换为小写
        text = text.lower()
        
        # 提取词项
        tokens = self.tokenizer.findall(text)
        
        # 移除停用词
        tokens = [token for token in tokens if token not in self.stopwords and len(token) > 1]
        
        return tokens
    
    def calculate_tf(self, tokens: List[str]) -> Dict[str, int]:
        """计算词频"""
        tf = defaultdict(int)
        for token in tokens:
            tf[token] += 1
        return dict(tf)
    
    def calculate_tf_idf(self, tokens: List[str], doc_freq: Dict[str, int], total_docs: int) -> Dict[str, float]:
        """计算TF-IDF权重"""
        tf = self.calculate_tf(tokens)
        tf_idf = {}
        
        for term, tf_value in tf.items():
            idf = math.log(total_docs / (doc_freq.get(term, 1) + 1))
            tf_idf[term] = tf_value * idf
        
        return tf_idf

class AdvancedInvertedIndex:
    """高级倒排索引"""
    
    def __init__(self):
        self.inverted_index = defaultdict(lambda: defaultdict(int))  # term -> doc_id -> frequency
        self.document_freq = defaultdict(int)  # term -> number of documents containing term
        self.document_vectors = {}  # doc_id -> {term: tf_idf_weight}
        self.document_norms = {}  # doc_id -> vector norm
        self.total_documents = 0
        self.text_processor = TextProcessor()
    
    def index_document(self, doc_id: str, content: str, metadata: Optional[Dict[str, Any]] = None):
        """索引文档"""
        tokens = self.text_processor.tokenize(content)
        self.total_documents += 1
        
        # 计算TF-IDF
        tf_idf_weights = self.text_processor.calculate_tf_idf(tokens, self.document_freq, self.total_documents)
        
        # 更新倒排索引
        for term, frequency in self.text_processor.calculate_tf(tokens).items():
            self.inverted_index[term][doc_id] = frequency
            self.document_freq[term] += 1
        
        # 保存文档向量
        self.document_vectors[doc_id] = tf_idf_weights
        
        # 计算向量范数（用于余弦相似度）
        self.document_norms[doc_id] = math.sqrt(sum(weight ** 2 for weight in tf_idf_weights.values()))
        
        print(f"Indexed document {doc_id} with {len(tokens)} tokens")
    
    def remove_document(self, doc_id: str):
        """移除文档"""
        # 从倒排索引中移除
        for term in self.inverted_index:
            if doc_id in self.inverted_index[term]:
                del self.inverted_index[term][doc_id]
                if not self.inverted_index[term]:  # 如果词项没有文档了
                    del self.inverted_index[term]
        
        # 清理文档向量和范数
        if doc_id in self.document_vectors:
            del self.document_vectors[doc_id]
        if doc_id in self.document_norms:
            del self.document_norms[doc_id]
        
        # 重新计算文档频率
        self.document_freq = {
            term: len(posting_list) 
            for term, posting_list in self.inverted_index.items()
        }
        
        self.total_documents -= 1
    
    def search(self, query_text: str, limit: int = 10) -> List[Tuple[str, float]]:
        """搜索文档"""
        query_tokens = self.text_processor.tokenize(query_text)
        if not query_tokens:
            return []
        
        # 计算查询向量
        query_tf = self.text_processor.calculate_tf(query_tokens)
        query_tf_idf = self.text_processor.calculate_tf_idf(query_tokens, self.document_freq, self.total_documents)
        query_norm = math.sqrt(sum(weight ** 2 for weight in query_tf_idf.values()))
        
        if query_norm == 0:
            return []
        
        # 计算余弦相似度
        scores = {}
        
        # 获取候选文档
        candidate_docs = set()
        for token in query_tokens:
            candidate_docs.update(self.inverted_index[token].keys())
        
        # 计算相似度
        for doc_id in candidate_docs:
            if doc_id not in self.document_vectors:
                continue
            
            # 计算点积
            dot_product = 0
            for term, weight in query_tf_idf.items():
                if term in self.document_vectors[doc_id]:
                    dot_product += weight * self.document_vectors[doc_id][term]
            
            # 计算余弦相似度
            if self.document_norms[doc_id] > 0 and query_norm > 0:
                cosine_similarity = dot_product / (self.document_norms[doc_id] * query_norm)
                scores[doc_id] = cosine_similarity
        
        # 按得分排序
        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_results[:limit]
    
    def suggest_terms(self, prefix: str, limit: int = 10) -> List[str]:
        """自动补全"""
        suggestions = []
        
        for term in self.inverted_index:
            if term.startswith(prefix) and len(suggestions) < limit:
                suggestions.append((term, self.document_freq[term]))
        
        # 按文档频率排序
        suggestions.sort(key=lambda x: x[1], reverse=True)
        
        return [term for term, freq in suggestions]
    
    def get_term_stats(self, term: str) -> Dict[str, Any]:
        """获取词项统计"""
        if term not in self.inverted_index:
            return {}
        
        frequencies = list(self.inverted_index[term].values())
        
        return {
            "term": term,
            "document_frequency": self.document_freq[term],
            "total_frequency": sum(frequencies),
            "average_frequency": sum(frequencies) / len(frequencies),
            "max_frequency": max(frequencies),
            "min_frequency": min(frequencies)
        }

class SearchEngine:
    """搜索引擎核心"""
    
    def __init__(self, engine_type: SearchEngineType = SearchEngineType.INVERTED_INDEX):
        self.engine_type = engine_type
        self.index = AdvancedInvertedIndex()
        self.documents = {}  # doc_id -> document data
        self.search_history = []
        self.analytics = {
            "total_searches": 0,
            "popular_queries": defaultdict(int),
            "zero_result_queries": 0,
            "average_results_per_query": 0
        }
    
    def index_document(self, doc_id: str, title: str, content: str, 
                      metadata: Optional[Dict[str, Any]] = None) -> bool:
        """索引文档"""
        try:
            # 组合标题和内容用于搜索
            full_content = f"{title} {content}"
            
            # 索引文档
            self.index.index_document(doc_id, full_content, metadata)
            
            # 保存文档数据
            self.documents[doc_id] = {
                "title": title,
                "content": content,
                "metadata": metadata or {},
                "indexed_at": datetime.utcnow().isoformat()
            }
            
            return True
            
        except Exception as e:
            print(f"Error indexing document {doc_id}: {e}")
            return False
    
    def search(self, query: SearchQuery) -> List[SearchResult]:
        """执行搜索"""
        try:
            # 记录搜索统计
            self.analytics["total_searches"] += 1
            self.analytics["popular_queries"][query.query_text] += 1
            
            # 执行搜索
            doc_scores = self.index.search(query.query_text, limit=query.limit + query.offset)
            
            # 处理结果
            results = []
            start_idx = query.offset
            end_idx = start_idx + query.limit
            top_results = doc_scores[start_idx:end_idx]
            
            for doc_id, score in top_results:
                if doc_id in self.documents:
                    doc_data = self.documents[doc_id]
                    
                    # 生成高亮
                    highlights = self._generate_highlights(doc_data["content"], query.query_text)
                    
                    # 应用字段权重
                    boosted_score = self._apply_field_boosts(score, doc_data, query.boost_fields)
                    
                    result = SearchResult(
                        document_id=doc_id,
                        title=doc_data["title"],
                        content=doc_data["content"][:200] + "..." if len(doc_data["content"]) > 200 else doc_data["content"],
                        score=boosted_score,
                        highlights=highlights,
                        metadata=doc_data["metadata"]
                    )
                    
                    results.append(result)
            
            # 记录零结果搜索
            if len(results) == 0:
                self.analytics["zero_result_queries"] += 1
            
            # 更新平均结果数
            total_results = sum(len(r) for r in self.search_history) + len(results)
            self.search_history.append(results)
            
            return results
            
        except Exception as e:
            print(f"Error during search: {e}")
            return []
    
    def _generate_highlights(self, content: str, query_text: str) -> List[str]:
        """生成搜索高亮"""
        query_tokens = self.index.text_processor.tokenize(query_text)
        highlights = []
        
        # 查找包含查询词的位置
        content_lower = content.lower()
        positions = []
        
        for token in query_tokens:
            pos = content_lower.find(token.lower())
            if pos >= 0:
                positions.append(pos)
        
        # 生成高亮片段
        for pos in positions:
            start = max(0, pos - 50)
            end = min(len(content), pos + 100)
            snippet = content[start:end]
            
            # 高亮查询词
            for token in query_tokens:
                pattern = re.compile(re.escape(token), re.IGNORECASE)
                snippet = pattern.sub(f"**{token.upper()}**", snippet)
            
            highlights.append(f"...{snippet}...")
        
        return highlights[:3]  # 最多3个高亮
    
    def _apply_field_boosts(self, base_score: float, doc_data: Dict[str, Any], 
                           boosts: Dict[str, float]) -> float:
        """应用字段权重"""
        boosted_score = base_score
        
        # 简单的标题权重
        if "title" in boosts:
            title_match_bonus = 1.0
            for boost_field, boost_value in boosts.items():
                if boost_field == "title" and boost_value > 1:
                    title_match_bonus = boost_value
            
            boosted_score *= title_match_bonus
        
        return boosted_score
    
    def suggest_completions(self, prefix: str, limit: int = 5) -> List[str]:
        """自动补全"""
        return self.index.suggest_terms(prefix, limit)
    
    def get_search_analytics(self) -> Dict[str, Any]:
        """获取搜索分析"""
        return {
            "total_searches": self.analytics["total_searches"],
            "popular_queries": dict(self.analytics["popular_queries"]),
            "zero_result_rate": (
                self.analytics["zero_result_queries"] / 
                max(self.analytics["total_searches"], 1) * 100
            ),
            "index_statistics": {
                "total_documents": len(self.documents),
                "total_terms": len(self.index.inverted_index),
                "total_index_entries": sum(
                    len(postings) for postings in self.index.inverted_index.values()
                )
            }
        }
    
    def bulk_index(self, documents: List[Dict[str, str]]) -> Dict[str, int]:
        """批量索引"""
        success_count = 0
        error_count = 0
        
        for doc in documents:
            try:
                doc_id = doc.get("id", f"doc_{len(self.documents)}")
                title = doc.get("title", "")
                content = doc.get("content", "")
                metadata = doc.get("metadata", {})
                
                if self.index_document(doc_id, title, content, metadata):
                    success_count += 1
                else:
                    error_count += 1
                    
            except Exception as e:
                print(f"Error indexing document: {e}")
                error_count += 1
        
        return {"success": success_count, "errors": error_count}

# 使用示例
async def demo_search_engine():
    """演示搜索引擎"""
    
    print("=== Search Engine Demo ===\n")
    
    # 创建搜索引擎
    search_engine = SearchEngine()
    
    # 准备测试数据
    documents = [
        {
            "id": "doc_001",
            "title": "Python机器学习入门指南",
            "content": """Python是当前最受欢迎的机器学习编程语言之一。
            本文介绍了机器学习的基本概念，包括监督学习、无监督学习和强化学习。
            我们将学习如何使用scikit-learn、TensorFlow等库进行机器学习项目开发。
            机器学习算法包括线性回归、决策树、支持向量机、神经网络等。
            深度学习是机器学习的一个重要分支，特别适合处理图像和自然语言数据。""",
            "metadata": {"category": "技术教程", "tags": ["Python", "机器学习", "AI"], "author": "张三"}
        },
        {
            "id": "doc_002", 
            "title": "Java微服务架构设计模式",
            "content": """微服务架构是现代企业级应用的主流架构模式。
            本文详细介绍了微服务的拆分原则、服务治理、API网关、配置中心等核心概念。
            Spring Cloud是Java生态系统中微服务开发的优秀框架。
            服务注册与发现、负载均衡、熔断器、分布式追踪等是微服务架构的重要组件。
            Docker容器化技术帮助微服务实现更好的部署和扩展。""",
            "metadata": {"category": "架构设计", "tags": ["Java", "微服务", "Spring Cloud"], "author": "李四"}
        },
        {
            "id": "doc_003",
            "title": "React前端开发最佳实践",
            "content": """React是Facebook开发的前端UI库，广泛用于构建单页应用。
            本文介绍了React的核心概念，包括组件、状态管理、生命周期等。
            Redux是React应用状态管理的重要工具。
            TypeScript可以提高React代码的类型安全性和可维护性。
            React Hooks是React 16.8引入的新特性，使函数组件也能使用状态。""",
            "metadata": {"category": "前端开发", "tags": ["React", "JavaScript", "前端"], "author": "王五"}
        },
        {
            "id": "doc_004",
            "title": "数据库性能优化策略",
            "content": """数据库性能优化是系统架构设计的重要环节。
            本文介绍了索引优化、查询优化、连接池配置等数据库优化技术。
            MySQL和PostgreSQL是两种主流的关系型数据库。
            Redis作为内存数据库，常用于缓存和会话管理。
            分库分表可以解决大数据量下的性能问题。
            读写分离是提升数据库并发性能的有效方法。""",
            "metadata": {"category": "数据库", "tags": ["MySQL", "Redis", "性能优化"], "author": "张三"}
        },
        {
            "id": "doc_005",
            "title": "人工智能在自然语言处理中的应用",
            "content": """自然语言处理（NLP）是人工智能的重要应用领域。
            深度学习在NLP任务中取得了显著成果，如BERT、GPT等预训练模型。
            文本分类、情感分析、机器翻译、问答系统是NLP的主要应用。
            词向量表示、注意力机制、Transformer架构是现代NLP的核心技术。
            大语言模型正在重塑人工智能的发展方向。""",
            "metadata": {"category": "人工智能", "tags": ["AI", "NLP", "深度学习"], "author": "李四"}
        }
    ]
    
    # 批量索引文档
    print("1. Indexing Documents")
    index_result = search_engine.bulk_index(documents)
    print(f"Indexed {index_result['success']} documents successfully, {index_result['errors']} errors")
    
    # 执行搜索测试
    print("\n2. Search Tests")
    
    search_queries = [
        "Python机器学习",
        "微服务架构",
        "React前端开发", 
        "数据库性能",
        "人工智能NLP",
        "Java开发",
        "深度学习算法"
    ]
    
    for query_text in search_queries:
        print(f"\nSearching for: '{query_text}'")
        
        query = SearchQuery(
            query_text=query_text,
            limit=3,
            boost_fields={"title": 2.0}  # 标题权重2倍
        )
        
        results = search_engine.search(query)
        
        print(f"Found {len(results)} results:")
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result.title} (Score: {result.score:.3f})")
            if result.highlights:
                print(f"     Highlight: {result.highlights[0]}")
    
    # 测试自动补全
    print("\n3. Auto-completion Tests")
    
    completion_prefixes = ["Pyth", "微服务", "React", "数据"]
    
    for prefix in completion_prefixes:
        suggestions = search_engine.suggest_completions(prefix, limit=3)
        print(f"Completions for '{prefix}': {suggestions}")
    
    # 显示搜索分析
    print("\n4. Search Analytics")
    analytics = search_engine.get_search_analytics()
    
    print(f"Total searches performed: {analytics['total_searches']}")
    print(f"Zero result rate: {analytics['zero_result_rate']:.1f}%")
    
    print("\nIndex Statistics:")
    for key, value in analytics['index_statistics'].items():
        print(f"  {key}: {value}")
    
    print("\nPopular Queries:")
    sorted_queries = sorted(
        analytics['popular_queries'].items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    for query, count in sorted_queries[:5]:
        print(f"  '{query}': {count} times")

# 组合文档数据库和搜索引擎的示例
async def demo_integrated_system():
    """演示文档数据库与搜索引擎集成系统"""
    
    print("=== Document Database + Search Engine Integration ===\n")
    
    # 创建组件
    doc_db = DocumentDatabase()
    search_engine = SearchEngine()
    
    # 创建集合模式
    blog_schema = DocumentSchema(
        schema_id="blog_schema",
        collection_name="blog_posts",
        fields=[
            FieldSchema("title", "string", is_indexed=True, is_required=True),
            FieldSchema("content", "string", is_indexed=True, is_required=True),
            FieldSchema("author", "string", is_indexed=True),
            FieldSchema("tags", "array", is_indexed=True),
            FieldSchema("category", "string", is_indexed=True),
            FieldSchema("publish_date", "date"),
            FieldSchema("views", "number"),
            FieldSchema("likes", "number"),
            FieldSchema("_all", "string", is_indexed=True)
        ]
    )
    
    doc_db.create_collection(blog_schema)
    
    # 博客文章数据
    blog_posts = [
        {
            "title": "深入理解Python装饰器",
            "content": """装饰器是Python中强大的语法糖，它允许我们修改或增强函数或类的行为。
            装饰器广泛应用于日志记录、性能测量、访问控制、缓存等功能。
            函数装饰器是最常用的装饰器类型，它接受一个函数并返回一个新函数。
            类装饰器可以用于修改类的行为，而wraps装饰器用于保持被装饰函数的元数据。""",
            "author": "Python专家",
            "tags": ["Python", "装饰器", "编程技巧"],
            "category": "编程教程",
            "publish_date": "2023-10-15",
            "views": 1250,
            "likes": 89
        },
        {
            "title": "React状态管理最佳实践",
            "content": """React状态管理是前端开发中的重要话题。
            组件内部状态可以使用useState hook，管理简单本地状态。
            全局状态可以使用Context API或Redux进行管理。
            对于复杂应用，建议使用Redux Toolkit简化样板代码。
            状态管理模式的选择取决于应用规模和复杂度。""",
            "author": "前端架构师",
            "tags": ["React", "状态管理", "前端开发"],
            "category": "前端技术",
            "publish_date": "2023-10-12",
            "views": 2100,
            "likes": 156
        },
        {
            "title": "微服务架构设计模式详解",
            "content": """微服务架构将单体应用拆分为多个小型服务，每个服务负责特定的业务功能。
            服务拆分需要遵循单一职责原则，考虑数据一致性和团队协作。
            API网关作为微服务的统一入口，负责路由、认证、限流等功能。
            服务发现、配置管理、分布式追踪是微服务架构的基础设施。""",
            "author": "架构专家",
            "tags": ["微服务", "架构设计", "分布式系统"],
            "category": "架构设计",
            "publish_date": "2023-10-10",
            "views": 1800,
            "likes": 234
        }
    ]
    
    print("1. Inserting Blog Posts")
    for post_data in blog_posts:
        # 插入到文档数据库
        post_doc = Document(doc_id="", collection="blog_posts", data=post_data)
        success = doc_db.insert_document("blog_posts", post_doc)
        
        if success:
            # 同时索引到搜索引擎
            metadata = {
                "category": post_data["category"],
                "tags": post_data["tags"],
                "author": post_data["author"],
                "views": post_data["views"],
                "likes": post_data["likes"]
            }
            
            search_engine.index_document(
                post_doc.doc_id, 
                post_data["title"], 
                post_data["content"], 
                metadata
            )
            
            print(f"✓ Indexed: {post_data['title']}")
    
    print("\n2. Search Operations")
    
    # 全文搜索
    search_terms = ["Python", "装饰器"]
    results = search_engine.search(SearchQuery(
        query_text=" ".join(search_terms),
        limit=5,
        boost_fields={"title": 2.0}
    ))
    
    print(f"Search results for {search_terms}:")
    for result in results:
        print(f"  - {result.title} (Score: {result.score:.3f})")
        if result.highlights:
            print(f"    {result.highlights[0]}")
    
    print("\n3. Database Queries")
    
    # 按分类查询
    category_filter = {"category": "编程教程"}
    tutorials = doc_db.find_documents("blog_posts", filter_criteria=category_filter)
    print(f"\nProgramming tutorials: {len(tutorials)} articles")
    for post in tutorials:
        print(f"  - {post.data['title']} by {post.data['author']}")
    
    # 按标签查询
    tag_filter = {"tags": {"in": ["Python"]}}
    python_articles = doc_db.find_documents("blog_posts", filter_criteria=tag_filter)
    print(f"\nPython-related articles: {len(python_articles)}")
    for post in python_articles:
        print(f"  - {post.data['title']}")
    
    # 统计查询
    print("\n4. Statistics")
    
    # 文档数据库统计
    blog_stats = doc_db.get_collection_stats("blog_posts")
    print("Blog database statistics:")
    for key, value in blog_stats.items():
        print(f"  {key}: {value}")
    
    # 搜索引擎统计
    search_stats = search_engine.get_search_analytics()
    print(f"\nSearch engine statistics:")
    print(f"  Total indexed documents: {search_stats['index_statistics']['total_documents']}")
    print(f"  Total search terms: {search_stats['index_statistics']['total_terms']}")
    
    print("\n5. Advanced Search Operations")
    
    # 复杂搜索查询
    complex_queries = [
        "React状态管理",
        "微服务架构设计",
        "Python编程技巧"
    ]
    
    for query_text in complex_queries:
        print(f"\nAdvanced search: '{query_text}'")
        
        results = search_engine.search(SearchQuery(
            query_text=query_text,
            limit=3
        ))
        
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result.title}")
            print(f"     Score: {result.score:.3f}, Views: {result.metadata.get('views', 0)}")
            if result.metadata.get('tags'):
                print(f"     Tags: {', '.join(result.metadata['tags'])}")

# 运行演示
if __name__ == "__main__":
    print("Running Document Database and Search Engine Demonstrations...\n")
    
    # 运行各个演示
    asyncio.run(demo_document_database())
    print("\n" + "="*80 + "\n")
    
    asyncio.run(demo_search_engine())
    print("\n" + "="*80 + "\n")
    
    asyncio.run(demo_integrated_system())
```

## 3. 分布式搜索架构

### 3.1 分片搜索与索引分布式

```python
import hashlib
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import asyncio
import random

@dataclass
class ShardInfo:
    """分片信息"""
    shard_id: str
    primary_node: str
    replica_nodes: List[str]
    document_range: tuple  # (start_hash, end_hash)
    status: str = "active"  # active, inactive, recovering

class ShardManager:
    """分片管理器"""
    
    def __init__(self, total_shards: int = 5):
        self.total_shards = total_shards
        self.shards: Dict[str, ShardInfo] = {}
        self.search_engines = {}  # shard_id -> search_engine
        self.node_assignments = {}  # node_id -> list of shard_ids
        
    def create_shards(self, nodes: List[str]):
        """创建分片"""
        # 为每个节点分配分片
        for i in range(self.total_shards):
            # 简单的轮询分配
            primary_node = nodes[i % len(nodes)]
            replica_nodes = [nodes[(i + j + 1) % len(nodes)] for j in range(min(2, len(nodes) - 1))]
            
            shard_info = ShardInfo(
                shard_id=f"shard_{i:03d}",
                primary_node=primary_node,
                replica_nodes=replica_nodes,
                document_range=(i * (2**32 // self.total_shards), (i + 1) * (2**32 // self.total_shards))
            )
            
            self.shards[shard_info.shard_id] = shard_info
            
            # 更新节点分配映射
            if primary_node not in self.node_assignments:
                self.node_assignments[primary_node] = []
            self.node_assignments[primary_node].append(shard_info.shard_info)
            
            for replica_node in replica_nodes:
                if replica_node not in self.node_assignments:
                    self.node_assignments[replica_node] = []
                if shard_info.shard_id not in self.node_assignments[replica_node]:
                    self.node_assignments[replica_node].append(shard_info.shard_id)
            
            print(f"Created shard {shard_info.shard_id} on {primary_node} with replicas {replica_nodes}")
    
    def get_document_shard(self, doc_id: str) -> str:
        """获取文档应该存储在哪个分片"""
        doc_hash = int(hashlib.md5(doc_id.encode()).hexdigest(), 16)
        
        # 简单的哈希取模
        shard_index = doc_hash % self.total_shards
        return f"shard_{shard_index:03d}"
    
    def route_search(self, query: SearchQuery) -> List[SearchQuery]:
        """路由搜索查询到相关分片"""
        # 为每个活跃分片创建搜索查询
        shard_queries = []
        
        for shard_id, shard_info in self.shards.items():
            if shard_info.status == "active":
                # 为每个分片创建查询副本
                shard_query = SearchQuery(
                    query_text=query.query_text,
                    filters=query.filters,
                    sort_criteria=query.sort_criteria,
                    limit=max(1, query.limit // len(self.shards)),  # 均匀分配限制
                    boost_fields=query.boost_fields
                )
                shard_queries.append((shard_id, shard_query))
        
        return shard_queries
    
    def merge_search_results(self, shard_results: List[Tuple[str, List[SearchResult]]]) -> List[SearchResult]:
        """合并来自不同分片的搜索结果"""
        all_results = []
        
        # 收集所有分片的结果
        for shard_id, results in shard_results:
            for result in results:
                # 添加分片信息到元数据
                result.metadata["shard_id"] = shard_id
                all_results.append(result)
        
        # 去重（基于document_id）
        unique_results = {}
        for result in all_results:
            if result.document_id not in unique_results:
                unique_results[result.document_id] = result
            else:
                # 如果重复，保留得分较高的
                if result.score > unique_results[result.document_id].score:
                    unique_results[result.document_id] = result
        
        # 按得分排序
        sorted_results = sorted(unique_results.values(), key=lambda x: x.score, reverse=True)
        
        return sorted_results

class DistributedSearchEngine:
    """分布式搜索引擎"""
    
    def __init__(self, shard_manager: ShardManager):
        self.shard_manager = shard_manager
        self.search_engines = {}  # shard_id -> SearchEngine
        self.node_health = {}  # node_id -> bool
        
    def register_node(self, node_id: str, shard_ids: List[str]):
        """注册搜索节点"""
        # 为节点的分片创建搜索引擎实例
        for shard_id in shard_ids:
            if shard_id not in self.search_engines:
                self.search_engines[shard_id] = SearchEngine()
            print(f"Registered node {node_id} for shard {shard_id}")
    
    async def index_document(self, doc_id: str, title: str, content: str, 
                           metadata: Optional[Dict[str, Any]] = None) -> bool:
        """分布式索引文档"""
        # 确定文档应该存储在哪个分片
        target_shard = self.shard_manager.get_document_shard(doc_id)
        
        # 获取对应的搜索引擎
        search_engine = self.search_engines.get(target_shard)
        if not search_engine:
            print(f"No search engine available for shard {target_shard}")
            return False
        
        # 索引文档
        return search_engine.index_document(doc_id, title, content, metadata)
    
    async def distributed_search(self, query: SearchQuery) -> List[SearchResult]:
        """分布式搜索"""
        # 路由查询到相关分片
        shard_queries = self.shard_manager.route_search(query)
        
        # 并行执行搜索
        search_tasks = []
        for shard_id, shard_query in shard_queries:
            search_engine = self.search_engines.get(shard_id)
            if search_engine:
                task = self._search_shard(search_engine, shard_id, shard_query)
                search_tasks.append(task)
        
        # 等待所有搜索完成
        if search_tasks:
            shard_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        else:
            shard_results = []
        
        # 处理结果
        valid_results = []
        for result in shard_results:
            if isinstance(result, list):
                valid_results.extend(result)
            elif isinstance(result, Exception):
                print(f"Search error: {result}")
        
        # 合并和排序结果
        final_results = self._merge_and_rank_results(valid_results)
        
        return final_results
    
    async def _search_shard(self, search_engine: SearchEngine, shard_id: str, 
                          query: SearchQuery) -> List[SearchResult]:
        """在单个分片上执行搜索"""
        try:
            results = search_engine.search(query)
            # 添加分片信息
            for result in results:
                result.metadata["shard_id"] = shard_id
            return results
        except Exception as e:
            print(f"Error searching shard {shard_id}: {e}")
            return []
    
    def _merge_and_rank_results(self, all_results: List[SearchResult]) -> List[SearchResult]:
        """合并和排序结果"""
        # 去重
        unique_results = {}
        for result in all_results:
            if result.document_id not in unique_results:
                unique_results[result.document_id] = result
            else:
                # 合并相同文档的得分
                existing = unique_results[result.document_id]
                existing.score = max(existing.score, result.score)
                existing.metadata.update(result.metadata)
        
        # 按得分排序
        sorted_results = sorted(
            unique_results.values(), 
            key=lambda x: x.score, 
            reverse=True
        )
        
        return sorted_results

# 使用示例
async def demo_distributed_search():
    """演示分布式搜索"""
    
    print("=== Distributed Search Engine Demo ===\n")
    
    # 创建分片管理器
    shard_manager = ShardManager(total_shards=3)
    
    # 模拟集群节点
    cluster_nodes = ["node-1", "node-2", "node-3", "node-4", "node-5"]
    
    # 创建分片
    shard_manager.create_shards(cluster_nodes)
    
    # 创建分布式搜索引擎
    distributed_engine = DistributedSearchEngine(shard_manager)
    
    # 注册节点
    for shard_id, shard_info in shard_manager.shards.items():
        # 注册主节点
        distributed_engine.register_node(shard_info.primary_node, [shard_id])
        
        # 注册副本节点
        for replica_node in shard_info.replica_nodes:
            distributed_engine.register_node(replica_node, [shard_id])
    
    # 准备测试数据
    test_documents = [
        {"title": "Python入门教程", "content": "Python是一种解释型、面向对象、动态数据类型的高级程序设计语言。"},
        {"title": "JavaScript进阶指南", "content": "JavaScript是一种高级的、解释执行的编程语言，是Web开发的核心技术之一。"},
        {"title": "Java微服务架构", "content": "微服务架构是一种将单体应用开发为一组小型服务的方法。"},
        {"title": "React状态管理", "content": "React状态管理是React应用开发中的重要概念，包括Redux、Context API等。"},
        {"title": "数据库设计原理", "content": "数据库设计是软件系统设计中的重要环节，关系到系统的性能和可维护性。"},
        {"title": "分布式系统设计", "content": "分布式系统是由多台计算机组成的系统，通过网络进行通信和协调。"}
    ]
    
    print("\n1. Distributed Document Indexing")
    
    # 分布式索引文档
    for i, doc in enumerate(test_documents):
        doc_id = f"doc_{i+1:03d}"
        success = await distributed_engine.index_document(
            doc_id, 
            doc["title"], 
            doc["content"],
            {"author": "示例作者", "category": "技术文章"}
        )
        
        if success:
            target_shard = shard_manager.get_document_shard(doc_id)
            print(f"✓ Indexed document {doc_id} on shard {target_shard}")
    
    print("\n2. Distributed Search Operations")
    
    # 测试分布式搜索
    search_queries = ["Python", "微服务", "JavaScript", "分布式系统", "数据库"]
    
    for query_text in search_queries:
        print(f"\nSearching for: '{query_text}'")
        
        query = SearchQuery(
            query_text=query_text,
            limit=5,
            boost_fields={"title": 2.0}
        )
        
        results = await distributed_engine.distributed_search(query)
        
        print(f"Found {len(results)} results:")
        for i, result in enumerate(results[:3], 1):
            print(f"  {i}. {result.title} (Score: {result.score:.3f})")
            if result.metadata.get('shard_id'):
                print(f"     From shard: {result.metadata['shard_id']}")
    
    print("\n3. Shard Distribution Analysis")
    
    # 分析文档分布
    shard_distribution = defaultdict(int)
    
    for i, doc in enumerate(test_documents):
        doc_id = f"doc_{i+1:03d}"
        shard_id = shard_manager.get_document_shard(doc_id)
        shard_distribution[shard_id] += 1
    
    print("Document distribution across shards:")
    for shard_id, count in shard_distribution.items():
        print(f"  {shard_id}: {count} documents")
    
    print("\n4. Node Load Distribution")
    
    # 分析节点负载
    for node_id, assigned_shards in shard_manager.node_assignments.items():
        print(f"Node {node_id}: {len(assigned_shards)} shards")
        for shard_id in assigned_shards:
            if shard_id in shard_manager.shards:
                shard_info = shard_manager.shards[shard_id]
                role = "Primary" if shard_info.primary_node == node_id else "Replica"
                print(f"  - {shard_id} ({role})")

# 运行所有演示
if __name__ == "__main__":
    asyncio.run(demo_distributed_search())
```

## 4. 最佳实践与架构建议

### 4.1 文档数据库最佳实践

1. **数据模型设计**
   - 选择合适的文档结构，平衡嵌套和引用
   - 合理使用数组和嵌套对象
   - 考虑查询模式和性能要求

2. **索引策略**
   - 为高频查询字段创建索引
   - 避免过度索引影响写入性能
   - 使用复合索引优化多条件查询

3. **查询优化**
   - 使用投影减少数据传输量
   - 合理使用聚合管道
   - 避免全表扫描

### 4.2 搜索引擎最佳实践

1. **索引设计**
   - 根据搜索需求设计索引结构
   - 考虑字段权重和boosting策略
   - 定期重建和优化索引

2. **搜索优化**
   - 使用搜索结果缓存
   - 实现搜索建议和自动补全
   - 提供相关性调整机制

3. **性能考虑**
   - 实现分页和结果限制
   - 使用异步处理大量索引操作
   - 监控搜索性能指标

### 4.3 架构集成模式

```python
# 文档数据库与搜索引擎集成架构
class IntegratedDocumentSearchSystem:
    """集成文档搜索系统"""
    
    def __init__(self):
        self.document_db = DocumentDatabase()
        self.search_engine = SearchEngine()
        self.sync_manager = DocumentSyncManager()
        
    def create_collection_with_search(self, schema: DocumentSchema):
        """创建支持搜索的集合"""
        # 创建文档集合
        self.document_db.create_collection(schema)
        
        # 设置搜索引擎字段映射
        search_fields = {
            "title": {"boost": 2.0},
            "content": {"boost": 1.0},
            "tags": {"boost": 1.5}
        }
        
        # 初始化搜索索引
        print(f"Initialized search support for collection {schema.collection_name}")
    
    async def insert_and_index(self, collection: str, document: Document) -> bool:
        """插入文档并同步索引"""
        # 插入文档数据库
        success = self.document_db.insert_document(collection, document)
        
        if success:
            # 同步到搜索引擎
            await self._sync_to_search_engine(document)
        
        return success
    
    async def _sync_to_search_engine(self, document: Document):
        """同步到搜索引擎"""
        title = document.data.get("title", "")
        content = self._extract_searchable_content(document.data)
        metadata = document.metadata.copy()
        
        self.search_engine.index_document(
            document.doc_id, 
            title, 
            content, 
            metadata
        )
    
    def _extract_searchable_content(self, data: Dict[str, Any]) -> str:
        """提取可搜索内容"""
        searchable_fields = ["title", "content", "tags", "description"]
        content_parts = []
        
        for field in searchable_fields:
            if field in data:
                value = data[field]
                if isinstance(value, list):
                    content_parts.extend(value)
                else:
                    content_parts.append(str(value))
        
        return " ".join(content_parts)

# 性能监控和优化
class PerformanceMonitor:
    """性能监控器"""
    
    def __init__(self):
        self.metrics = {
            "query_times": [],
            "index_times": [],
            "memory_usage": [],
            "disk_usage": []
        }
    
    def record_query_time(self, duration: float):
        """记录查询时间"""
        self.metrics["query_times"].append(duration)
        if len(self.metrics["query_times"]) > 1000:
            self.metrics["query_times"] = self.metrics["query_times"][-1000:]
    
    def record_index_time(self, duration: float):
        """记录索引时间"""
        self.metrics["index_times"].append(duration)
        if len(self.metrics["index_times"]) > 1000:
            self.metrics["index_times"] = self.metrics["index_times"][-1000:]
    
    def get_performance_report(self) -> Dict[str, Any]:
        """获取性能报告"""
        return {
            "average_query_time": sum(self.metrics["query_times"]) / max(len(self.metrics["query_times"]), 1),
            "average_index_time": sum(self.metrics["index_times"]) / max(len(self.metrics["index_times"]), 1),
            "total_queries": len(self.metrics["query_times"]),
            "total_indexes": len(self.metrics["index_times"]),
            "performance_trends": self._calculate_trends()
        }
    
    def _calculate_trends(self) -> Dict[str, str]:
        """计算性能趋势"""
        trends = {}
        
        if len(self.metrics["query_times"]) > 10:
            recent_avg = sum(self.metrics["query_times"][-10:]) / 10
            earlier_avg = sum(self.metrics["query_times"][-20:-10]) / 10 if len(self.metrics["query_times"]) > 20 else recent_avg
            
            if recent_avg > earlier_avg * 1.1:
                trends["query_time"] = "increasing"
            elif recent_avg < earlier_avg * 0.9:
                trends["query_time"] = "decreasing"
            else:
                trends["query_time"] = "stable"
        
        return trends
```

## 5. 总结

文档数据库与搜索引擎作为现代应用架构的两大支柱，各自发挥着重要作用：

**文档数据库优势：**
- 灵活的数据模型，适应复杂业务场景
- 强大的查询和聚合能力
- 良好的扩展性和性能

**搜索引擎特点：**
- 专业的全文检索能力
- 复杂的搜索功能和相关性排序
- 高性能的分布式搜索架构

**集成策略：**
- 实时同步保证数据一致性
- 合理的分工优化系统性能
- 完善的监控和维护体系

**选择指南：**
- 文档数据库适合结构化数据存储和复杂查询
- 搜索引擎适合全文检索和复杂搜索需求
- 集成系统提供最完整的解决方案

通过合理的架构设计和最佳实践，可以构建出高性能、高可用的文档存储与搜索系统，满足现代应用的各种需求。