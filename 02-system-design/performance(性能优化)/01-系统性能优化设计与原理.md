# 系统性能优化设计与原理

## 概述

性能优化是系统设计中的关键环节，它直接影响用户体验、系统成本和业务价值。深入理解性能优化的原理和方法，是构建高质量系统的重要技能。

## 性能优化的本质

### 性能的定义与衡量

#### 核心性能指标

| 指标 | 定义 | 单位 | 关注点 |
|------|------|------|--------|
| **响应时间** | 用户发出请求到收到响应的总时间 | 毫秒(ms) | 平均值、P95、P99、P999 |
| **吞吐量** | 单位时间系统能够处理的请求数量 | TPS/QPS/RPS | 峰值、平均值、稳定性 |
| **并发数** | 同时处理的请求数量 | 个 | 并发用户数、并发连接数 |
| **资源利用率** | CPU、内存、磁盘I/O、网络等资源的使用程度 | 百分比(%) | 瓶颈识别、容量规划 |

```python
# 核心性能指标计算示例
import math
from typing import List

class PerformanceMetricsCalculator:
    # 计算P分位数响应时间
    @staticmethod
    def calculate_percentile_response_time(response_times: List[float], percentile: float) -> float:
        if not response_times:
            return 0.0
        
        sorted_times = sorted(response_times)
        index = min(
            math.ceil(len(sorted_times) * percentile) - 1,
            len(sorted_times) - 1
        )
        return sorted_times[index]
    
    # 计算吞吐量
    @staticmethod
    def calculate_throughput(request_count: int, time_elapsed_in_seconds: float) -> float:
        return request_count / time_elapsed_in_seconds if time_elapsed_in_seconds > 0 else 0.0
```


### 性能优化的层次

**算法层面优化**：
- 时间复杂度和空间复杂度的改进
- 数据结构选择和算法优化
- 缓存算法和查找算法优化

**系统层面优化**：
- 操作系统调优
- 虚拟机或容器优化
- 编译器优化

**架构层面优化**：
- 微服务拆分和部署
- 缓存架构设计
- 数据库优化

**应用层面优化**：
- 代码优化
- 内存管理
- 并发处理

**基础设施层面优化**：
- 硬件升级
- 网络优化
- 存储优化

## 性能分析与瓶颈识别

### 性能分析工具与方法

**系统监控工具**：
```bash
# Linux系统监控
# CPU使用情况
top -p $(pgrep -f "your-application")

# 内存使用情况
free -h
vmstat 1 10

# 磁盘I/O
iostat -x 1 10

# 网络监控
netstat -i
ss -tuln

# Java应用监控
jstat -gc pid 1s
jmap -heap pid
```

**应用性能监控（APM）**：
```python
# Flask应用性能监控示例（使用Prometheus客户端）
from flask import Flask
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
import time

app = Flask(__name__)

# 定义性能指标
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP Requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP Request Duration', ['method', 'endpoint'])

# 性能监控中间件
@app.before_request
def before_request():
    request.start_time = time.time()

@app.after_request
def after_request(response):
    duration = time.time() - request.start_time
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.endpoint,
        status=response.status_code
    ).inc()
    REQUEST_DURATION.labels(
        method=request.method,
        endpoint=request.endpoint
    ).observe(duration)
    return response

# Prometheus指标暴露端点
@app.route('/metrics')
def metrics():
    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}

if __name__ == '__main__':
    app.run(debug=True)
```

**分布式链路追踪**：
```python
# OpenTelemetry分布式追踪示例
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter

# 初始化追踪器
provider = TracerProvider()
processor = BatchSpanProcessor(ConsoleSpanExporter())
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

class UserService:
    def get_user_by_id(self, user_id: int):
        with tracer.start_as_current_span("getUserById") as span:
            # 设置属性
            span.set_attribute("user.id", user_id)
            
            # 业务逻辑（模拟数据库查询）
            user = self._find_user_in_database(user_id)
            
            return user
    
    def _find_user_in_database(self, user_id: int):
        with tracer.start_as_current_span("findUserInDatabase"):
            # 模拟数据库查询延迟
            import time
            time.sleep(0.1)
            
            # 返回模拟用户数据
            return {
                "id": user_id,
                "name": f"User {user_id}",
                "email": f"user{user_id}@example.com"
            }
```

### 瓶颈识别的系统性方法

#### 性能分析方法对比

| 分析方法 | 特点 | 适用场景 | 实施步骤 |
|---------|------|---------|--------|
| **自顶向下分析** | 从用户体验到系统内部，逐层深入 | 整体性能问题排查 | 1. 测量端到端响应时间<br>2. 分解各层耗时<br>3. 定位主要瓶颈 |
| **自底向上分析** | 从基础设施到应用层，逐步向上 | 资源瓶颈识别 | 1. 监控系统资源使用<br>2. 分析数据库性能<br>3. 检查应用层效率 |
| **热点分析** | 识别系统中耗资源最多的部分 | 代码级优化 | 1. 性能采样<br>2. 热点代码识别<br>3. 针对性优化 |
| **全链路追踪** | 追踪单个请求的完整调用链路 | 分布式系统排障 | 1. 调用链采集<br>2. 路径分析<br>3. 异常点定位 |

```python
# 性能分析框架示例
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceBreakdown:
    network_latency: float
    database_time: float
    application_time: float
    cache_efficiency: float

@dataclass
class Bottleneck:
    component: str
    latency: float
    severity: str = "MEDIUM"

class PerformanceAnalysisFramework:
    # 定义瓶颈阈值（毫秒）
    BOTTLENECK_THRESHOLD_DB = 1000  # 数据库操作超过1秒视为瓶颈
    BOTTLENECK_THRESHOLD_APP = 500   # 应用处理超过500毫秒视为瓶颈
    BOTTLENECK_THRESHOLD_NETWORK = 300  # 网络延迟超过300毫秒视为瓶颈
    
    def breakdown_end_to_end_request(self, request: Dict[str, Any]) -> PerformanceBreakdown:
        """端到端性能分解"""
        return PerformanceBreakdown(
            network_latency=self._measure_network_latency(request),
            database_time=self._measure_database_operations(request),
            application_time=self._measure_application_processing(request),
            cache_efficiency=self._analyze_cache_efficiency(request)
        )
    
    def identify_bottlenecks(self, breakdown: PerformanceBreakdown) -> List[Bottleneck]:
        """识别性能瓶颈"""
        bottlenecks = []
        
        # 基于阈值识别瓶颈点
        if breakdown.database_time > self.BOTTLENECK_THRESHOLD_DB:
            bottlenecks.append(Bottleneck(
                component="DATABASE",
                latency=breakdown.database_time,
                severity="HIGH"
            ))
        
        if breakdown.application_time > self.BOTTLENECK_THRESHOLD_APP:
            bottlenecks.append(Bottleneck(
                component="APPLICATION",
                latency=breakdown.application_time,
                severity="MEDIUM"
            ))
        
        if breakdown.network_latency > self.BOTTLENECK_THRESHOLD_NETWORK:
            bottlenecks.append(Bottleneck(
                component="NETWORK",
                latency=breakdown.network_latency,
                severity="LOW"
            ))
        
        return bottlenecks
    
    def _measure_network_latency(self, request: Dict[str, Any]) -> float:
        """测量网络延迟"""
        # 实际实现中可以通过监控工具获取
        return 150.0  # 模拟值（毫秒）
    
    def _measure_database_operations(self, request: Dict[str, Any]) -> float:
        """测量数据库操作时间"""
        # 实际实现中可以通过数据库监控或追踪获取
        return 800.0  # 模拟值（毫秒）
    
    def _measure_application_processing(self, request: Dict[str, Any]) -> float:
        """测量应用处理时间"""
        # 实际实现中可以通过应用性能监控获取
        return 350.0  # 模拟值（毫秒）
    
    def _analyze_cache_efficiency(self, request: Dict[str, Any]) -> float:
        """分析缓存效率"""
        # 返回缓存命中率（0.0-1.0）
        return 0.85  # 85%命中率
```

#### 关键分析维度详解

**业务维度**：
- 关注用户直接体验的指标：响应时间、吞吐量、错误率
- 通过业务指标变化，快速感知性能异常
- 建立业务指标与技术指标的关联模型

**系统维度**：
- 资源利用率监控：CPU、内存、磁盘I/O、网络
- 应用性能监控：JVM状态、线程池、连接池
- 数据库性能分析：查询执行计划、锁竞争、索引使用

**代码维度**：
- CPU热点分析：找出消耗CPU最多的代码路径
- 内存分析：检测内存泄漏、对象分配热点
- I/O阻塞分析：识别导致线程阻塞的操作

### 性能测试方法

**负载测试（Load Testing）**：
```bash
# JMeter负载测试示例
# 创建一个测试计划，包含100个线程，每个线程循环10次
jmeter -n -t load_test_plan.jmx -l results.jtl -e -o html_report/
```

**压力测试（Stress Testing）**：
```bash
# 渐进式压力测试
# 从100QPS开始，逐步增加到1000QPS
wrk -t12 -c400 -d30s --script=load_test.lua http://your-api-endpoint
```

**并发测试（Concurrency Testing）**：
```python
# 模拟并发用户访问
import time
import threading
from concurrent.futures import ThreadPoolExecutor, Future
from typing import List

class ConcurrentUserAccessTest:
    def test_concurrent_user_access(self) -> None:
        thread_count = 100
        
        # 使用事件同步线程开始
        start_event = threading.Event()
        # 用于等待所有线程完成
        threads = []
        results: List[float] = []
        results_lock = threading.Lock()
        
        def user_thread(thread_id: int) -> None:
            # 等待开始信号
            start_event.wait()
            
            start_time = time.time()
            
            # 执行测试操作
            self._test_user_operation(thread_id)
            
            end_time = time.time()
            
            # 记录执行时间
            with results_lock:
                results.append((end_time - start_time) * 1000)  # 转换为毫秒
        
        # 创建并启动线程
        with ThreadPoolExecutor(max_workers=thread_count) as executor:
            for i in range(thread_count):
                executor.submit(user_thread, i)
            
            # 所有线程准备就绪后，发出开始信号
            start_event.set()
        
        # 分析结果
        self._analyze_results(results)
    
    def _test_user_operation(self, thread_id: int) -> None:
        """模拟用户操作"""
        # 这里可以替换为实际的测试操作
        time.sleep(0.01)  # 模拟10ms的操作延迟
    
    def _analyze_results(self, results: List[float]) -> None:
        """分析测试结果"""
        if not results:
            return
        
        total_requests = len(results)
        total_time = sum(results)
        min_time = min(results)
        max_time = max(results)
        avg_time = total_time / total_requests
        
        # 计算95%和99%分位数
        sorted_results = sorted(results)
        p95_idx = int(0.95 * total_requests)
        p99_idx = int(0.99 * total_requests)
        p95_time = sorted_results[p95_idx] if p95_idx < total_requests else sorted_results[-1]
        p99_time = sorted_results[p99_idx] if p99_idx < total_requests else sorted_results[-1]
        
        # 输出结果
        print(f"并发用户数: {total_requests}")
        print(f"总耗时(ms): {total_time:.2f}")
        print(f"平均响应时间(ms): {avg_time:.2f}")
        print(f"最小响应时间(ms): {min_time:.2f}")
        print(f"最大响应时间(ms): {max_time:.2f}")
        print(f"95%响应时间(ms): {p95_time:.2f}")
        print(f"99%响应时间(ms): {p99_time:.2f}")
        print(f"吞吐量(req/s): {total_requests / (total_time / 1000):.2f}")

# 运行测试
if __name__ == "__main__":
    test = ConcurrentUserAccessTest()
    test.test_concurrent_user_access()
```

## 缓存优化策略

### 缓存的层次化设计

#### 多级缓存架构设计原理

多级缓存通过分层缓存数据，平衡性能与一致性，典型的缓存层次结构：
```
浏览器缓存 → CDN → 反向代理缓存 → 应用缓存 → 数据库缓存
```

缓存层次对比表：

| 缓存级别 | 技术方案 | 访问延迟 | 适用场景 | 一致性策略 |
|---------|---------|---------|---------|----------|
| L1（本地缓存） | Caffeine, Guava Cache | 微秒级 | 热点数据，读多写少 | 本地更新，TTL过期 |
| L2（分布式缓存） | Redis, Memcached | 毫秒级 | 分布式共享数据 | 发布订阅，消息通知 |
| L3（数据库） | MySQL, PostgreSQL | 毫秒至秒级 | 持久化存储，事务性数据 | 事务，锁机制 |

**L1缓存（应用级缓存）**：
```python
# Python本地缓存配置 (使用cachetools库)
from cachetools import cached, TTLCache
from typing import Optional, Any
import time

class LocalCacheManager:
    # 定义不同类型的缓存
    USER_CACHE = TTLCache(maxsize=1000, ttl=30 * 60)  # 30分钟过期
    PRODUCT_CACHE = TTLCache(maxsize=5000, ttl=60 * 60)  # 1小时过期
    
    @staticmethod
    def get_cache(name: str) -> Optional[TTLCache]:
        """获取指定名称的缓存"""
        cache_mapping = {
            'user': LocalCacheManager.USER_CACHE,
            'product': LocalCacheManager.PRODUCT_CACHE
        }
        return cache_mapping.get(name)

# 缓存装饰器用法示例
@cached(cache=LocalCacheManager.USER_CACHE)
def get_user_info(user_id: int) -> dict:
    """从数据库获取用户信息，使用本地缓存"""
    # 模拟数据库查询延迟
    time.sleep(0.1)
    return {
        'id': user_id,
        'name': f'User_{user_id}',
        'email': f'user{user_id}@example.com'
    }
```

**L2缓存（分布式缓存）**：
```python
# Redis分布式缓存配置和使用 (使用redis-py库)
import redis
from typing import Optional, Any
import json

class RedisCacheManager:
    def __init__(self, host: str = 'redis-cluster', port: int = 6379, db: int = 0):
        self.redis_client = redis.Redis(
            host=host,
            port=port,
            db=db,
            socket_timeout=5.0,  # 5秒超时
            max_connections=20   # 最大连接数
        )
    
    def get(self, key: str) -> Optional[Any]:
        """获取缓存数据"""
        value = self.redis_client.get(key)
        if value:
            return json.loads(value)
        return None
    
    def set(self, key: str, value: Any, expire_seconds: int = 3600) -> bool:
        """设置缓存数据"""
        return self.redis_client.set(
            key,
            json.dumps(value),
            ex=expire_seconds
        )
    
    def delete(self, key: str) -> bool:
        """删除缓存数据"""
        return self.redis_client.delete(key) > 0

# Redis缓存管理器实例
redis_cache = RedisCacheManager()
```

#### 多级缓存实现模式

多级缓存的核心是"层层递进"的数据访问策略，结合缓存回写与失效机制，提供高性能同时保证数据一致性。

### 缓存策略设计

**缓存模式选择**：

**Cache-Aside模式**：
```python
from typing import Optional
from cachetools import TTLCache

class UserService:
    def __init__(self):
        # 模拟数据库访问层
        self.user_repository = UserRepository()
        # 本地缓存配置
        self.cache = TTLCache(maxsize=1000, ttl=30 * 60)  # 30分钟过期
    
    def get_user_by_id(self, user_id: int) -> Optional[dict]:
        cache_key = f"user:{user_id}"
        
        # 1. 先查缓存
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # 2. 缓存不命中，查询数据库
        user = self.user_repository.find_by_id(user_id)
        if user:
            # 3. 将结果存入缓存
            self.cache[cache_key] = user
        
        return user

class UserRepository:
    def find_by_id(self, user_id: int) -> Optional[dict]:
        # 模拟数据库查询
        # 这里可以替换为实际的数据库查询逻辑
        users = {
            1: {"id": 1, "name": "Alice", "email": "alice@example.com"},
            2: {"id": 2, "name": "Bob", "email": "bob@example.com"},
            3: {"id": 3, "name": "Charlie", "email": "charlie@example.com"}
        }
        return users.get(user_id)
```

**Cache-Through模式**：
```python
from typing import Optional
from cachetools import TTLCache

class ProductService:
    def __init__(self):
        self.repository = ProductRepository()
        self.cache = TTLCache(maxsize=5000, ttl=60 * 60)  # 1小时过期
    
    def save_product(self, product: dict) -> None:
        # 1. 同时写入数据库和缓存
        self.repository.save(product)
        cache_key = f"product:{product['id']}"
        self.cache[cache_key] = product
    
    def get_product(self, product_id: int) -> Optional[dict]:
        # 2. 读取时，如果缓存不存在则自动加载
        cache_key = f"product:{product_id}"
        
        def load_from_db():
            product = self.repository.find_by_id(product_id)
            if product:
                self.cache[cache_key] = product
            return product
        
        return self.cache.get(cache_key, load_from_db())

class ProductRepository:
    def __init__(self):
        self.products = {}
    
    def save(self, product: dict) -> None:
        self.products[product['id']] = product
    
    def find_by_id(self, product_id: int) -> Optional[dict]:
        return self.products.get(product_id)
```

**Write-Behind模式**：
```python
from typing import Dict
from collections import deque
import threading
import time
import queue

class StatisticsService:
    def __init__(self):
        # 内存缓存
        self.cache: Dict[str, Statistics] = {}
        # 写队列
        self.write_queue = queue.Queue()
        # 缓存锁
        self.cache_lock = threading.Lock()
        # 启动异步写入线程
        self.writer_thread = threading.Thread(target=self._write_worker, daemon=True)
        self.writer_thread.start()
    
    def record_event(self, event_type: str) -> None:
        # 1. 快速写入缓存
        key = f"stats:{event_type}"
        
        with self.cache_lock:
            if key not in self.cache:
                self.cache[key] = Statistics(event_type)
            self.cache[key].increment()
        
        # 2. 将更新放入写队列
        self.write_queue.put(event_type)
        
    def _write_worker(self) -> None:
        # 异步写入数据库
        while True:
            try:
                # 批量处理或定时处理
                time.sleep(5)  # 每5秒批量写入一次
                
                # 复制当前需要写入的数据
                with self.cache_lock:
                    # 创建需要写入的统计数据副本
                    to_write = list(self.cache.values())
                
                if to_write:
                    # 批量写入数据库
                    self._batch_write_to_db(to_write)
                    print(f"已异步写入 {len(to_write)} 条统计数据到数据库")
                    
            except Exception as e:
                print(f"写入数据库时出错: {e}")
                time.sleep(1)  # 出错后重试间隔

    def _batch_write_to_db(self, statistics_list: list) -> None:
        # 模拟批量写入数据库
        # 实际实现中这里会调用数据库API
        for stats in statistics_list:
            # 模拟数据库写入延迟
            time.sleep(0.01)

class Statistics:
    def __init__(self, event_type: str):
        self.event_type = event_type
        self.count = 0
        self.last_updated = time.time()
    
    def increment(self) -> None:
        self.count += 1
        self.last_updated = time.time()
```
```

### 缓存失效策略

**TTL（Time To Live）设计**：
```python
# 分层TTL策略
class CacheTTLManager:
    # 静态数据：较长的TTL（单位：秒）
    STATIC_DATA_TTL = 24 * 60 * 60  # 24小时
    
    # 用户相关数据：中等TTL
    USER_DATA_TTL = 30 * 60  # 30分钟
    
    # 实时数据：较短TTL
    REALTIME_DATA_TTL = 30  # 30秒
    
    @classmethod
    def get_ttl_by_business_rule(cls, data_type: str, user_level: str) -> int:
        """根据业务规则动态计算TTL"""
        if data_type == "PRODUCT_CATALOG":
            return 12 * 60 * 60  # 12小时
        elif data_type == "USER_PREFERENCES":
            return 15 * 60 if user_level == "VIP" else 30 * 60  # VIP 15分钟，普通用户30分钟
        elif data_type == "REAL_TIME_STOCK":
            return 5  # 5秒
        else:
            return 10 * 60  # 默认10分钟
```

**主动失效策略**：
```python
# 主动失效策略实现
import redis
from typing import Set

class CacheInvalidationService:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
    
    def handle_user_update(self, user_id: int, department_id: int) -> None:
        """处理用户更新事件，失效相关缓存"""
        # 失效用户相关缓存
        self.invalidate_cache(f"user:{user_id}")
        
        # 失效列表缓存
        self.invalidate_cache(f"users:list:{department_id}")
        
        # 失效统计缓存
        self.invalidate_cache(f"stats:users:department:{department_id}")
    
    def invalidate_cache(self, pattern: str) -> int:
        """批量删除匹配模式的缓存"""
        # 批量删除匹配模式的缓存
        keys: Set[str] = self.redis_client.keys(f"{pattern}*")
        if keys:
            return self.redis_client.delete(*keys)
        return 0

# 使用示例
# redis_client = redis.Redis(host='localhost', port=6379, db=0)
# invalidation_service = CacheInvalidationService(redis_client)
# invalidation_service.handle_user_update(123, 45)
```

## 数据库优化策略

### SQL优化基础

**查询优化原则**：

1. **索引优化**：

#### 索引设计核心原则

| 设计原则 | 说明 | 示例 | 最佳实践 |
|---------|------|------|----------|
| **选择性优先** | 优先为选择性高的字段创建索引 | 用户邮箱、手机号 | 唯一性字段索引效率最高 |
| **查询频率** | 高频查询字段优先创建索引 | 商品ID、用户ID | 80%查询覆盖原则 |
| **最左前缀** | 复合索引遵循最左匹配规则 | 按(name, age)查询时，name必须在前 | 查询条件包含前缀字段 |
| **更新成本** | 平衡查询性能与写入性能 | 频繁更新的状态字段谨慎建索引 | 读多写少场景索引更有效 |

```sql
-- 复合索引优化示例
CREATE INDEX idx_user_dept_status ON users(department_id, status, created_at);

-- 有效利用索引
SELECT * FROM users 
WHERE department_id = 1  -- 使用复合索引前缀
  AND status = 'ACTIVE'
ORDER BY created_at DESC; -- 利用索引排序

-- 索引失效示例
SELECT * FROM users 
WHERE status = 'ACTIVE'  -- 跳过索引前缀
  AND department_id = 1; -- 索引无法充分利用
```

**执行计划分析**：
```sql
EXPLAIN SELECT * FROM orders WHERE user_id = 123 AND status = 'PAID';
-- 关注：type、key、rows、Extra字段
-- 优化目标：type达到range或ref级别，避免ALL(全表扫描)

2. **避免全表扫描**：
```sql
-- 避免函数在WHERE条件中
-- 不好的写法
SELECT * FROM orders WHERE DATE(created_at) = '2024-01-01';

-- 好的写法
SELECT * FROM orders 
WHERE created_at >= '2024-01-01 00:00:00'
  AND created_at < '2024-01-02 00:00:00';
```

3. **LIMIT优化**：
```sql
-- 大表分页优化
-- 不好的写法
SELECT * FROM orders ORDER BY created_at DESC LIMIT 1000000, 20;

-- 好的写法（使用子查询）
SELECT o.* FROM orders o
INNER JOIN (
    SELECT id FROM orders 
    ORDER BY created_at DESC 
    LIMIT 1000000, 20
) t ON o.id = t.id
ORDER BY o.created_at DESC;
```

### 数据库架构优化

**读写分离**：
```python
# 使用SQLAlchemy实现主从数据源读写分离
from sqlalchemy import create_engine, orm
from sqlalchemy.orm import sessionmaker
from contextlib import contextmanager
import threading

# 全局上下文存储当前数据库类型
_db_context = threading.local()

class DatabaseContextHolder:
    """数据库上下文管理器"""
    MASTER = "master"
    SLAVE = "slave"
    
    @classmethod
    def set_database_type(cls, db_type: str) -> None:
        _db_context.db_type = db_type
    
    @classmethod
    def get_database_type(cls) -> str:
        return getattr(_db_context, "db_type", cls.MASTER)

class RoutingSession:
    """路由会话，根据上下文选择数据源"""
    def __init__(self, master_engine, slave_engine):
        self.master_engine = master_engine
        self.slave_engine = slave_engine
        
        # 创建会话工厂
        self.master_session_factory = sessionmaker(bind=master_engine)
        self.slave_session_factory = sessionmaker(bind=slave_engine)
    
    @contextmanager
    def get_session(self, read_only: bool = False):
        """获取数据库会话"""
        # 根据上下文或read_only参数选择数据源
        if not read_only:
            db_type = DatabaseContextHolder.MASTER
        else:
            db_type = DatabaseContextHolder.get_database_type()
        
        session_factory = self.master_session_factory if db_type == DatabaseContextHolder.MASTER else self.slave_session_factory
        session = session_factory()
        
        try:
            yield session
            session.commit()
        except Exception as e:
            session.rollback()
            raise e
        finally:
            session.close()

# 数据源配置
def create_data_sources():
    """创建主从数据源"""
    # 主数据库（写操作）
    master_engine = create_engine(
        "mysql+pymysql://user:password@master-db:3306/app",
        pool_size=10,
        max_overflow=20
    )
    
    # 从数据库（读操作）
    slave_engine = create_engine(
        "mysql+pymysql://user:password@slave-db:3306/app",
        pool_size=10,
        max_overflow=20
    )
    
    return RoutingSession(master_engine, slave_engine)

# 示例用法
# 创建数据源路由会话
session_manager = create_data_sources()

class UserService:
    @staticmethod
    def get_all_users():
        """获取所有用户（读操作，默认使用slave）"""
        with session_manager.get_session(read_only=True) as session:
            # 使用SQLAlchemy ORM查询
            from models import User
            return session.query(User).all()
    
    @staticmethod
    def create_user(user_data):
        """创建用户（写操作，使用master）"""
        with session_manager.get_session(read_only=False) as session:
            from models import User
            user = User(**user_data)
            session.add(user)
            session.commit()
            return user

# 高级用法：手动指定数据源
with session_manager.get_session(read_only=True) as session:
    DatabaseContextHolder.set_database_type(DatabaseContextHolder.MASTER)  # 强制使用master
    # 执行查询操作
```

**分库分表策略**：

#### 分库分表设计模式

| 分片策略 | 特点 | 适用场景 | 优缺点 |
|---------|------|---------|--------|
| **范围分片** | 按时间范围或ID范围划分 | 日志、订单等时序数据 | 优点：扩展简单<br>缺点：热点问题 |
| **哈希分片** | 基于分片键哈希计算 | 用户数据、交易记录 | 优点：数据分布均匀<br>缺点：扩容困难 |
| **列表分片** | 按业务属性列表划分 | 地域、业务类型数据 | 优点：业务隔离<br>缺点：分片维护复杂 |
| **复合分片** | 组合多种分片策略 | 复杂业务场景 | 优点：灵活<br>缺点：实现复杂 |

```python
# 分片路由核心实现
class ShardRouter:
    def __init__(self, db_count: int = 2, table_count: int = 4):
        """
        初始化分片路由器
        :param db_count: 分库数量
        :param table_count: 单库分表数量
        """
        self.db_count = db_count
        self.table_count = table_count
    
    def get_table_name(self, base_table_name: str, user_id: int) -> str:
        """
        根据用户ID进行分表（哈希分片）
        :param base_table_name: 基础表名
        :param user_id: 用户ID
        :return: 分表后的完整表名
        """
        shard_num = user_id % self.table_count + 1
        return f"{base_table_name}_{shard_num}"
    
    def get_database_name(self, base_db_name: str, user_id: int) -> str:
        """
        根据用户ID进行分库（范围分片）
        :param base_db_name: 基础数据库名
        :param user_id: 用户ID
        :return: 分库后的完整数据库名
        """
        # 每10000个用户划分到一个数据库
        db_num = (user_id // 10000) % self.db_count + 1
        return f"{base_db_name}_{db_num}"
    
    def get_full_route(self, base_db_name: str, base_table_name: str, user_id: int) -> tuple:
        """
        获取完整的分库分表路由
        :return: (database_name, table_name)
        """
        return (
            self.get_database_name(base_db_name, user_id),
            self.get_table_name(base_table_name, user_id)
        )

# 使用示例
router = ShardRouter(db_count=2, table_count=4)

# 为用户ID=12345获取路由信息
db_name, table_name = router.get_full_route("app_db", "users", 12345)
print(f"用户12345的路由: 数据库={db_name}, 表={table_name}")
# 输出: 用户12345的路由: 数据库=app_db_2, 表=users_2


# SQLAlchemy分片扩展示例
from sqlalchemy import Table, MetaData, create_engine
from sqlalchemy.orm import sessionmaker

class ShardedSessionManager:
    """分片会话管理器"""
    def __init__(self, base_db_url: str, db_count: int, table_count: int):
        self.base_db_url = base_db_url  # 如: mysql+pymysql://user:password@host:port/
        self.db_count = db_count
        self.table_count = table_count
        self.engines = {}
        self.session_factories = {}
        self.router = ShardRouter(db_count, table_count)
        
        # 初始化所有数据库引擎
        for i in range(1, db_count + 1):
            db_name = f"app_db_{i}"
            engine_url = f"{base_db_url}{db_name}"
            self.engines[db_name] = create_engine(engine_url)
            self.session_factories[db_name] = sessionmaker(bind=self.engines[db_name])
    
    def get_session(self, user_id: int):
        """根据用户ID获取对应分片的会话"""
        db_name, _ = self.router.get_full_route("app_db", "users", user_id)
        return self.session_factories[db_name]()
    
    def get_table(self, user_id: int, base_table_name: str, metadata: MetaData):
        """根据用户ID获取对应分片表对象"""
        _, table_name = self.router.get_full_route("app_db", base_table_name, user_id)
        return Table(table_name, metadata, autoload_with=self.engines["app_db_1"])

# 分片会话管理器使用示例
# session_manager = ShardedSessionManager(
#     base_db_url="mysql+pymysql://user:password@localhost:3306/",
#     db_count=2,
#     table_count=4
# )

# # 根据用户ID获取会话
# with session_manager.get_session(user_id=12345) as session:
#     # 获取用户对应的分片表
#     users_table = session_manager.get_table(12345, "users", MetaData())
#     # 执行查询
#     result = session.execute(users_table.select().where(users_table.c.id == 12345))
#     user = result.fetchone()
```

## 分库分表最佳实践

### 分片设计要点

1. **分片键选择**
   - 选择高频查询条件作为分片键，避免跨库查询
   - 优先选择业务主键或唯一标识作为分片键
   - 避免使用频繁更新的字段作为分片键

2. **数据一致性保障**
   - 同事务数据应尽量落在同一分片
   - 使用分布式事务框架（如Seata）处理跨分片事务
   - 实现最终一致性机制（如消息队列、定时任务）

3. **扩容策略**
   - 预留扩容空间，设计平滑扩容方案
   - 采用预分片策略，提前创建足够的分片
   - 实现数据迁移工具，支持在线扩容

4. **跨分片查询优化**
   - 避免或优化跨分片JOIN和聚合操作
   - 使用分布式查询引擎（如MyCAT、ShardingSphere）
   - 实现数据冗余或缓存机制减少跨分片查询

5. **监控与维护**
   - 建立分片数据分布监控
   - 实现慢查询分析与优化
   - 定期进行数据一致性校验
```

### 数据库连接池优化

**连接池配置**：
```python
# 使用DBUtils实现高性能数据库连接池
from dbutils.pooled_db import PooledDB
import pymysql
import threading

class DatabasePoolConfig:
    """数据库连接池配置"""
    
    def __init__(self):
        # 基础配置
        self.host = "localhost"
        self.port = 3306
        self.database = "app"
        self.user = "user"
        self.password = "password"
        self.charset = "utf8mb4"
        
        # 连接池配置
        self.max_connections = 20           # 最大连接数
        self.mincached = 5                  # 初始化时创建的空闲连接数
        self.maxcached = 10                 # 最大空闲连接数
        self.maxshared = 0                  # 最大共享连接数，0表示所有连接都不可共享
        self.blocking = True                # 连接池耗尽时是否阻塞等待
        self.maxusage = 0                   # 单个连接最大使用次数，0表示无限制
        self.setsession = ["SET AUTOCOMMIT = 0"]  # 连接创建时执行的SQL语句
        self.reset = True                   # 连接返回池时是否重置
        self.failing_action = "raise"        # 获取连接失败时的操作：raise, log, ignore
        self. ping = 1                      # 连接健康检查频率，1表示每次获取连接时检查
    
    def create_pool(self):
        """创建连接池"""
        return PooledDB(
            creator=pymysql,               # 使用pymysql作为数据库驱动
            host=self.host,
            port=self.port,
            user=self.user,
            password=self.password,
            database=self.database,
            charset=self.charset,
            maxconnections=self.max_connections,
            mincached=self.mincached,
            maxcached=self.maxcached,
            maxshared=self.maxshared,
            blocking=self.blocking,
            maxusage=self.maxusage,
            setsession=self.setsession,
            reset=self.reset,
            failingaction=self.failing_action,
            ping=self.ping
        )

# 单例模式的连接池管理器
class DatabasePoolManager:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(DatabasePoolManager, cls).__new__(cls)
                config = DatabasePoolConfig()
                cls._instance.pool = config.create_pool()
        return cls._instance
    
    def get_connection(self):
        """从连接池获取连接"""
        return self.pool.connection()

# 使用示例
db_pool = DatabasePoolManager()

# 获取连接并执行SQL
def execute_query(sql, params=None):
    conn = None
    cursor = None
    try:
        conn = db_pool.get_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)  # 使用字典游标
        
        cursor.execute(sql, params)
        result = cursor.fetchall()
        
        conn.commit()  # 手动提交事务
        return result
    except Exception as e:
        if conn:
            conn.rollback()
        raise e
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()  # 将连接返回池，而不是真正关闭

# 执行查询示例
# users = execute_query("SELECT * FROM users WHERE status = %s", ("active",))


# SQLAlchemy连接池配置（推荐用于ORM）
from sqlalchemy import create_engine

class SQLAlchemyPoolConfig:
    """SQLAlchemy连接池配置"""
    
    @staticmethod
    def create_engine():
        """创建配置优化的SQLAlchemy引擎"""
        return create_engine(
            "mysql+pymysql://user:password@localhost:3306/app",
            
            # 连接池配置
            pool_size=20,                # 连接池大小
            max_overflow=10,             # 超出连接池大小的最大连接数
            pool_timeout=30,             # 获取连接超时时间
            pool_recycle=600,            # 连接最大生命周期（秒）
            pool_pre_ping=True,          # 连接池借用连接时进行健康检查
            
            # 性能优化配置
            echo=False,                  # 是否打印SQL语句
            echo_pool=False,             # 是否打印连接池操作
            pool_use_lifo=True,          # 使用LIFO策略获取连接
            
            # 其他配置
            isolation_level="READ COMMITTED"  # 事务隔离级别
        )

# 创建SQLAlchemy引擎
# engine = SQLAlchemyPoolConfig.create_engine()
# SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
```

## 并发优化策略

### 多线程性能优化

#### 线程池设计与优化

线程池是并发优化的核心，合理配置参数可以最大化系统吞吐量：

| 线程池类型 | 核心线程数 | 最大线程数 | 队列容量 | 适用场景 |
|----------|-----------|-----------|---------|----------|
| **CPU密集型** | CPU核心数 | CPU核心数 | 较小(100-500) | 计算任务、业务逻辑处理 |
| **IO密集型** | CPU核心数*2-4 | CPU核心数*4-8 | 较大(1000-5000) | 网络请求、数据库操作 |
| **混合型** | CPU核心数*1.5 | CPU核心数*3 | 中等(500-2000) | 综合业务场景 |

```python
# 使用concurrent.futures实现线程池配置
import concurrent.futures
import multiprocessing
import threading
import logging
import time

class ThreadPoolConfig:
    """线程池配置类"""
    
    def __init__(self):
        self.cpu_cores = multiprocessing.cpu_count()
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger('ThreadPoolConfig')
    
    def create_general_task_executor(self):
        """
        创建通用任务线程池（CPU密集型）
        :return: ThreadPoolExecutor实例
        """
        executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=self.cpu_cores * 2,  # 最大线程数
            thread_name_prefix="task-",      # 线程名称前缀
            initializer=self._thread_initializer,  # 线程初始化函数
            initargs=("general",)           # 初始化参数
        )
        
        self.logger.info(f"Created general task executor with {self.cpu_cores} core threads, max {self.cpu_cores * 2} threads")
        return executor
    
    def create_io_task_executor(self):
        """
        创建IO密集型任务线程池
        :return: ThreadPoolExecutor实例
        """
        # IO密集型任务可以使用更多线程
        max_workers = self.cpu_cores * 8
        executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=max_workers,         # IO密集型可使用更多线程
            thread_name_prefix="IO-Task-",   # 线程名称前缀
            initializer=self._thread_initializer,  # 线程初始化函数
            initargs=("io",)                # 初始化参数
        )
        
        self.logger.info(f"Created IO task executor with {self.cpu_cores * 4} core threads, max {max_workers} threads")
        return executor
    
    def _thread_initializer(self, pool_type):
        """线程初始化函数"""
        thread_name = threading.current_thread().name
        self.logger.debug(f"Thread {thread_name} initialized for {pool_type} pool")
    
    def shutdown_executor(self, executor):
        """优雅关闭线程池"""
        executor.shutdown(wait=True)
        self.logger.info(f"Executor {executor} shutdown gracefully")

# 线程池管理器（单例模式）
class ThreadPoolManager:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(ThreadPoolManager, cls).__new__(cls)
                cls._instance.config = ThreadPoolConfig()
                cls._instance.general_executor = cls._instance.config.create_general_task_executor()
                cls._instance.io_executor = cls._instance.config.create_io_task_executor()
        return cls._instance
    
    def get_general_executor(self):
        """获取通用任务执行器"""
        return self.general_executor
    
    def get_io_executor(self):
        """获取IO密集型任务执行器"""
        return self.io_executor
    
    def shutdown_all(self):
        """关闭所有线程池"""
        self.config.shutdown_executor(self.general_executor)
        self.config.shutdown_executor(self.io_executor)

# 使用示例
def main():
    # 获取线程池管理器实例
    pool_manager = ThreadPoolManager()
    general_executor = pool_manager.get_general_executor()
    io_executor = pool_manager.get_io_executor()
    
    # 提交CPU密集型任务
    def cpu_intensive_task(n):
        """模拟CPU密集型任务"""
        result = 0
        for i in range(n):
            result += i * i
        return result
    
    # 提交IO密集型任务
    def io_intensive_task(url):
        """模拟IO密集型任务"""
        time.sleep(1)  # 模拟IO等待
        return f"Processed {url}"
    
    # 执行CPU密集型任务
    cpu_futures = [
        general_executor.submit(cpu_intensive_task, 1000000) for _ in range(10)
    ]
    
    # 执行IO密集型任务
    io_futures = [
        io_executor.submit(io_intensive_task, f"http://example.com/{i}") for i in range(20)
    ]
    
    # 获取结果
    for future in concurrent.futures.as_completed(cpu_futures):
        print(f"CPU task result: {future.result()}")
    
    for future in concurrent.futures.as_completed(io_futures):
        print(f"IO task result: {future.result()}")
    
    # 关闭所有线程池
    pool_manager.shutdown_all()

# if __name__ == "__main__":
#     main()
```

#### 线程池监控与调优

线程池使用过程中需要关注以下关键指标：
- **活跃线程数**：监控线程使用情况
- **队列使用率**：避免队列溢出
- **任务执行时间**：识别慢任务
- **拒绝次数**：评估容量是否充足

**调优建议**：
1. 区分任务类型配置不同线程池
2. 监控线程池指标，动态调整参数
3. 合理设置拒绝策略，避免任务丢失
4. 使用有界队列防止内存溢出
```

**异步处理设计**：
```python
import asyncio
import concurrent.futures
from typing import Dict, Any
import logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('OrderService')

# 模拟邮件服务
class EmailService:
    async def send_order_confirmation(self, order: Dict[str, Any]) -> None:
        """发送订单确认邮件"""
        # 模拟网络IO延迟
        await asyncio.sleep(1)
        logger.info(f"发送确认邮件给订单: {order['order_id']}")

# 模拟短信服务
class SmsService:
    async def send_order_notification(self, order: Dict[str, Any]) -> None:
        """发送订单通知短信"""
        # 模拟网络IO延迟
        await asyncio.sleep(0.5)
        logger.info(f"发送短信通知给订单: {order['order_id']}")

# 模拟订单仓库
class OrderRepository:
    def __init__(self):
        self.orders = {}  # 模拟数据库
    
    def save(self, order: Dict[str, Any]) -> Dict[str, Any]:
        """保存订单到数据库"""
        self.orders[order['order_id']] = order
        logger.info(f"订单已保存: {order['order_id']}")
        return order

# 订单服务
class OrderService:
    def __init__(self):
        self.email_service = EmailService()
        self.sms_service = SmsService()
        self.order_repository = OrderRepository()
        # 创建线程池执行器用于异步操作
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=5, thread_name_prefix="order-task-")
    
    async def send_order_confirmation(self, order: Dict[str, Any]) -> None:
        """异步发送订单确认通知"""
        try:
            # 使用asyncio.gather并行执行多个异步任务
            await asyncio.gather(
                self.email_service.send_order_confirmation(order),
                self.sms_service.send_order_notification(order)
            )
            logger.info(f"订单确认通知发送完成: {order['order_id']}")
        except Exception as e:
            logger.error(f"发送订单确认通知失败: {order['order_id']}, 错误: {str(e)}")
            raise
    
    def create_order(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """创建订单（同步方法）"""
        # 模拟事务边界
        order = {
            "order_id": f"order_{hash(frozenset(request.items()))}",
            "items": request["items"],
            "total": request["total"],
            "status": "created"
        }
        
        # 保存订单（模拟数据库操作）
        self.order_repository.save(order)
        
        # 异步处理副作用 - 使用线程池执行异步任务
        loop = asyncio.new_event_loop()
        future = asyncio.run_coroutine_threadsafe(self.send_order_confirmation(order), loop)
        
        # 启动事件循环（在单独线程中）
        def start_event_loop():
            asyncio.set_event_loop(loop)
            loop.run_forever()
        
        # 在后台线程中启动事件循环
        import threading
        threading.Thread(target=start_event_loop, daemon=True).start()
        
        logger.info(f"订单创建完成: {order['order_id']}")
        return order
    
    def shutdown(self) -> None:
        """关闭资源"""
        self.executor.shutdown(wait=True)

# 异步上下文管理器版本（更现代化的实现）
class AsyncOrderService:
    def __init__(self):
        self.email_service = EmailService()
        self.sms_service = SmsService()
        self.order_repository = OrderRepository()
    
    async def send_order_confirmation(self, order: Dict[str, Any]) -> None:
        """异步发送订单确认通知"""
        try:
            await asyncio.gather(
                self.email_service.send_order_confirmation(order),
                self.sms_service.send_order_notification(order)
            )
            logger.info(f"订单确认通知发送完成: {order['order_id']}")
        except Exception as e:
            logger.error(f"发送订单确认通知失败: {order['order_id']}, 错误: {str(e)}")
            raise
    
    async def create_order(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """异步创建订单"""
        order = {
            "order_id": f"order_{hash(frozenset(request.items()))}",
            "items": request["items"],
            "total": request["total"],
            "status": "created"
        }
        
        # 保存订单（同步操作转换为异步）
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, self.order_repository.save, order)
        
        # 异步处理副作用
        asyncio.create_task(self.send_order_confirmation(order))
        
        logger.info(f"订单创建完成: {order['order_id']}")
        return order

# 使用示例
async def main():
    # 使用异步服务
    async_order_service = AsyncOrderService()
    
    # 创建订单请求
    order_request = {
        "items": [{"product_id": "p1", "quantity": 2, "price": 100}],
        "total": 200
    }
    
    # 创建订单
    order = await async_order_service.create_order(order_request)
    
    # 等待一段时间以查看异步操作完成
    await asyncio.sleep(2)

# if __name__ == "__main__":
#     asyncio.run(main())
```

### 无锁并发编程

**原子操作优化**：
```python
import threading
import time
from typing import Optional

class AtomicCounter:
    """基于CAS操作的原子计数器"""
    def __init__(self, initial_value: int = 0):
        """初始化计数器"""
        self._value = initial_value
        self._lock = threading.Lock()
    
    def increment(self) -> int:
        """原子递增并返回新值"""
        with self._lock:
            self._value += 1
            return self._value
    
    def decrement(self) -> int:
        """原子递减并返回新值"""
        with self._lock:
            self._value -= 1
            return self._value
    
    def get(self) -> int:
        """获取当前值"""
        with self._lock:
            return self._value
    
    def compare_and_set(self, expected: int, update: int) -> bool:
        """CAS操作：如果当前值等于expected，则更新为update"""
        with self._lock:
            if self._value == expected:
                self._value = update
                return True
            return False

# 使用threading.Lock实现的AtomicLong替代
class AtomicLong:
    """原子长整型，模拟Java的AtomicLong"""
    def __init__(self, initial_value: int = 0):
        self._value = initial_value
        self._lock = threading.Lock()
    
    def get(self) -> int:
        """获取当前值"""
        with self._lock:
            return self._value
    
    def set(self, newValue: int) -> None:
        """设置新值"""
        with self._lock:
            self._value = newValue
    
    def increment_and_get(self) -> int:
        """原子递增并返回新值"""
        with self._lock:
            self._value += 1
            return self._value
    
    def compare_and_set(self, expect: int, update: int) -> bool:
        """CAS操作"""
        with self._lock:
            if self._value == expect:
                self._value = update
                return True
            return False

# 性能测试示例
def test_atomic_counter():
    counter = AtomicCounter()
    thread_count = 10
    iterations = 10000
    
    def worker():
        for _ in range(iterations):
            counter.increment()
    
    # 创建并启动线程
    threads = []
    start_time = time.time()
    
    for _ in range(thread_count):
        t = threading.Thread(target=worker)
        threads.append(t)
        t.start()
    
    # 等待所有线程完成
    for t in threads:
        t.join()
    
    end_time = time.time()
    
    print(f"预期值: {thread_count * iterations}")
    print(f"实际值: {counter.get()}")
    print(f"执行时间: {end_time - start_time:.2f}秒")

# 使用示例
def demo():
    # 创建原子计数器
    counter = AtomicCounter()
    
    # 原子递增
    print(f"初始值: {counter.get()}")
    counter.increment()
    print(f"递增后: {counter.get()}")
    
    # CAS操作示例
    current = counter.get()
    if counter.compare_and_set(current, current + 10):
        print(f"CAS操作成功，新值: {counter.get()}")
    else:
        print("CAS操作失败")
    
    # AtomicLong示例
    atomic_long = AtomicLong(100)
    atomic_long.increment_and_get()
    print(f"AtomicLong值: {atomic_long.get()}")

# if __name__ == "__main__":
#     demo()
#     test_atomic_counter()
```

**无锁数据结构**：
```python
from queue import Queue
import threading
import concurrent.futures
from typing import Any, Callable
import logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('AsyncEventProcessor')

# 事件类
class Event:
    """事件类"""
    def __init__(self, event_id: str, data: Any):
        self.event_id = event_id
        self.data = data

# 异步事件处理器
class AsyncEventProcessor:
    """异步事件处理器，使用线程安全的队列实现无锁并发"""
    
    def __init__(self, max_workers: int = 1):
        # 使用线程安全的队列
        self.event_queue = Queue()
        # 创建线程池用于处理事件
        self.processor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        # 启动事件处理循环
        self._running = True
        self._processing_thread = threading.Thread(target=self._process_events_loop, daemon=True)
        self._processing_thread.start()
    
    def process_event(self, event: Event):
        """提交事件进行处理"""
        self.event_queue.put(event)
        logger.info(f"事件已提交: {event.event_id}")
    
    def _process_events_loop(self):
        """事件处理主循环"""
        while self._running:
            try:
                # 阻塞等待事件
                event = self.event_queue.get(timeout=1)  # 1秒超时，定期检查是否停止
                self.handle_event(event)
                self.event_queue.task_done()
            except Queue.Empty:
                continue  # 队列为空，继续循环
    
    def handle_event(self, event: Event):
        """处理单个事件"""
        try:
            logger.info(f"处理事件: {event.event_id}, 数据: {event.data}")
            # 模拟事件处理逻辑
            # 实际应用中可以根据事件类型调用不同的处理方法
        except Exception as e:
            logger.error(f"处理事件 {event.event_id} 失败: {str(e)}")
    
    def shutdown(self):
        """关闭事件处理器"""
        self._running = False
        self._processing_thread.join()
        self.processor.shutdown(wait=True)
        logger.info("事件处理器已关闭")


# 使用示例
if __name__ == "__main__":
    # 创建事件处理器
    event_processor = AsyncEventProcessor(max_workers=2)
    
    # 提交多个事件
    for i in range(10):
        event = Event(f"event_{i}", {"value": i})
        event_processor.process_event(event)
    
    # 等待所有事件处理完成
    event_processor.event_queue.join()
    
    # 关闭事件处理器
    event_processor.shutdown()
```

**乐观锁机制**：
```python
from sqlalchemy import Column, Integer, String, Float, BigInteger
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy import create_engine
from sqlalchemy.exc import StaleDataError
import logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('OptimisticLockDemo')

# 创建基本模型类
Base = declarative_base()

# User模型，使用SQLAlchemy的乐观锁支持
class User(Base):
    __tablename__ = 'users'
    
    id = Column(BigInteger, primary_key=True, autoincrement=True)
    name = Column(String(50), nullable=False)
    balance = Column(Float, default=0.0)
    version = Column(Integer, nullable=False, default=1)  # 乐观锁版本字段
    
    def __repr__(self):
        return f"<User(id={self.id}, name='{self.name}', balance={self.balance}, version={self.version})>"

# 初始化数据库连接
engine = create_engine('sqlite:///:memory:', echo=True)  # 使用内存数据库进行演示
Base.metadata.create_all(engine)  # 创建所有表
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# User服务类，实现乐观锁更新
class UserService:
    def __init__(self):
        self.db: Session = SessionLocal()
    
    def get_user_by_id(self, user_id: int) -> User:
        """根据ID获取用户"""
        user = self.db.query(User).filter(User.id == user_id).first()
        if not user:
            raise ValueError(f"User with id {user_id} not found")
        return user
    
    def update_user_name(self, user_id: int, new_name: str) -> User:
        """使用乐观锁更新用户名"""
        try:
            # 1. 获取当前版本的用户
            user = self.get_user_by_id(user_id)
            original_version = user.version
            
            # 2. 修改用户信息
            user.name = new_name
            
            # 3. 手动版本控制更新
            updated = self.db.query(User).filter(
                User.id == user_id, 
                User.version == original_version
            ).update({
                "name": new_name,
                "version": original_version + 1
            })
            
            if updated == 0:
                # 版本不匹配，更新失败
                self.db.rollback()
                logger.warning(f"Update failed for user {user_id}: version mismatch")
                raise StaleDataError(f"Update failed for user {user_id}: version mismatch")
            
            # 4. 提交事务
            self.db.commit()
            
            # 5. 刷新用户对象以获取最新版本
            self.db.refresh(user)
            logger.info(f"User {user_id} name updated to '{new_name}' successfully")
            return user
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error updating user name: {str(e)}")
            raise
    
    def update_user_balance(self, user_id: int, amount: float) -> bool:
        """使用乐观锁更新用户余额"""
        try:
            # 手动实现乐观锁
            user = self.get_user_by_id(user_id)
            original_version = user.version
            
            # 计算新余额
            new_balance = user.balance + amount
            
            # 更新时检查版本号
            rows_updated = self.db.query(User).filter(
                User.id == user_id,
                User.version == original_version
            ).update({
                "balance": new_balance,
                "version": original_version + 1
            })
            
            if rows_updated > 0:
                self.db.commit()
                logger.info(f"User {user_id} balance updated successfully, new balance: {new_balance}")
                return True
            else:
                self.db.rollback()
                logger.warning(f"Failed to update balance for user {user_id}: version mismatch")
                return False
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error updating user balance: {str(e)}")
            return False
    
    def create_user(self, name: str, initial_balance: float = 0.0) -> User:
        """创建新用户"""
        try:
            user = User(name=name, balance=initial_balance)
            self.db.add(user)
            self.db.commit()
            self.db.refresh(user)
            logger.info(f"Created new user: {user}")
            return user
        except Exception as e:
            self.db.rollback()
            logger.error(f"Error creating user: {str(e)}")
            raise
    
    def close(self):
        """关闭数据库会话"""
        self.db.close()

# 演示并发场景下的乐观锁
import threading
import time

def demo_concurrent_updates():
    # 初始化服务
    user_service = UserService()
    
    try:
        # 创建一个测试用户
        user = user_service.create_user("Test User", 100.0)
        user_id = user.id
        
        # 模拟两个并发更新操作
        def update_operation1():
            """操作1：更新用户名为'Updated User 1'"""
            try:
                time.sleep(0.1)  # 模拟网络延迟
                user_service.update_user_name(user_id, "Updated User 1")
            except Exception as e:
                logger.error(f"Operation 1 failed: {str(e)}")
        
        def update_operation2():
            """操作2：更新用户名为'Updated User 2'"""
            try:
                time.sleep(0.1)  # 模拟网络延迟
                user_service.update_user_name(user_id, "Updated User 2")
            except Exception as e:
                logger.error(f"Operation 2 failed: {str(e)}")
        
        # 启动两个并发线程
        thread1 = threading.Thread(target=update_operation1, name="UpdateThread-1")
        thread2 = threading.Thread(target=update_operation2, name="UpdateThread-2")
        
        thread1.start()
        thread2.start()
        
        thread1.join()
        thread2.join()
        
        # 查看最终结果
        final_user = user_service.get_user_by_id(user_id)
        logger.info(f"Final user state: {final_user}")
        
    finally:
        user_service.close()

# 主函数
if __name__ == "__main__":
    demo_concurrent_updates()
```

**JVM性能调优**部分由于是Java特定内容，下面提供Python性能优化的等效内容：

### Python性能调优

**内存管理优化**：
```python
import gc
import tracemalloc
import sys

# 1. 内存使用监控
def monitor_memory_usage():
    """监控Python程序的内存使用情况"""
    tracemalloc.start()
    
    # 记录初始快照
    snapshot1 = tracemalloc.take_snapshot()
    
    # 执行你的代码
    # ... your code here ...
    
    # 记录结束快照
    snapshot2 = tracemalloc.take_snapshot()
    
    # 比较快照，找出内存分配差异
    top_stats = snapshot2.compare_to(snapshot1, 'lineno')
    
    print("内存分配统计:")
    for stat in top_stats[:10]:  # 显示前10个内存分配点
        print(stat)

# 2. 垃圾回收优化
def optimize_garbage_collection():
    """优化Python的垃圾回收"""
    # 禁用自动垃圾回收
    gc.disable()
    
    try:
        # 执行内存密集型操作
        # ... your memory intensive code here ...
        
        # 手动触发垃圾回收
        collected = gc.collect()
        print(f"手动回收了 {collected} 个对象")
    finally:
        # 重新启用自动垃圾回收
        gc.enable()

# 3. 大对象处理
def process_large_data():
    """高效处理大对象"""
    # 使用生成器替代列表，减少内存占用
    def large_data_generator():
        for i in range(1000000):
            yield i * 2
    
    # 处理生成器数据
    total = sum(large_data_generator())
    print(f"处理结果: {total}")

# 4. 减少引用循环
def avoid_reference_cycles():
    """避免引用循环导致的内存泄漏"""
    class Node:
        def __init__(self, data):
            self.data = data
            self.next = None  # 使用弱引用代替强引用
    
    # 使用弱引用
    import weakref
    
    class WeakNode:
        def __init__(self, data):
            self.data = data
            self.next = None  # 使用弱引用
    
    # 创建节点
    n1 = Node(1)
    n2 = Node(2)
    n1.next = weakref.ref(n2)  # 使用弱引用
    n2.next = weakref.ref(n1)  # 使用弱引用
    
    # 这样即使存在循环，也不会阻止垃圾回收

# 5. 内存分析工具使用
import objgraph

def analyze_object_graph():
    """分析对象引用图，找出内存泄漏"""
    # 显示对象数量
    objgraph.show_most_common_types(limit=10)
    
    # 查找特定类型的对象
    # objgraph.show_backrefs([obj], filename='backrefs.png')
    # objgraph.show_refs([obj], filename='refs.png')
```
```

## JVM性能调优

### 内存管理优化

**堆内存配置**：
```bash
# 启动参数配置
java -Xms4g -Xmx4g \
     -XX:NewRatio=3 \
     -XX:SurvivorRatio=8 \
     -XX:+UseG1GC \
     -XX:MaxGCPauseMillis=200 \
     -XX:G1HeapRegionSize=16m \
     -XX:+G1UseAdaptiveIHOP \
     app.jar
```

**垃圾回收器选择**：
```python
# Python性能优化配置类
class PythonPerformanceConfig:
    """Python性能优化配置类，用于管理性能相关参数"""
    
    # 内存管理配置
    MAX_MEMORY_USAGE = 4 * 1024 * 1024 * 1024  # 最大内存使用限制（4GB）
    GC_THRESHOLD_PERCENT = 80  # 垃圾回收触发阈值（80%）
    DISABLE_AUTO_GC = False  # 是否禁用自动垃圾回收
    
    # 线程池配置
    DEFAULT_THREAD_POOL_SIZE = 4  # 默认线程池大小
    IO_INTENSIVE_THREAD_POOL_SIZE = 16  # IO密集型任务线程池大小
    CPU_INTENSIVE_THREAD_POOL_SIZE = 8  # CPU密集型任务线程池大小
    
    # 缓存配置
    DEFAULT_CACHE_SIZE = 10000  # 默认缓存大小
    CACHE_TTL = 3600  # 缓存过期时间（秒）
    
    # 内存监控配置
    MEMORY_MONITOR_INTERVAL = 60  # 内存监控间隔（秒）
    MEMORY_ALERT_THRESHOLD = 90  # 内存告警阈值（90%）
    
    # 性能采样配置
    PROFILER_SAMPLE_INTERVAL = 10  # 性能采样间隔（毫秒）
    PROFILER_ENABLED = False  # 是否启用性能分析
    
    @classmethod
    def get_memory_limit(cls):
        """获取内存使用限制"""
        return cls.MAX_MEMORY_USAGE
    
    @classmethod
    def get_gc_threshold(cls):
        """获取垃圾回收触发阈值"""
        return cls.GC_THRESHOLD_PERCENT
    
    @classmethod
    def should_disable_auto_gc(cls):
        """判断是否应该禁用自动垃圾回收"""
        return cls.DISABLE_AUTO_GC


# 使用示例
if __name__ == "__main__":
    print(f"最大内存使用限制: {PythonPerformanceConfig.get_memory_limit() / (1024*1024):.0f}MB")
    print(f"垃圾回收触发阈值: {PythonPerformanceConfig.get_gc_threshold()}%")
    print(f"是否禁用自动GC: {PythonPerformanceConfig.should_disable_auto_gc()}")
    print(f"默认线程池大小: {PythonPerformanceConfig.DEFAULT_THREAD_POOL_SIZE}")
    print(f"IO密集型线程池大小: {PythonPerformanceConfig.IO_INTENSIVE_THREAD_POOL_SIZE}")
    print(f"CPU密集型线程池大小: {PythonPerformanceConfig.CPU_INTENSIVE_THREAD_POOL_SIZE}")
```

**内存泄漏检测**：
```python
# Python内存泄漏监控器
import psutil
import gc
import threading
import logging
import time
from typing import Optional

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('MemoryLeakDetector')

class MemoryLeakDetector:
    """内存泄漏检测器，定期监控内存使用情况"""
    
    def __init__(self, check_interval: int = 60, warning_threshold: float = 80.0):
        """
        初始化内存泄漏检测器
        
        参数:
            check_interval: 检查间隔（秒）
            warning_threshold: 警告阈值（百分比）
        """
        self.check_interval = check_interval
        self.warning_threshold = warning_threshold
        self.running = False
        self.monitor_thread: Optional[threading.Thread] = None
    
    def start_monitoring(self):
        """启动内存监控"""
        if self.running:
            logger.warning("内存监控已经在运行")
            return
        
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        logger.info(f"内存监控已启动，检查间隔: {self.check_interval}秒")
    
    def stop_monitoring(self):
        """停止内存监控"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("内存监控已停止")
    
    def _monitoring_loop(self):
        """监控循环"""
        while self.running:
            self.check_memory_usage()
            time.sleep(self.check_interval)
    
    def check_memory_usage(self):
        """检查内存使用情况"""
        try:
            # 获取当前进程的内存使用情况
            process = psutil.Process()
            memory_info = process.memory_info()
            
            # 计算内存使用率
            used_memory = memory_info.rss  # 实际使用的物理内存（单位：字节）
            total_memory = psutil.virtual_memory().total  # 系统总内存
            usage_percent = (used_memory / total_memory) * 100
            
            logger.info("内存使用率: {:.2f}% ({}MB / {}MB)".format(
                usage_percent,
                used_memory // (1024 * 1024),
                total_memory // (1024 * 1024)
            ))
            
            # 当内存使用率达到警告阈值时发出警告
            if usage_percent > self.warning_threshold:
                logger.warn("内存使用率过高: {:.2f}%".format(usage_percent))
                
                # 手动触发垃圾回收
                logger.info("手动触发垃圾回收")
                collected = gc.collect()
                logger.info(f"垃圾回收完成，回收了 {collected} 个对象")
                
                # 检查对象引用数量
                self.check_object_references()
            
        except Exception as e:
            logger.error(f"内存检查失败: {str(e)}")
    
    def check_object_references(self):
        """检查对象引用数量，检测可能的内存泄漏"""
        try:
            # 获取当前垃圾回收对象统计
            gc_stats = gc.get_stats()
            logger.info(f"垃圾回收统计: {gc_stats}")
            
            # 获取对象引用数量
            objects = {}  # 统计不同类型对象的数量
            
            for obj in gc.get_objects():
                obj_type = type(obj).__name__
                objects[obj_type] = objects.get(obj_type, 0) + 1
            
            # 显示前10种最常见的对象类型
            top_objects = sorted(objects.items(), key=lambda x: x[1], reverse=True)[:10]
            logger.info("最常见的对象类型:")
            for obj_type, count in top_objects:
                logger.info(f"  {obj_type}: {count}个对象")
                
        except Exception as e:
            logger.error(f"对象引用检查失败: {str(e)}")


# 使用示例
if __name__ == "__main__":
    # 创建内存泄漏检测器
    detector = MemoryLeakDetector(check_interval=10, warning_threshold=80.0)
    
    # 启动监控
    detector.start_monitoring()
    
    # 运行一段时间
    try:
        logger.info("内存监控正在运行，按Ctrl+C停止...")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("停止内存监控")
        detector.stop_monitoring()
```

### GC日志分析

**GC日志配置**：
```bash
# 详细的GC日志配置
java -Xms4g -Xmx4g \
     -XX:+UseG1GC \
     -XX:+PrintGCDetails \
     -XX:+PrintGCTimeStamps \
     -XX:+PrintGCApplicationStoppedTime \
     -XX:+PrintReferenceGC \
     -XX:+PrintAdaptiveSizePolicy \
     -XX:+PrintHeapAtGC \
     -Xloggc:gc.log \
     -XX:NumberOfGCLogFiles=10 \
     -XX:GCLogFileSize=100M \
     app.jar
```

**GC性能分析**：
```python
# Python GC性能监控
import gc
import threading
import time
import logging
from typing import List, Dict, Tuple
from dataclasses import dataclass

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('GCPerformanceMonitor')


@dataclass
class GCEvent:
    """GC事件数据类"""
    gc_type: str  # GC类型
    count: int    # GC次数
    total_time: float  # 总耗时（秒）
    average_time: float  # 平均耗时（秒）
    timestamp: float  # 时间戳
    memory_before: int  # 收集前内存（字节）
    memory_after: int   # 收集后内存（字节）
    memory_freed: int   # 释放内存（字节）


class GCPerformanceMonitor:
    """GC性能监控器"""
    
    def __init__(self, interval: float = 10.0):
        """
        初始化GC性能监控器
        
        Args:
            interval: 监控间隔（秒）
        """
        self.interval = interval
        self.gc_events: List[GCEvent] = []
        self.monitoring_thread: Optional[threading.Thread] = None
        self.stop_event = threading.Event()
        
        # 记录初始GC统计
        self.last_gc_stats = self._get_gc_stats()
    
    def _get_gc_stats(self) -> Tuple[int, float, int]:
        """
        获取当前GC统计信息
        
        Returns:
            (总收集次数, 总收集时间, 当前内存使用)
        """
        gc_stats = gc.get_stats()
        total_collections = sum(stat['collections'] for stat in gc_stats)
        total_time = sum(stat['collection_time'] / 1000.0 for stat in gc_stats)  # 转换为秒
        current_memory = gc.get_total_allocated() - gc.get_total_freed()  # 当前已分配内存
        
        return total_collections, total_time, current_memory
    
    def _monitor_gc(self):
        """
        GC监控主循环
        """
        while not self.stop_event.is_set():
            time.sleep(self.interval)
            
            try:
                current_gc_stats = self._get_gc_stats()
                
                # 计算GC统计差异
                collections_diff = current_gc_stats[0] - self.last_gc_stats[0]
                time_diff = current_gc_stats[1] - self.last_gc_stats[1]
                memory_diff = self.last_gc_stats[2] - current_gc_stats[2]
                
                if collections_diff > 0:
                    # 记录GC事件
                    gc_event = GCEvent(
                        gc_type="Python GC",
                        count=collections_diff,
                        total_time=time_diff,
                        average_time=time_diff / collections_diff if collections_diff > 0 else 0,
                        timestamp=time.time(),
                        memory_before=self.last_gc_stats[2],
                        memory_after=current_gc_stats[2],
                        memory_freed=memory_diff if memory_diff > 0 else 0
                    )
                    
                    self.gc_events.append(gc_event)
                    
                    # 输出GC统计信息
                    logger.info(f"GC统计: {gc_event.gc_type} - \
                                次数: {gc_event.count}, \
                                总耗时: {gc_event.total_time:.3f}秒, \
                                平均耗时: {gc_event.average_time:.3f}秒, \
                                释放内存: {gc_event.memory_freed / (1024 * 1024):.2f} MB")
                
                # 更新上次统计
                self.last_gc_stats = current_gc_stats
                
            except Exception as e:
                logger.error(f"GC监控出错: {e}")
    
    def start_monitoring(self):
        """
        启动GC监控
        """
        if not self.monitoring_thread or not self.monitoring_thread.is_alive():
            self.stop_event.clear()
            self.monitoring_thread = threading.Thread(target=self._monitor_gc, daemon=True)
            self.monitoring_thread.start()
            logger.info(f"GC性能监控已启动，监控间隔: {self.interval}秒")
    
    def stop_monitoring(self):
        """
        停止GC监控
        """
        self.stop_event.set()
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=1.0)
            logger.info("GC性能监控已停止")
    
    def get_gc_events(self) -> List[GCEvent]:
        """
        获取收集的GC事件
        
        Returns:
            GC事件列表
        """
        return list(self.gc_events)
    
    def get_summary(self) -> Dict[str, any]:
        """
        获取GC性能摘要
        
        Returns:
            性能摘要字典
        """
        if not self.gc_events:
            return {
                "total_events": 0,
                "total_collections": 0,
                "total_time": 0.0,
                "average_time_per_collection": 0.0,
                "total_memory_freed": 0
            }
        
        total_collections = sum(event.count for event in self.gc_events)
        total_time = sum(event.total_time for event in self.gc_events)
        total_memory_freed = sum(event.memory_freed for event in self.gc_events)
        
        return {
            "total_events": len(self.gc_events),
            "total_collections": total_collections,
            "total_time": total_time,
            "average_time_per_collection": total_time / total_collections if total_collections > 0 else 0.0,
            "total_memory_freed": total_memory_freed
        }


# 使用示例
if __name__ == "__main__":
    # 启用GC调试
    gc.set_debug(gc.DEBUG_STATS)
    
    # 创建并启动GC监控器
    gc_monitor = GCPerformanceMonitor(interval=5.0)  # 每5秒监控一次
    gc_monitor.start_monitoring()
    
    try:
        # 模拟一些内存分配和GC活动
        logger.info("开始模拟内存分配和GC活动...")
        
        # 分配大量小对象
        for _ in range(1000000):
            obj = [1, 2, 3, 4, 5] * 10
            if _ % 100000 == 0:
                logger.info(f"已分配 {_} 个对象")
                time.sleep(0.1)
        
        # 强制GC
        logger.info("执行强制GC...")
        gc.collect()
        
        time.sleep(6.0)  # 等待监控器捕获GC事件
        
        # 获取并打印摘要
        summary = gc_monitor.get_summary()
        logger.info(f"GC性能摘要: {summary}")
        
    except KeyboardInterrupt:
        logger.info("收到停止信号...")
    finally:
        # 停止监控
        gc_monitor.stop_monitoring()
    
    logger.info("程序结束")
```

**GC性能监控最佳实践**：
- 定期监控GC活动，及时发现内存泄漏和性能问题
- 设置合理的监控间隔，避免监控本身成为性能瓶颈
- 结合内存使用情况分析GC效率
- 对GC频繁或耗时过长的场景进行优化，如减少对象创建、使用对象池等

```

## 网络性能优化

### 连接池优化

**HTTP连接池**：
```python
# Python HTTP连接池配置
import httpx
import logging
from typing import Optional, Dict

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('HttpClientConfig')


class HttpClientConfig:
    """HTTP客户端配置类"""
    
    def __init__(self, 
                 max_connections: int = 200, 
                 max_connections_per_route: int = 50, 
                 connect_timeout: float = 5.0, 
                 read_timeout: float = 10.0, 
                 pool_timeout: float = 2.0, 
                 keep_alive: float = 30.0,
                 user_agent: str = "YourApp/1.0"):
        """
        初始化HTTP客户端配置
        
        Args:
            max_connections: 最大连接数
            max_connections_per_route: 每个路由最大连接数
            connect_timeout: 连接超时时间（秒）
            read_timeout: 读取超时时间（秒）
            pool_timeout: 获取连接超时时间（秒）
            keep_alive: 连接保持时间（秒）
            user_agent: 用户代理
        """
        self.max_connections = max_connections
        self.max_connections_per_route = max_connections_per_route
        self.connect_timeout = connect_timeout
        self.read_timeout = read_timeout
        self.pool_timeout = pool_timeout
        self.keep_alive = keep_alive
        self.user_agent = user_agent
    
    def create_client(self, base_url: Optional[str] = None) -> httpx.Client:
        """
        创建并配置HTTP客户端
        
        Args:
            base_url: 基础URL
        
        Returns:
            httpx.Client: 配置好的HTTP客户端
        """
        # 创建连接池配置
        limits = httpx.Limits(
            max_connections=self.max_connections,
            max_keepalive_connections=self.max_connections_per_route,
            keepalive_expiry=self.keep_alive
        )
        
        # 创建超时配置
        timeout = httpx.Timeout(
            connect=self.connect_timeout,
            read=self.read_timeout,
            write=self.read_timeout,
            pool=self.pool_timeout
        )
        
        # 创建客户端
        client = httpx.Client(
            base_url=base_url,
            limits=limits,
            timeout=timeout,
            headers={
                "User-Agent": self.user_agent
            },
            follow_redirects=True
        )
        
        logger.info(f"创建HTTP客户端: {base_url if base_url else '无基础URL'}")
        logger.debug(f"客户端配置: 最大连接数={self.max_connections}, \
                     每个路由最大连接数={self.max_connections_per_route}, \
                     连接超时={self.connect_timeout}s, \
                     读取超时={self.read_timeout}s")
        
        return client
    
    def create_async_client(self, base_url: Optional[str] = None) -> httpx.AsyncClient:
        """
        创建并配置异步HTTP客户端
        
        Args:
            base_url: 基础URL
        
        Returns:
            httpx.AsyncClient: 配置好的异步HTTP客户端
        """
        # 创建连接池配置
        limits = httpx.Limits(
            max_connections=self.max_connections,
            max_keepalive_connections=self.max_connections_per_route,
            keepalive_expiry=self.keep_alive
        )
        
        # 创建超时配置
        timeout = httpx.Timeout(
            connect=self.connect_timeout,
            read=self.read_timeout,
            write=self.read_timeout,
            pool=self.pool_timeout
        )
        
        # 创建异步客户端
        async_client = httpx.AsyncClient(
            base_url=base_url,
            limits=limits,
            timeout=timeout,
            headers={
                "User-Agent": self.user_agent
            },
            follow_redirects=True
        )
        
        logger.info(f"创建异步HTTP客户端: {base_url if base_url else '无基础URL'}")
        logger.debug(f"客户端配置: 最大连接数={self.max_connections}, \
                     每个路由最大连接数={self.max_connections_per_route}, \
                     连接超时={self.connect_timeout}s, \
                     读取超时={self.read_timeout}s")
        
        return async_client


# 使用示例
if __name__ == "__main__":
    # 创建配置
    config = HttpClientConfig(
        max_connections=100,
        max_connections_per_route=20,
        connect_timeout=3.0,
        read_timeout=8.0
    )
    
    # 创建同步客户端
    client = config.create_client(base_url="https://api.example.com")
    
    try:
        # 发送请求
        response = client.get("/data")
        print(f"同步请求响应: {response.status_code}")
    finally:
        # 关闭客户端
        client.close()
    
    # 创建异步客户端（需要在异步上下文中使用）
    async def async_example():
        async_client = config.create_async_client(base_url="https://api.example.com")
        
        try:
            # 发送异步请求
            response = await async_client.get("/data")
            print(f"异步请求响应: {response.status_code}")
        finally:
            # 关闭异步客户端
            await async_client.aclose()
    
    # 运行异步示例
    import asyncio
    asyncio.run(async_example())
```

**HTTP连接池最佳实践**：
- 根据系统负载和网络环境设置合理的最大连接数
- 每个路由的最大连接数应根据目标服务的承载能力调整
- 连接超时和读取超时应根据实际业务场景设置，避免过长或过短
- 定期验证连接有效性，避免使用无效连接
- 优先使用异步客户端处理高并发请求场景
```

**数据库连接池监控**：
```python
# Python数据库连接池监控
from typing import Dict, Any
import logging
from prometheus_client import Gauge, CollectorRegistry
import threading

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('DatabasePoolMonitor')

# 创建Prometheus指标
class DatabasePoolMetrics:
    """数据库连接池指标收集器"""
    
    def __init__(self, registry: CollectorRegistry = None):
        self.registry = registry or CollectorRegistry()
        
        # 连接池活跃连接数指标
        self.active_connections = Gauge(
            'db_connections_active',
            '当前活跃的数据库连接数',
            ['pool_name'],
            registry=self.registry
        )
        
        # 连接池空闲连接数指标
        self.idle_connections = Gauge(
            'db_connections_idle',
            '当前空闲的数据库连接数',
            ['pool_name'],
            registry=self.registry
        )
        
        # 连接池等待连接数指标
        self.waiting_connections = Gauge(
            'db_connections_waiting',
            '正在等待数据库连接的线程数',
            ['pool_name'],
            registry=self.registry
        )
        
        # 连接池最大连接数指标
        self.max_connections = Gauge(
            'db_connections_max',
            '数据库连接池的最大连接数',
            ['pool_name'],
            registry=self.registry
        )
        
        # 连接池总连接数指标
        self.total_connections = Gauge(
            'db_connections_total',
            '数据库连接池的总连接数',
            ['pool_name'],
            registry=self.registry
        )


# 数据库连接池监控器
class DatabasePoolMonitor:
    """数据库连接池监控器"""
    
    def __init__(self, metrics: DatabasePoolMetrics):
        self.metrics = metrics
        self.pool_stats = {}
    
    def handle_connection_pool_stats(self, event: Dict[str, Any]):
        """处理连接池统计事件"""
        pool_name = event.get('pool_name', 'default')
        
        # 更新连接池指标
        self.metrics.active_connections.labels(pool_name).set(event.get('active_connections', 0))
        self.metrics.idle_connections.labels(pool_name).set(event.get('idle_connections', 0))
        self.metrics.waiting_connections.labels(pool_name).set(event.get('threads_awaiting_connection', 0))
        self.metrics.max_connections.labels(pool_name).set(event.get('max_connections', 0))
        self.metrics.total_connections.labels(pool_name).set(event.get('total_connections', 0))
        
        # 记录连接池状态
        self.pool_stats[pool_name] = {
            'active': event.get('active_connections', 0),
            'idle': event.get('idle_connections', 0),
            'waiting': event.get('threads_awaiting_connection', 0),
            'max': event.get('max_connections', 0),
            'total': event.get('total_connections', 0)
        }
        
        logger.info(f"连接池 {pool_name} 统计: {self.pool_stats[pool_name]}")
    
    def get_pool_stats(self, pool_name: str = 'default') -> Dict[str, int]:
        """获取连接池统计信息"""
        return self.pool_stats.get(pool_name, {})


# 使用示例
if __name__ == "__main__":
    # 创建指标收集器
    metrics = DatabasePoolMetrics()
    
    # 创建监控器
    monitor = DatabasePoolMonitor(metrics)
    
    # 模拟连接池统计事件
    pool_event = {
        'pool_name': 'primary_db_pool',
        'active_connections': 5,
        'idle_connections': 10,
        'threads_awaiting_connection': 2,
        'max_connections': 20,
        'total_connections': 15
    }
    
    # 处理事件
    monitor.handle_connection_pool_stats(pool_event)
    
    # 获取连接池统计信息
    stats = monitor.get_pool_stats('primary_db_pool')
    logger.info(f"获取连接池统计: {stats}")
```

### 网络I/O优化

**异步I/O模型**：
```python
# Python异步HTTP服务器示例
import asyncio
from aiohttp import web
import logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('AsyncHttpServer')


async def handle_request(request):
    """异步处理HTTP请求"""
    # 从请求中获取路径参数或查询参数
    path = request.path
    query = request.query
    
    # 异步处理业务逻辑（模拟耗时操作）
    await asyncio.sleep(0.1)  # 模拟IO等待
    
    # 构建响应
    response_data = {
        'message': 'Hello from async server',
        'path': path,
        'query': dict(query),
        'status': 'success'
    }
    
    logger.info(f"处理请求: {path}，查询参数: {dict(query)}")
    
    return web.json_response(response_data)


async def handle_heavy_request(request):
    """处理耗时的请求"""
    logger.info(f"开始处理耗时请求: {request.path}")
    
    # 执行CPU密集型任务（使用线程池）
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None,  # 使用默认的线程池
        lambda: heavy_computation()
    )
    
    logger.info(f"耗时请求处理完成: {request.path}")
    
    return web.json_response({
        'message': 'Heavy computation completed',
        'result': result
    })


def heavy_computation():
    """模拟CPU密集型计算"""
    import time
    time.sleep(2)  # 模拟长时间CPU计算
    return {
        'value': 42,
        'timestamp': time.time()
    }


async def create_server(host='0.0.0.0', port=8080):
    """创建异步HTTP服务器"""
    # 创建应用实例
    app = web.Application()
    
    # 注册路由
    app.add_routes([
        web.get('/', handle_request),
        web.get('/api/data', handle_request),
        web.post('/api/data', handle_request),
        web.get('/api/heavy', handle_heavy_request),
    ])
    
    # 创建服务器
    runner = web.AppRunner(app)
    await runner.setup()
    
    # 绑定端口
    site = web.TCPSite(runner, host, port)
    
    logger.info(f"异步服务器启动在 http://{host}:{port}")
    
    return runner, site


async def main():
    """主函数"""
    # 创建并启动服务器
    runner, site = await create_server()
    await site.start()
    
    try:
        # 保持服务器运行
        await asyncio.Event().wait()
    except KeyboardInterrupt:
        logger.info("收到停止信号，正在关闭服务器...")
    finally:
        # 优雅关闭服务器
        await runner.cleanup()
        logger.info("服务器已关闭")


# 使用示例
if __name__ == "__main__":
    # 运行异步服务器
    asyncio.run(main())
```

**异步I/O的优势**：
- **高并发处理能力**：单个线程可以处理数千个并发连接
- **资源利用率高**：避免线程上下文切换带来的开销
- **响应式编程模型**：通过async/await语法实现清晰的异步代码结构
- **良好的可扩展性**：可以轻松添加更多请求处理程序和路由

```

## 性能监控与告警

### 关键性能指标（KPI）

**系统性能指标**：
```python
# Python性能指标收集器
from prometheus_client import Counter, Gauge, Histogram, Summary
import time
import logging
from typing import Dict, Any, Callable

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('PerformanceMetricsCollector')


class PerformanceMetricsCollector:
    """性能指标收集器"""
    
    def __init__(self):
        # 初始化性能指标
        self._init_metrics()
    
    def _init_metrics(self):
        """初始化所有性能指标"""
        # HTTP请求指标
        self.http_requests_total = Counter(
            'http_requests_total', 
            'HTTP请求总数',
            ['method', 'endpoint', 'status']
        )
        
        self.http_request_duration_seconds = Histogram(
            'http_request_duration_seconds',
            'HTTP请求响应时间分布',
            ['method', 'endpoint', 'status'],
            buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
        )
        
        self.http_request_size_bytes = Summary(
            'http_request_size_bytes',
            'HTTP请求大小',
            ['method', 'endpoint']
        )
        
        self.http_response_size_bytes = Summary(
            'http_response_size_bytes',
            'HTTP响应大小',
            ['method', 'endpoint', 'status']
        )
        
        # 数据库指标
        self.database_connections_active = Gauge(
            'database_connections_active',
            '数据库活跃连接数'
        )
        
        self.database_operations_total = Counter(
            'database_operations_total',
            '数据库操作总数',
            ['operation', 'table']
        )
        
        self.database_operation_duration_seconds = Histogram(
            'database_operation_duration_seconds',
            '数据库操作耗时分布',
            ['operation', 'table'],
            buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5]
        )
        
        # 系统指标
        self.system_cpu_usage = Gauge(
            'system_cpu_usage',
            '系统CPU使用率'
        )
        
        self.system_memory_usage = Gauge(
            'system_memory_usage',
            '系统内存使用率'
        )
    
    def record_http_request(self, method: str, endpoint: str, status: int, 
                           duration: float, request_size: int = 0, response_size: int = 0):
        """记录HTTP请求指标"""
        self.http_requests_total.labels(method=method, endpoint=endpoint, status=status).inc()
        self.http_request_duration_seconds.labels(method=method, endpoint=endpoint, status=status).observe(duration)
        
        if request_size > 0:
            self.http_request_size_bytes.labels(method=method, endpoint=endpoint).observe(request_size)
            
        if response_size > 0:
            self.http_response_size_bytes.labels(method=method, endpoint=endpoint, status=status).observe(response_size)
        
        logger.debug(f"记录HTTP请求: {method} {endpoint} {status} {duration:.3f}s")
    
    def record_database_operation(self, operation: str, table: str, duration: float):
        """记录数据库操作指标"""
        self.database_operations_total.labels(operation=operation, table=table).inc()
        self.database_operation_duration_seconds.labels(operation=operation, table=table).observe(duration)
        
        logger.debug(f"记录数据库操作: {operation} {table} {duration:.3f}s")
    
    def set_active_connections(self, count: int):
        """更新数据库活跃连接数"""
        self.database_connections_active.set(count)
    
    def update_system_metrics(self, cpu_usage: float, memory_usage: float):
        """更新系统指标"""
        self.system_cpu_usage.set(cpu_usage)
        self.system_memory_usage.set(memory_usage)


# 使用示例
if __name__ == "__main__":
    # 创建指标收集器实例
    metrics_collector = PerformanceMetricsCollector()
    
    # 模拟HTTP请求
    start_time = time.time()
    time.sleep(0.1)  # 模拟请求处理
    duration = time.time() - start_time
    
    metrics_collector.record_http_request(
        method="GET",
        endpoint="/api/users",
        status=200,
        duration=duration,
        request_size=100,
        response_size=1500
    )
    
    # 模拟数据库操作
    start_time = time.time()
    time.sleep(0.05)  # 模拟数据库查询
    duration = time.time() - start_time
    
    metrics_collector.record_database_operation(
        operation="SELECT",
        table="users",
        duration=duration
    )
    
    # 更新系统指标
    metrics_collector.update_system_metrics(cpu_usage=0.75, memory_usage=0.65)
    metrics_collector.set_active_connections(count=8)
    
    logger.info("性能指标记录完成")
```

**性能指标监控最佳实践**：
- **关键指标优先**：重点监控业务核心指标和系统资源指标
- **维度设计合理**：通过标签(TAG)设计多维度指标，便于分析和排查问题
- **数据持久化**：将性能指标数据持久化存储，用于趋势分析和容量规划
- **告警阈值设置**：根据业务需求设置合理的告警阈值，及时发现性能问题    
    }
}
```

**性能指标监控（Python实现）**：
```python
from prometheus_client import Counter, Histogram, Gauge, Summary
import time
import threading
from typing import Dict, Any
import logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('PerformanceMetrics')

class PerformanceMetricsCollector:
    """性能指标收集器"""
    
    def __init__(self):
        # HTTP请求指标
        self.http_requests_total = Counter(
            'http_requests_total', 
            'HTTP请求总数',
            ['method', 'endpoint', 'status_code']
        )
        
        self.http_request_duration_seconds = Histogram(
            'http_request_duration_seconds', 
            'HTTP请求响应时间（秒）',
            ['method', 'endpoint', 'status_code'],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]  # 自定义桶
        )
        
        # 数据库操作指标
        self.database_operations_total = Counter(
            'database_operations_total', 
            '数据库操作总数',
            ['operation', 'status']  # operation: query, insert, update, delete
        )
        
        self.database_operation_duration_seconds = Histogram(
            'database_operation_duration_seconds', 
            '数据库操作耗时（秒）',
            ['operation'],
            buckets=[0.01, 0.05, 0.1, 0.5, 1.0]
        )
        
        # 数据库连接指标
        self.database_connections_active = Gauge(
            'database_connections_active', 
            '数据库活跃连接数'
        )
        
        # 初始化连接数监控
        self._active_connections = 0
        
    def handle_http_request(self, method: str, endpoint: str, status_code: int, duration: float) -> None:
        """处理HTTP请求指标"""
        self.http_requests_total.labels(
            method=method, 
            endpoint=endpoint, 
            status_code=status_code
        ).inc()
        
        self.http_request_duration_seconds.labels(
            method=method, 
            endpoint=endpoint, 
            status_code=status_code
        ).observe(duration)
        
    def record_database_operation(self, operation: str, status: str = "success") -> Any:
        """记录数据库操作指标（装饰器风格）"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                # 增加活跃连接数
                self.database_connections_active.inc()
                
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    self.database_operations_total.labels(operation=operation, status="success").inc()
                    return result
                except Exception as e:
                    logger.error(f"Database operation {operation} failed: {str(e)}")
                    self.database_operations_total.labels(operation=operation, status="failure").inc()
                    raise
                finally:
                    duration = time.time() - start_time
                    self.database_operation_duration_seconds.labels(operation=operation).observe(duration)
                    # 减少活跃连接数
                    self.database_connections_active.dec()
            return wrapper
        return decorator
    
    def set_active_connections(self, count: int) -> None:
        """设置当前活跃的数据库连接数"""
        self._active_connections = count
        self.database_connections_active.set(count)
    
    def get_active_connections(self) -> int:
        """获取当前活跃的数据库连接数"""
        return self._active_connections

# 事件监听器示例
class EventListener:
    """事件监听器基类"""
    def __init__(self, metrics_collector: PerformanceMetricsCollector):
        self.metrics_collector = metrics_collector
    
    def on_event(self, event_type: str, event_data: Dict[str, Any]) -> None:
        """处理事件"""
        raise NotImplementedError("子类必须实现on_event方法")

class HttpRequestListener(EventListener):
    """HTTP请求事件监听器"""
    def on_event(self, event_type: str, event_data: Dict[str, Any]) -> None:
        if event_type == "HTTP_REQUEST":
            self.metrics_collector.handle_http_request(
                method=event_data["method"],
                endpoint=event_data["endpoint"],
                status_code=event_data["status_code"],
                duration=event_data["duration"]
            )
```

**业务性能指标（Python实现）**：
```python
from prometheus_client import Counter, Histogram, Summary
import logging
from typing import Dict, Any
import time

logger = logging.getLogger('BusinessMetrics')

class BusinessMetricsService:
    """业务指标监控服务"""
    
    def __init__(self):
        # 订单相关指标
        self.orders_created_total = Counter(
            'orders_created_total', 
            '创建的订单总数',
            ['status', 'channel']  # status: created, paid, shipped; channel: web, mobile, api
        )
        
        self.order_amount_summary = Summary(
            'order_amount', 
            '订单金额分布'
        )
        
        self.order_processing_duration_seconds = Histogram(
            'order_processing_duration_seconds', 
            '订单处理耗时（秒）'
        )
        
        # 用户相关指标
        self.user_logins_total = Counter(
            'user_logins_total', 
            '用户登录总数',
            ['user_type', 'login_method']  # user_type: registered, guest; login_method: email, phone, social
        )
        
        # 支付相关指标
        self.payment_processing_total = Counter(
            'payment_processing_total', 
            '支付处理总数',
            ['payment_type', 'result']  # payment_type: credit_card, wechat, alipay; result: success, failure
        )
        
        self.payment_amount_summary = Summary(
            'payment_amount', 
            '支付金额分布'
        )
    
    def handle_order_created(self, order_data: Dict[str, Any]) -> None:
        """处理订单创建事件"""
        status = order_data.get("status", "created")
        channel = order_data.get("channel", "unknown")
        amount = order_data.get("amount", 0.0)
        
        # 记录订单创建指标
        self.orders_created_total.labels(status=status, channel=channel).inc()
        self.order_amount_summary.observe(amount)
        
        logger.info(f"Order created: {order_data['order_id']}, status: {status}, channel: {channel}")
    
    def record_order_processing(self, func) -> Any:
        """记录订单处理耗时（装饰器）"""
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            self.order_processing_duration_seconds.observe(duration)
            return result
        return wrapper
    
    def record_user_login(self, user_type: str, login_method: str) -> None:
        """记录用户登录"""
        self.user_logins_total.labels(
            user_type=user_type, 
            login_method=login_method
        ).inc()
        
    def record_payment_processing(self, payment_type: str, success: bool, amount: float = 0.0) -> None:
        """记录支付处理"""
        result = "success" if success else "failure"
        self.payment_processing_total.labels(
            payment_type=payment_type, 
            result=result
        ).inc()
        
        if success and amount > 0:
            self.payment_amount_summary.observe(amount)

# 使用示例
if __name__ == "__main__":
    from prometheus_client import start_http_server
    import time
    
    # 启动Prometheus指标暴露服务器
    start_http_server(8000)
    
    # 创建指标服务实例
    perf_metrics = PerformanceMetricsCollector()
    business_metrics = BusinessMetricsService()
    
    # 模拟HTTP请求
    def simulate_http_request():
        perf_metrics.handle_http_request(
            method="GET",
            endpoint="/api/users",
            status_code=200,
            duration=0.123
        )
    
    # 模拟订单创建
    def simulate_order_created():
        business_metrics.handle_order_created({
            "order_id": "order_12345",
            "status": "created",
            "channel": "web",
            "amount": 99.99
        })
    
    # 持续模拟数据
    while True:
        simulate_http_request()
        simulate_order_created()
        time.sleep(1)  # 每秒模拟一次
```

**Prometheus指标暴露**：
```python
from prometheus_client import make_wsgi_app
from wsgiref.simple_server import make_server
from flask import Flask

# Flask应用集成示例
app = Flask(__name__)

# 注册Prometheus指标端点
@app.route('/metrics')
def metrics():
    return make_wsgi_app()(request.environ, request.start_response)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```
```

### 性能告警策略

**阈值告警**：
```python
# 性能告警配置
import logging
from typing import Dict, Any

logger = logging.getLogger('PerformanceAlert')

# 告警管理器
class AlertManager:
    """告警管理器"""
    
    @staticmethod
    def send_alert(title: str, message: str, severity: str = "WARNING") -> None:
        """发送告警通知"""
        # 实际实现中可以对接邮件、短信、Slack等告警渠道
        logger.info(f"[{severity}] {title}: {message}")
        # 示例：发送邮件告警
        # EmailService.send_alert(title, message, severity)
        

# 性能告警配置类
class PerformanceAlertConfig:
    """性能告警配置"""
    
    def __init__(self):
        # 告警阈值配置
        self.cpu_threshold = 80.0  # CPU使用率阈值(%)
        self.response_time_threshold = 2000  # 响应时间阈值(ms)
        self.error_rate_threshold = 5.0  # 错误率阈值(%)
    
    def handle_high_cpu_usage(self, event: Dict[str, Any]) -> None:
        """处理CPU使用率过高告警"""
        cpu_usage = event.get("cpu_usage", 0.0)
        if cpu_usage > self.cpu_threshold:
            AlertManager.send_alert(
                "CPU使用率过高",
                f"当前CPU使用率: {cpu_usage:.2f}% (阈值: {self.cpu_threshold}%)",
                "CRITICAL"
            )
    
    def handle_slow_response(self, event: Dict[str, Any]) -> None:
        """处理响应时间过慢告警"""
        response_time = event.get("response_time", 0)
        if response_time > self.response_time_threshold:
            AlertManager.send_alert(
                "响应时间过慢",
                f"API响应时间: {response_time}ms (阈值: {self.response_time_threshold}ms)",
                "WARNING"
            )
    
    def handle_high_error_rate(self, event: Dict[str, Any]) -> None:
        """处理错误率过高告警"""
        error_rate = event.get("error_rate", 0.0)
        if error_rate > self.error_rate_threshold:
            AlertManager.send_alert(
                "错误率过高",
                f"错误率: {error_rate:.2f}% (阈值: {self.error_rate_threshold}%)",
                "WARNING"
            )
    
    def process_event(self, event_type: str, event_data: Dict[str, Any]) -> None:
        """处理各类性能事件"""
        handlers = {
            "CPU_USAGE_ALERT": self.handle_high_cpu_usage,
            "SLOW_RESPONSE_ALERT": self.handle_slow_response,
            "HIGH_ERROR_RATE_ALERT": self.handle_high_error_rate
        }
        
        handler = handlers.get(event_type)
        if handler:
            handler(event_data)
        else:
            logger.warning(f"未知的事件类型: {event_type}")


# 使用示例
if __name__ == "__main__":
    alert_config = PerformanceAlertConfig()
    
    # 模拟CPU使用率过高事件
    alert_config.process_event("CPU_USAGE_ALERT", {"cpu_usage": 85.5})
    
    # 模拟响应时间过慢事件
    alert_config.process_event("SLOW_RESPONSE_ALERT", {"response_time": 3500})
    
    # 模拟错误率过高事件
    alert_config.process_event("HIGH_ERROR_RATE_ALERT", {"error_rate": 7.2})
```

**智能告警**：
```python
# 基于机器学习的异常检测
from enum import Enum
from typing import List, Dict, Any
import logging
import numpy as np
from sklearn.ensemble import IsolationForest

logger = logging.getLogger('IntelligentAlert')


class AlertSeverity(Enum):
    """告警严重程度枚举"""
    INFO = "INFO"
    WARNING = "WARNING"
    CRITICAL = "CRITICAL"


class AnomalyPattern:
    """异常模式类"""
    
    def __init__(self, metric_name: str, value: float, expected_value: float, confidence: float):
        self.metric_name = metric_name
        self.value = value
        self.expected_value = expected_value
        self.confidence = confidence


class AnomalyDetector:
    """异常检测类"""
    
    def __init__(self, contamination: float = 0.1):
        # 初始化异常检测模型
        self.model = IsolationForest(contamination=contamination, random_state=42)
        self.is_trained = False
        
    def train(self, training_data: np.ndarray) -> None:
        """训练异常检测模型"""
        self.model.fit(training_data)
        self.is_trained = True
        logger.info("异常检测模型训练完成")
    
    def detect(self, metrics: np.ndarray) -> List[AnomalyPattern]:
        """检测异常模式"""
        if not self.is_trained:
            raise RuntimeError("模型尚未训练，请先调用train方法")
        
        # 检测异常
        predictions = self.model.predict(metrics)
        anomalies = []
        
        for i, (prediction, metric) in enumerate(zip(predictions, metrics)):
            if prediction == -1:  # -1表示异常
                # 简单计算预期值和置信度（实际项目中可能更复杂）
                expected_value = np.mean(metric)
                confidence = 0.95  # 示例置信度
                
                anomaly = AnomalyPattern(
                    metric_name=f"metric_{i}",
                    value=float(metric[-1]),
                    expected_value=float(expected_value),
                    confidence=confidence
                )
                anomalies.append(anomaly)
        
        return anomalies


# 智能告警管理器
class IntelligentAlertManager:
    """智能告警管理器"""
    
    def __init__(self, anomaly_detector: AnomalyDetector):
        self.anomaly_detector = anomaly_detector
    
    def handle_metrics_update(self, event: Dict[str, Any]) -> None:
        """处理指标更新事件"""
        metrics = event.get("metrics", [])
        
        if not metrics:
            logger.warning("没有检测到指标数据")
            return
        
        # 将指标数据转换为numpy数组
        metrics_array = np.array(metrics)
        
        try:
            # 检测异常模式
            anomalies = self.anomaly_detector.detect(metrics_array)
            
            for anomaly in anomalies:
                # 根据异常严重程度决定告警策略
                severity = self.calculate_severity(anomaly)
                
                if severity == AlertSeverity.CRITICAL:
                    self.send_critical_alert(anomaly)
                elif severity == AlertSeverity.WARNING:
                    self.send_warning_alert(anomaly)
                else:
                    self.log_info_alert(anomaly)
                    
        except Exception as e:
            logger.error(f"异常检测失败: {str(e)}")
    
    def calculate_severity(self, anomaly: AnomalyPattern) -> AlertSeverity:
        """计算异常严重程度"""
        # 计算异常偏离程度
        deviation = abs(anomaly.value - anomaly.expected_value) / max(anomaly.expected_value, 1e-9)
        
        if deviation > 0.5 and anomaly.confidence > 0.9:
            return AlertSeverity.CRITICAL
        elif deviation > 0.2 and anomaly.confidence > 0.8:
            return AlertSeverity.WARNING
        else:
            return AlertSeverity.INFO
    
    def send_critical_alert(self, anomaly: AnomalyPattern) -> None:
        """发送严重告警"""
        title = f"关键指标异常: {anomaly.metric_name}"
        message = (f"指标值: {anomaly.value:.2f}, "
                  f"预期值: {anomaly.expected_value:.2f}, "
                  f"偏离度: {(abs(anomaly.value - anomaly.expected_value)/anomaly.expected_value*100):.2f}%, "
                  f"置信度: {anomaly.confidence:.2f}")
        AlertManager.send_alert(title, message, "CRITICAL")
    
    def send_warning_alert(self, anomaly: AnomalyPattern) -> None:
        """发送警告告警"""
        title = f"指标异常: {anomaly.metric_name}"
        message = (f"指标值: {anomaly.value:.2f}, "
                  f"预期值: {anomaly.expected_value:.2f}, "
                  f"偏离度: {(abs(anomaly.value - anomaly.expected_value)/anomaly.expected_value*100):.2f}%, "
                  f"置信度: {anomaly.confidence:.2f}")
        AlertManager.send_alert(title, message, "WARNING")
    
    def log_info_alert(self, anomaly: AnomalyPattern) -> None:
        """记录信息级别的异常"""
        title = f"指标异常 (INFO): {anomaly.metric_name}"
        message = (f"指标值: {anomaly.value:.2f}, "
                  f"预期值: {anomaly.expected_value:.2f}, "
                  f"偏离度: {(abs(anomaly.value - anomaly.expected_value)/anomaly.expected_value*100):.2f}%, "
                  f"置信度: {anomaly.confidence:.2f}")
        logger.info(f"[INFO] {title}: {message}")


# 使用示例
if __name__ == "__main__":
    # 创建异常检测器
    detector = AnomalyDetector(contamination=0.1)
    
    # 训练数据（模拟历史指标）
    training_data = np.random.normal(0, 1, size=(1000, 5))  # 1000个样本，5个指标
    detector.train(training_data)
    
    # 创建智能告警管理器
    alert_manager = IntelligentAlertManager(detector)
    
    # 模拟指标更新事件
    # 正常指标
    normal_metrics = np.random.normal(0, 1, size=(10, 5))
    # 异常指标（最后一个指标异常高）
    anomaly_metrics = np.random.normal(0, 1, size=(10, 5))
    anomaly_metrics[-1, -1] = 10  # 最后一个指标设置为异常值
    
    # 发送指标更新事件
    alert_manager.handle_metrics_update({
        "metrics": anomaly_metrics
    })
    }


# 扩展异常检测功能：基于历史数据的智能告警

# 更新AnomalyPattern类，添加score和historical_trend属性
class EnhancedAnomalyPattern(AnomalyPattern):
    """扩展的异常模式类，包含历史趋势信息"""
    
    def __init__(self, metric_name: str, value: float, expected_value: float, confidence: float, score: float, historical_trend: float):
        super().__init__(metric_name, value, expected_value, confidence)
        self.score = score  # 异常分数
        self.historical_trend = historical_trend  # 历史趋势分数


# 更新智能告警管理器，添加基于历史数据的严重程度计算
class EnhancedIntelligentAlertManager(IntelligentAlertManager):
    """增强的智能告警管理器，支持基于历史数据的告警"""
    
    def calculate_severity(self, anomaly: EnhancedAnomalyPattern) -> AlertSeverity:
        """基于历史数据计算异常严重程度"""
        # 使用异常分数和历史趋势计算严重程度
        if anomaly.score > 0.9 and anomaly.historical_trend > 0.8:
            return AlertSeverity.CRITICAL
        elif anomaly.score > 0.7:
            return AlertSeverity.WARNING
        else:
            return AlertSeverity.INFO


# 使用示例
if __name__ == "__main__":
    # 创建异常检测器
    detector = AnomalyDetector(contamination=0.1)
    
    # 训练数据（模拟历史指标）
    training_data = np.random.normal(0, 1, size=(1000, 5))
    detector.train(training_data)
    
    # 创建增强型智能告警管理器
    alert_manager = EnhancedIntelligentAlertManager(detector)
    
    # 创建包含历史趋势信息的异常模式
    enhanced_anomaly = EnhancedAnomalyPattern(
        metric_name="cpu_usage",
        value=95.5,
        expected_value=45.0,
        confidence=0.98,
        score=0.95,
        historical_trend=0.85
    )
    
    # 计算严重程度
    severity = alert_manager.calculate_severity(enhanced_anomaly)
    print(f"异常严重程度: {severity.value}")
```

## 性能优化最佳实践

### 性能优化方法论

**性能优化的5W1H原则**：

1. **What（什么）**：识别性能问题
   - 用户体验：页面加载慢、API响应慢
   - 系统指标：CPU高、内存泄露、I/O阻塞

2. **Why（为什么）**：分析性能问题的原因
   - 资源不足：CPU、内存、网络、存储
   - 算法复杂度：O(n²)算法在小数据量时运行慢
   - 数据库设计：缺少索引、查询语句优化

3. **When（什么时候）**：确定性能问题的时机
   - 峰值时段：业务高峰期
   - 特定操作：某些特定的业务操作
   - 环境变化：部署、配置变更后

4. **Where（在哪里）**：定位性能问题的位置
   - 代码层面：CPU热点、内存泄露、I/O操作
   - 系统层面：CPU、内存、磁盘、网络
   - 网络层面：延迟、带宽、丢包

5. **Who（谁）**：确定负责的人
   - 开发团队：代码优化、算法改进
   - 运维团队：系统配置、资源分配
   - 架构师：设计优化、架构调整

6. **How（如何）**：制定性能优化方案
   - 短期措施：快速解决问题
   - 中期措施：系统优化和改进
   - 长期措施：架构重构和升级

### 性能优化清单

**应用层优化**：
- [ ] 代码优化：算法复杂度、数据结构选择
- [ ] 并发优化：线程池、锁优化、无锁编程
- [ ] 缓存策略：多级缓存、缓存预热、缓存更新
- [ ] 异步处理：消息队列、事件驱动、异步I/O

**数据层优化**：
- [ ] 数据库优化：索引设计、查询优化、分库分表
- [ ] 连接池优化：参数调优、连接复用
- [ ] 数据访问优化：ORM配置、批量操作、数据预取

**系统层优化**：
- [ ] JVM调优：垃圾回收器选择、内存参数、GC优化
- [ ] 操作系统优化：内核参数、文件描述符、网络参数
- [ ] 资源监控：CPU、内存、磁盘、网络监控

**网络层优化**：
- [ ] 连接池：HTTP连接池、数据库连接池
- [ ] 网络I/O：异步I/O、零拷贝、连接复用
- [ ] 协议优化：HTTP/2、gRPC、协议压缩

### 性能优化的反模式

**过早优化**：
```python
# 不好的做法：过早优化
class UnnecessaryOptimization:
    def __init__(self):
        # 过度复杂的优化，可能降低代码可读性
        self.cache = [0] * 1000  # 预分配缓存数组（实际未使用）
        self.complex_cache = {}  # 复杂的缓存结构
    
    def calculate_factorial(self, n):
        # 过度复杂的缓存策略
        key = f"factorial_{n}"
        if key in self.complex_cache:
            return self.complex_cache[key]
        
        result = self.factorial(n)
        self.complex_cache[key] = result
        return result
    
    def factorial(self, n):
        # 简单清晰的递归实现
        return 1 if n <= 1 else n * self.factorial(n - 1)


# 好的做法：根据实际需求进行优化
class ReasonableOptimization:
    def __init__(self):
        # 只在必要时使用缓存
        self._factorial_cache = {0: 1, 1: 1}  # 基础情况的缓存
    
    def calculate_factorial(self, n):
        # 对于小数据量，简单清晰的实现已经足够
        # 注意：Python默认递归深度限制（约1000）
        return 1 if n <= 1 else n * self.calculate_factorial(n - 1)
    
    # 只在性能测试发现问题后再进行优化
    def calculate_factorial_optimized(self, n):
        # 使用更高效的迭代实现并缓存结果
        if n in self._factorial_cache:
            return self._factorial_cache[n]
            
        result = 1
        for i in range(2, n + 1):
            result *= i
            self._factorial_cache[i] = result
        
        return result


# 使用示例
if __name__ == "__main__":
    # 演示过早优化的问题
    unnecessary = UnnecessaryOptimization()
    print(f"过早优化实现的5! = {unnecessary.calculate_factorial(5)}")
    
    # 演示合理的优化
    reasonable = ReasonableOptimization()
    print(f"简单实现的5! = {reasonable.calculate_factorial(5)}")
    print(f"优化实现的5! = {reasonable.calculate_factorial_optimized(5)}")
    
    # 性能比较（对于较大的数值）
    import time
    
    start_time = time.time()
    result = reasonable.calculate_factorial_optimized(100)
    print(f"优化实现计算100!耗时: {time.time() - start_time:.6f}秒")
    
    # 注意：直接递归计算100!会导致栈溢出
    # result = reasonable.calculate_factorial(100)  # 会抛出RecursionError
```

**过早优化的危害**：
- **降低代码可读性**：过度复杂的优化使代码难以理解和维护
- **浪费开发时间**：在不需要优化的地方花费时间和精力
- **引入潜在bug**：复杂的优化可能引入难以发现的错误
- **阻碍后续优化**：过早的优化可能限制了更好的优化方案

**优化时机的判断**：
1. **先确保功能正确**：在优化之前，确保代码功能完整且正确
2. **进行性能测试**：通过基准测试和性能分析识别真正的瓶颈
3. **基于数据决策**：只有在数据表明存在性能问题时才进行优化
4. **保持代码清晰**：优化后的代码仍应保持良好的可读性和可维护性

**过度缓存**：
```python
// 不好的做法：过度缓存
import time
import threading
from typing import Dict, Any, Optional
from functools import lru_cache


# 不好的做法：过度缓存
class OverCachingProblem:
    def __init__(self):
        # 使用线程安全的字典作为缓存
        self.huge_cache: Dict[str, Any] = {}  # 可以使用 threading.Lock 确保线程安全
        self.cache_lock = threading.Lock()
    
    def cache_everything(self):
        """缓存所有数据，包括不经常访问的数据"""
        with self.cache_lock:
            # 缓存不经常访问的数据
            self.huge_cache["system_start_time"] = time.time()
            self.huge_cache["app_version"] = "1.0.0"
            self.huge_cache["database_config"] = self.get_database_config()
    
    def get_database_config(self):
        """获取数据库配置（示例方法）"""
        # 模拟从配置文件读取
        return {"host": "localhost", "port": 3306, "db": "app_db"}


# 好的做法：合理选择缓存内容
class ReasonableCaching:
    def __init__(self, max_size: int = 1000, expire_minutes: int = 30):
        """
        初始化合理的缓存
        
        Args:
            max_size: 最大缓存条目数
            expire_minutes: 缓存过期时间（分钟）
        """
        # 使用Python内置的lru_cache装饰器或使用第三方库如cachetools
        # 这里使用cachetools实现带过期时间的缓存
        from cachetools import TTLCache
        
        self.application_cache = TTLCache(
            maxsize=max_size, 
            ttl=expire_minutes * 60  # 转换为秒
        )
    
    def cache_expensive_operations(self):
        """只缓存真正昂贵的操作"""
        key = "expensive_calculation"
        
        if key not in self.application_cache:
            # 执行昂贵的计算并缓存结果
            result = self.perform_expensive_calculation()
            self.application_cache[key] = result
        else:
            # 从缓存获取结果
            result = self.application_cache[key]
        
        return result
    
    def perform_expensive_calculation(self) -> str:
        """执行昂贵的计算（示例方法）"""
        # 模拟耗时计算
        time.sleep(2)  # 模拟2秒的计算时间
        return "expensive_calculation_result"


# 使用示例
if __name__ == "__main__":
    # 演示过度缓存的问题
    over_cache = OverCachingProblem()
    over_cache.cache_everything()
    print(f"过度缓存的内容: {list(over_cache.huge_cache.keys())}")
    
    # 演示合理缓存的做法
    reasonable_cache = ReasonableCaching()
    
    # 第一次调用：执行昂贵计算
    start_time = time.time()
    result = reasonable_cache.cache_expensive_operations()
    print(f"第一次调用耗时: {time.time() - start_time:.2f}秒，结果: {result}")
    
    # 第二次调用：从缓存获取
    start_time = time.time()
    result = reasonable_cache.cache_expensive_operations()
    print(f"第二次调用耗时: {time.time() - start_time:.2f}秒，结果: {result}")
```

**盲目的微服务化**：
```python
# 不好的做法：过度微服务化
# 原本简单的逻辑被拆分成多个微服务
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

# 服务URL配置（模拟多个微服务）
SERVICES_CONFIG = {
    'name_validation': 'http://name-validation-service:8080/validate',
    'email_validation': 'http://email-validation-service:8080/validate',
    'age_calculation': 'http://age-calculation-service:8080/calculate'
}

@app.route('/users', methods=['POST'])
def create_user():
    """创建用户（过度微服务化的实现）"""
    request_data = request.get_json()
    
    try:
        # 简单的用户创建变成了多个微服务调用
        # 调用名称验证服务
        name_response = requests.post(
            SERVICES_CONFIG['name_validation'],
            json={'name': request_data.get('name')}
        )
        name_response.raise_for_status()  # 检查请求是否成功
        
        # 调用邮箱验证服务
        email_response = requests.post(
            SERVICES_CONFIG['email_validation'],
            json={'email': request_data.get('email')}
        )
        email_response.raise_for_status()  # 检查请求是否成功
        
        # 调用年龄计算服务
        age_response = requests.post(
            SERVICES_CONFIG['age_calculation'],
            json={'birth_date': request_data.get('birth_date')}
        )
        age_response.raise_for_status()  # 检查请求是否成功
        age = age_response.json().get('age')
        
        # 创建用户（这里仅模拟）
        user = {
            'id': 'user-123',
            'name': request_data.get('name'),
            'email': request_data.get('email'),
            'age': age
        }
        
        return jsonify(user), 201
        
    except requests.exceptions.RequestException as e:
        # 处理微服务调用失败
        return jsonify({'error': f'微服务调用失败: {str(e)}'}), 503


# 好的做法：合理的服务划分
class UserValidator:
    """用户验证器"""
    
    def validate_user(self, user_data):
        """验证用户数据"""
        errors = []
        
        # 名称验证
        name = user_data.get('name')
        if not name or len(name) < 2:
            errors.append('名称长度至少为2个字符')
        
        # 邮箱验证
        email = user_data.get('email')
        if not email or '@' not in email:
            errors.append('邮箱格式无效')
        
        # 生日验证
        birth_date = user_data.get('birth_date')
        if not birth_date:
            errors.append('生日不能为空')
        
        return {
            'is_valid': len(errors) == 0,
            'errors': errors
        }


class AgeCalculator:
    """年龄计算器"""
    
    def calculate_age(self, birth_date):
        """根据生日计算年龄"""
        # 简单实现：这里假设birth_date是ISO格式字符串
        from datetime import datetime
        import dateutil.parser
        
        birth = dateutil.parser.isoparse(birth_date)
        today = datetime.now()
        
        age = today.year - birth.year
        if today.month < birth.month or \
           (today.month == birth.month and today.day < birth.day):
            age -= 1
        
        return age


class UserService:
    """用户服务"""
    
    def __init__(self):
        self.user_validator = UserValidator()
        self.age_calculator = AgeCalculator()
    
    def create_user(self, user_data):
        """创建用户"""
        # 相关的验证逻辑聚合在一起
        validation_result = self.user_validator.validate_user(user_data)
        if not validation_result['is_valid']:
            raise ValueError(f'验证失败: {validation_result["errors"]}')
        
        # 计算年龄
        age = self.age_calculator.calculate_age(user_data.get('birth_date'))
        
        # 业务逻辑内聚
        user = {
            'id': 'user-123',  # 实际项目中应该使用唯一ID生成器
            'name': user_data.get('name'),
            'email': user_data.get('email'),
            'age': age,
            'created_at': datetime.now().isoformat()
        }
        
        # 保存用户到数据库（这里仅模拟）
        # self.user_repository.save(user)
        
        return user


# 使用示例
if __name__ == "__main__":
    # 演示合理的服务划分
    user_service = UserService()
    
    # 用户数据
    user_data = {
        'name': 'John Doe',
        'email': 'john.doe@example.com',
        'birth_date': '1990-01-01'
    }
    
    try:
        # 创建用户
        user = user_service.create_user(user_data)
        print(f"用户创建成功: {user}")
    except ValueError as e:
        print(f"用户创建失败: {e}")
```

**服务划分的最佳实践**：
- **单一职责原则**：每个服务只负责一个明确的业务领域
- **高内聚低耦合**：相关的功能应该放在同一个服务中
- **服务大小适中**：避免过大（单体）或过小（过度微服务化）的服务
- **领域驱动设计**：基于业务领域划分服务边界
- **考虑团队结构**：服务划分应考虑团队规模和能力
- **性能和可用性**：避免过多的跨服务调用影响性能
- **演进式设计**：从小开始，根据需要逐步拆分服务

## 总结

系统性能优化是一个系统性工程，需要从多个层面进行考虑和优化。关键是要建立科学的性能分析方法论，基于数据驱动的决策，制定合理的优化策略。

同时，要注意性能优化和其他质量属性之间的平衡，避免过度优化和过早优化。通过建立完善的监控和告警体系，持续跟踪和优化系统性能，才能构建出真正高性能、高可用的系统。